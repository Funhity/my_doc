find /opt/saltstack/    -path *test_v6_*/jdbc*| xargs -I {} sh -c ' sed -i s/rac-scan.xinxindai.com/192.168.31.223/g  {}'

获取svn主干下最新的分支：
export svn_address=http://192.168.38.230/YXCOD/01DEV/v6webapp_branches/v6webapp_20151016;trunk_address=`echo $svn_address|cut -d "/" -f -6` ;echo $trunk_address | { svn ls $trunk_address  --verbose |sort -n  -k 1 | grep -v "\./"  | awk '{ print $6 }' | tail -3 |xargs -I {}  sh -c 'svn log --stop-on-copy  "$svn_address"   | tail -n  10  | grep  -E "r[0-9]{5}.*" | tail -n 1| awk "{print \$5 \" \"  \$6 }" | tr -d "\n" ;echo " " {}  '  ; }  | sort -n -k1,2 | tail -n 1 | awk '{ print $3}'
 
 {}
==========
备份salt目录和salt服务器同步
    rsync -avz  wujunrong@192.168.38.10:/opt/     ~/salt_backup  --exclude ci/workspace --exclude walle   --exclude  ci/rsync/v6_static_rsync  --exclude  ci/rsync/webapp_static_rsync  --exclude ci/packages   --exclude ci/packages_jenkins   --exclude ci/rsync_jenkins  --exclude saltstack/salt/wujunrong/  --exclude test/  --delete-excluded   --delete
备份salt目录，把从salt服务器同步来的内容同步到svn目录下
    rsync -avh /home/wujunrong/salt_backup/    /home/wujunrong/salt-svn/salt_office_env/     --cvs-exclude --dry-run

```````日志````
   scp wujunrong@192.168.38.10:/home/wujunrong/svn/wujunrong/rsyslog.conf.client-tomcat   .
   salt '*tomcat'  state.single      file.managed  name='/etc/rsyslog.conf'   source='salt://files/rsyslog.conf.client-tomcat'  backup='minion'

   scp wujunrong@192.168.38.10:/home/wujunrong/svn/wujunrong/logrotate_tomcat.logclient    .
   salt '*tomcat'  state.single      file.managed  name='/etc/logrotate.d/logrotate_tomcat'   source='salt://files/logrotate_tomcat.logclient'  backup='minion'
============
tomcat 相关配置

   
   创建tomcat v6项目war文件部署目录
   ansible   perf_*_tomcat  -m shell  -a "mkdir  -p \`cat /etc/salt/grains|  grep tomcat_app_dir|cut -d \" \" -f 2\`  "
   或ansible perf*tomcat    -m shell  -a  "mkdir -p /opt/webserver/v6_tomcat/"
   
   安装tomcat
   ansible perf*tomcat -m copy -a "src=/root/apache-tomcat-6.0.45.tar.gz   dest=/root/"  
   ansible perf*tomcat    -m shell  -a  "tar zxf /root/apache-tomcat-6.0.45.tar.gz  -C   /opt/webserver/"
   ansible perf*tomcat    -m shell  -a  "mv    /opt/webserver/apache-tomcat-6.0.45/*    /opt/webserver/v6_tomcat/"
   

     
   从stage环境copy tomcat文件：scp root@192.168.31.50:/etc/init.d/tomcat  /root/
   ansible perf*tomcat -m copy -a "src=/root/tomcat  dest=/etc/init.d/tomcat"
   ansible perf*tomcat -m shell -a "chmod +x  /etc/init.d/tomcat"

   tomcat用户相关
   ansible perf*tomcat -m shell -a "useradd tomcat && usermod -u 1501  tomcat && chown -R tomcat:tomcat  /opt/webserver/v6_tomcat/"
   ansible perf*tomcat -m shell -a "usermod -u 1501  tomcat"
   salt *tomcat    cmd.run 'chown -R tomcat:tomcat  /opt/webserver/v6_tomcat/'
   
   配置tomcat服务器的hosts文件，获取ip地址和主机名称（另外，配置salt 域名也需要修改hosts文件）
   ansible perf*tomcat -m shell -a "ip a | grep 192 |awk '{print \$2}'  |cut -d "/" -f 1  | tr -d \"\n\" ;echo -ne ' '  ;hostname "
   ansible perf_credit01_tomcat  -m shell -a "{ ip a | grep 192 |awk '{print \$2}'  |cut -d "/" -f 1  | tr -d \"\n\" ;echo -ne ' '  ;hostname; }>>/etc/hosts"
   
   
   配置java环境和tomcat环境
   `````````````
      ansible perf*tomcat     -m copy  -a "src=/root/jdk-6u45-linux-x64-rpm.bin  dest=/root/"
      
      ansible perf_zoo*       -m shell -a  "chmod +x /root/jdk-6u45-linux-x64-rpm.bin"
      ansible perf_zoo*       -m shell -a  "/root/jdk-6u45-linux-x64-rpm.bin"
      ansible perf_zoo*       -m shell -a  "yum -y remove java-1.7.0-openjdk.x86_64"
      ansible perf*tomcat -m shell  -a  "rpm -e jdk-1.6.0_45-fcs.x86_64"
      ansible perf*tomcat -m shell  -a   "rm -fr /root/jdk-6u45-linux-amd64.rpm /root/sun*.rpm"
      ansible perf*tomcat -m shell  -a  "/root/jdk-6u45-linux-x64-rpm.bin"
   `````````````
   
   创建环境变量设置脚本（java和tomcat） vi /root/install_java_env.sh
   脚本位于服务器上                      ansible perf*tomcat -m  script -a "/root/install_java_env.sh"
   vi /root/install_java_env.sh
   #!/bin/bash
   set -e
   sudo tee /etc/profile.d/java.sh<<-'EOF'
      export JAVA_HOME=/usr/java/default
      export CATALINA_HOME=/opt/webserver/v6_tomcat/
      #export PATH=/root/apache-maven-3.2.5/bin:$PATH
   EOF
   chmod +x /etc/profile.d/java.sh

   sudo tee /opt/webserver/v6_tomcat/bin/setenv.sh<<-'EOF' 
       #JAVA_HOME=/usr/java/latest
       CATALINA_PID="$CATALINA_BASE/tomcat.pid"
       #CATALINA_OPTS="-server -Xms1024m -Xmx5120m -XX:PermSize=128m -XX:MaxPermSize=2048m -XX:NewSize=192m -XX:MaxNewSize=2048m -Dfile.encoding=UTF-8 -Dcom.sun.management.jmxremote  -Dcom.sun.management.jmxremote.port=1234  -Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.ssl=false"  
        CATALINA_OPTS="                  -Xms512m -Xmx2048m -XX:PermSize=128m -XX:MaxPermSize=256m -XX:NewSize=192m -XX:MaxNewSize=384m                -Dcom.sun.management.jmxremote  -Dcom.sun.management.jmxremote.port=8999  -Dcom.sun.management.jmxremote.ssl=false          -Dcom.sun.management.jmxremote.authenticate=false  "   
   EOF

   chmod +x  /opt/webserver/v6_tomcat/bin/setenv.sh

   ```````````````
   
=================== 
v6配置文件

	修改webservice配置文件
        find ./stage_*       |xargs -I {}  grep -iH  webservice {}    | cut -d ":" -f 1 |xargs -I {}  sh -c 'echo -e  =====file-name {} ======"\n";sed -i s/192.168.31.34/192.168.129.63/g {}'
 		find ./stage_* |xargs -I {}  grep -iH  webservice {}    | cut -d ":" -f 1 |xargs -I {}  sh -c 'echo -e  =====file-name {} ======"\n";sed  s/192.168.31.67/192.168.129.62/g {}' | grep "129.62\|file-name" 
        find ./stage_* |xargs -I {}  grep -iH  192.168.31.67 {} | cut -d ":" -f 1 |xargs -I {}  sh -c 'echo -e  =====file-name {} ======"\n";sed  s/192.168.31.67/192.168.129.62/g {}' | grep "129.62\|file-name"
	    
		find   ./perf_v6_mobile/ | grep -v "./perf_v6_mobile/$"    |xargs -I {}  grep -iH  webservice {}    | cut -d ":" -f 1 |xargs -I {}  sh -c 'echo -e  =====file-name {} ======"\n";sudo sed -i  s/192.168.31.34/192.168.129.63/g {}'
```````````````````````````````````````
        查询性能环境中缺少的配置文件
        { find  ./stage/   -type f  | xargs  -I {} sh -c 'a={}; echo   ${a/stage/xn} ' |xargs -I {} ls -l {} | grep "cannot access"; } 2>&1 |  sed -n  "s/.*\(\.\/xn.*\):.*/\1/p"
        查询性能环境中缺少的配置文件，从stage环境拷贝过去
        { find  ./stage/   -type f  | xargs  -I {} sh -c 'a={}; echo   ${a/stage/xn} ' |xargs -I {} ls -l {} | grep "cannot access"; } 2>&1 |  sed -n  "s/.*\(\.\/xn.*\):.*/\1/p"   | xargs -I {} sh -c 'a={}; cd stage;cp  --parent  ${a/.\/xn/.} ../xn '
 
		webservice查询：find   ./xn   -type f  |xargs -I {}  grep -iH  webService_url {}   
	    webservice修改：find   ./xn   -type f  |xargs -I {}  sed -i s/192.168.31.34/192.168.129.63/g {}   | grep -v '#'
	    tradews   查询：find   ./xn   -type f  |xargs -I {}  grep -iH  trade_url   {}
        
        redis     查询：find   -path ./xn*redis*    |  xargs -I {} grep -iH 'redis\.ip'  {}
        redis     修改：find   -path ./xn*redis*    |  xargs -I {}  sed -i s/192.168.31.67/192.168.129.62/g {}
		
		zookeeper 查询：find   -path ./xn*zook*    | xargs -I {} grep servers  {}
		                find   -path ./xn*zook*    | xargs -I {} sed -i s/192.168.31.34/192.168.129.63/g {}
		jdbc查询：	  
	          find ./xn   -type f  |xargs -I {}  grep -iH  jdbc\.url  {} | grep -v '#'
              find ./xn   -type f  |xargs -I {} sed -i s/rac-scan.xinxindai.com/192.168.129.75/g {}
              find ./xn   -type f  |xargs -I {} sed -i s/192.168.31.223/192.168.129.75/g {}			  
````````````````````````````````````	
	修改redis配置文件
        find ./stage_*/redis*     -type f | xargs sed -i  s/192.168.129.63/192.168.129.62/g
    
	检查grains配置文件
        salt *mobile0?    cmd.run "cat /etc/salt/grains|grep tomcat_app_dir|cut -d ':' -f 2-| xargs ls "
    
	nginxws配置文件
         salt perf_nginxws0? state.single file.managed  name='/etc/nginx/nginx.conf'         source='salt://files/nginx.conf.xxd' backup='minion'
         salt perf_nginxws0? state.single file.managed 'name=/etc/nginx/conf.d/tomcat.conf'  source='salt://files/tomcat.conf.xxd' backup='minion'
    
	rsync配置
         salt perf_nginx0?   state.single file.managed 'name=/etc/rsync.pass'  source='salt://configure/rsync.pass' backup='minion'
    
	nginx配置
	     ansible perf_ngi*   -m copy -a "src=/root/nginx-1.10.0.tar.gz dest=/root/"
	     scp root@192.168.31.22:/usr/local/nginx/conf/v6.xinxindai.com.crt          /etc/nginx/
         scp root@192.168.31.22:/usr/local/nginx/conf/v6.xinxindai_nopass.key       /etc/nginx/
		 
		 
	配置keepalived	 
     ``````````````
     ansible perf_nginxws0?  -m shell -a "yum install keepalived -y"
     
     salt    perf_nginxws0?  state.single file.managed name='/etc/keepalived/keepalived.conf'  source='salt://files/keepalived.conf' backup='minion'
     salt    perf_nginxws0?  file.list_backups  /etc/keepalived/keepalived.conf
     salt    perf_nginxws0?  state.single file.managed name='/root/nginx_pid.sh'  source='salt://files/nginx_pid.sh' backup='minion'
     ansible perf_nginxws01  -m shell -a "sed -i.bak3  s/192.168.31.34/192.168.129.63/g /etc/keepalived/keepalived.conf"
     ansible perf_nginxws02  -m shell -a "sed -i.bak3  -e s/state\ MASTER/state\ BACKUP/g  -e s/priority\ 100/priority\ 50/g  -e s/192.168.31.34/192.168.129.63/g /etc/keepalived/keepalived.conf"
     ````````` 
    配置zookeeper
	     ansible perf_salt       -m copy  -a "src=/root/zookeeper-3.4.8.tar.gz dest=/root/"
         find ./stage_*/zookeeper.p*|xargs -I {} sed -i.bak -e s/192.168.31.60/192.168.129.50/g -e s/192.168.31.61/192.168.129.51/g  -e  s/192.168.31.62/192.168.129.52/g   {} 
    
    安装zookeeper
         ansible perf_zookeep02  -m shell -a "cp  /root/xxd/zookeeper-3.4.8/conf/zoo_sample.cfg  /root/xxd/zookeeper-3.4.8/conf/zoo.cfg"
         ansible perf_zookeep02  -m shell -a "echo -e \"server.1=192.168.129.50:2888:3888\nserver.2=192.168.129.51:2888:3888\nserver.3=192.168.129.52:2888:3888\">>/root/xxd/zookeeper-3.4.8/conf/zoo.cfg"
         ansible perf_zookeep0?  -m shell -a "mkdir   /tmp/zookeeper/"
         ansible perf_zookeep01  -m shell -a "echo 1 >   /tmp/zookeeper/myid"
         echo -e "server.1=192.168.129.50:2888:3888\nserver.2=192.168.129.51:2888:3888\nserver.3=192.168.129.52:2888:3888"
         
         ansible perf_zookeep0?  -m shell -a "cd /root/xxd/zookeeper-3.4.8/bin/ &&./zkServer.sh restart"
         ansible perf_zookeep0?  -m shell -a "cd /root/xxd/zookeeper-3.4.8/bin/ &&./zkServer.sh status"
	
		 ansible perf_zookeep0?  -m shell -a "cd /root/xxd/zookeeper-3.4.8/bin/ &&./zkServer.sh start"
         ansible perf_zookeep0?  -m shell -a "cd /root/xxd/zookeeper-3.4.8/bin/ &&./zkServer.sh status"
         ansible perf_zookeep0?  -m shell -a "cd /root/xxd/zookeeper-3.4.8/bin/ &&./zkServer.sh stop"
		 
		 
     ```````````		 
     修改webservice配置文件
         find ./stage_* |xargs -I {}  grep -iH  webservice {} | cut -d ":" -f 1 |xargs -I {}  sh -c 'echo -e  =====file-name {} ======"\n";sed -i s/192.168.31.34/192.168.129.63/g {}'
		 
==========
salt安装

     ansible perf_zookeep0?  -m shell -a "echo 192.168.38.10 salt>>/etc/hosts"
``````
     ansible integration*   -m shell -a "sudo yum -y install https://repo.saltstack.com/yum/redhat/salt-repo-latest-1.el6.noarch.rpm"
     ansible perf_zookeep0?  -m shell -a " yum -y install salt-minion"      
``````
	 或ansible salt_perf*   -m shell -a "sudo yum -y install epel-release"
     
	 ansible salt_perf*      -m shell -a "yum --disablerepo=epel -y update  ca-certificates"
     ansible perf_zookeep0?  -m shell -a " yum update python  -y"
     ansible perf_salt       -m shell -a " yum -y install libselinux-python"
	 
     
	 
     
	 ansible perf_salt       -m shell -a  "service salt-master start; chkconfig salt-master on"
	 ansible perf_zookeep0?  -m shell -a  "service salt-minion start ;chkconfig  salt-minion on"
	 

	清除minion上的salt名称，重新到salt-master上认证
       ansible perf* -m shell -a ">  /etc/salt/minion_id"

	
	配置/etc/salt/master
     file_roots:
      base:
        - /srv/salt
==========
操作系统OS基础配置
配置host名称
     ansible perf_zookeep01  -m shell -a "sed -i.bak  s/HOSTNAME=.*/HOSTNAME=pf-zookeep01/g   /etc/sysconfig/network;hostname pf-zookeep01"

	 根据ansible配置文件批量修改hostname
     cat /etc/ansible/hosts| grep perf_ |awk '{print $1}'|xargs -I {} ansible {} -m shell -a "hostname {};sed -i.bak  s/HOSTNAME=.*/HOSTNAME={}/g   /etc/sysconfig/network ;hostname"
配置网络
     ansible perf*  -m shell -a "chkconfig iptables off"
     ansible perf_credit01 -m shell -a  "ip r add  172.16.0.0/255.255.0.0   via 192.168.129.253"
     ansible perf_credit01 -m shell -a  "ip r delete  default"
     ansible perf_credit01 -m shell -a  "ip r add  default via 192.168.129.254"
     ansible perf_credit01 -m shell -a  "ip r add  192.168.0.0/255.255.0.0   via 192.168.129.253"
    或  vi /root/ifup-local
    `````
    #!/bin/bash
	sudo tee /sbin/ifup-local<<-'EOF'  
    if [[ "$1" == "eth0" ]]
    then
         echo start configure $1    
         ip r add  172.16.0.0/255.255.0.0   via 192.168.129.253
         ip r delete  default
         ip r add  default via 192.168.129.254
         ip r add  192.168.0.0/255.255.0.0   via 192.168.129.253
    
    fi
	EOF
    chmod +x /sbin/ifup-local
	`````
    ansible perf_mobile01_tomcat  -m  script -a "/root/ifup-local"
	
安装zabbix agent
     ansible  perf*  -m  shell  -a  "rpm -Uvh http://repo.zabbix.com/zabbix/2.4/rhel/6/x86_64/zabbix-release-2.4-1.el6.noarch.rpm"
     ansible  perf*  -m  shell  -a  "yum -y install zabbix-agent"	 
	 ansible perf_*  -m copy     -a "src=/root/zabbix_agentd.conf            dest=/etc/zabbix/zabbix_agentd.conf  "
	 ansible perf_*       -m shell   -a " /etc/init.d/zabbix-agent restart "
	
============
安装redis
    ansible perf_redis0? -m copy -a "src=/root/redis-3.2.0.tar.gz   dest=/root/"
    ansible perf_redis0? -m shell -a "tar zxf /root/redis-3.2.0.tar.gz -C /root/" 
    cd  /root/redis-3.2.0/src  &&  yum -y groupinstall 'Development Tools' && make && make install

    sed -e "s/^daemonize no$/daemonize yes/" -e "s/^dir \.\//dir \/var\/lib\/redis\//" -e "s/^loglevel debug$/loglevel notice/" -e "s/^logfile stdout$/logfile \/var\/log\/redis.log/" redis.conf > /etc/redis/redis.conf    
=============




ansible perf_redis0? -m shell  -a "yum install  libselinux-python  -y"
ansible perf_redis01   -m shell -a "sed -i.bak  s/HOSTNAME=.*/HOSTNAME=pf-redis01/g   /etc/sysconfig/network;hostname pf-redis01"

ansible perf_nginxws0?  -m shell -a "sudo yum -y install epel-release"



ansible  perf_credit01  -m copy -a "src=/root/apache-tomcat-6.0.45.tar.gz   dest=/root/"
ansible perf*tomcat -m copy -a "src=/root/jdk-6u45-linux-x64-rpm.bin  dest=/root/"
ansible perf*tomcat -m shell  -a  "chmod +x /root/jdk-6u45-linux-x64-rpm.bin"


ansible perf*tomcat -m shell  -a   "yum -y remove java-1.7.0-openjdk.x86_64"
ansible perf*tomcat -m shell  -a   "yum -y remove java-1.6.0-openjdk.x86_64"

=====xxd grains检查====
检查tomcat部署目录是否合法
sudo  salt -L  "$minion_id"    cmd.run ' cat /etc/salt/grains|grep tomcat_app_dir|cut -d ':' -f 2-    | xargs ls  '

检查mount
sudo  salt -L  "$minion_id"    cmd.run "mount | grep 60.34"
=======



````nginx section`````````

sudo tee /etc/yum.repos.d/docker.repo <<-'EOF'
[dockerrepo]
name=Docker Repository
baseurl=https://yum.dockerproject.org/repo/main/centos/$releasever/
enabled=1
gpgcheck=1
gpgkey=https://yum.dockerproject.org/gpg
EOF

vi install_nginx.sh
sudo tee /etc/yum.repos.d/nginx.repo<<-'EOF'
[nginx]
name=nginx repo
baseurl=http://nginx.org/packages/centos/6/$basearch/
gpgcheck=0
enabled=1
EOF
ansible perf_ngi*   -m  script -a "/root/install_nginx.sh"
ansible perf_ngi*  -m shell -a "cat /etc/yum.repos.d/nginx.repo"
ansible perf_ngi*  -m shell -a "yum -y install nginx"
ansible perf_ngi*  -m shell -a "/etc/init.d/nginx start"
ansible perf_ngi*  -m shell -a "nginx -v"


```zabbix section````````
{192.168.31.196:system.run[/opt/dell/srvadmin/bin/omreport storage pdisk controller=0 ].regexp(^Status.*Ok,#1)}=0
 cat   2015-10-15.csv | grep -v '^$' |awk '{ for (i=1;i<=NF;i++) {   print $i NR-1  }  }'
``````````
virsh list --all| grep runn   | awk '{print $2}' |xargs  -I '{}' sh -c "echo -n {} ' ' ;sudo virsh domiflist {}|grep vnet|awk '{printf \"%s \", \$5  }';echo "
nmap -sP 192.168.31.0/24   > network31.txt
grep -i 52:54:00:3d:a1:03  network31.txt -B 2 | grep -v Host
````````````````

virsh list --all| grep runn                   | awk '{print $2}' |xargs  -I '{}' sh -c "echo {} ;sudo virsh domiflist '{}'"

docker build --rm -t local/centos7-reviewboard   .
docker commit --change='CMD ["/root/start.sh"]'   --change='ENTRYPOINT ["/tini", "--"]'  -c "EXPOSE 8080"  pensive_austin    xxd-tomcat:final
sudo salt 'test_v6_redis01' cmd.run "netstat -antup| grep redis| awk '{print \$5}' | awk -F: '{print \$1}'" | grep 192|xargs -I {} grep  -B 1 {} ./office.ip.list

http://192.168.38.119/cdh5.5.0-parcels/
http://192.168.38.119/centos6-cm5.5.0

salt-ssh -i    cloudera-agent3    state.low '{ {% set my_path = salt['grains.get']('id') %} ,  state: cmd ,fun: run ,name: "unzip -o /root/v6_webservice.war  -d  /tmp/root{{ my_path }}" }'

```pv section````
svn list  http://192.168.38.230/YXITD/saltstack/trunk/wujunrong/    --username wujunrong| grep  ansible |xargs -I {} svn export  http://192.168.38.230/YXITD/saltstack/trunk/wujunrong/{} --force  /etc/ansible/hosts
ansible test_nginx    -m fetch  -a "src=/var/log/nginx/bigdata_pv.log     dest=/opt/logs/share/{{ inventory_hostname }}.bigdata_pv.log  flat=yes"
```````````````````

del "\"sys_cofing_RISK_CALL_SWITCH\""

redis安装在192.168.32.251上
huangchao(黄超（高级系统工程师）) 11-30 17:32:50
硬盘给50G就可以了

ansible  fix-env   -m shell -a "cd  /tmp/v6/install_packet/&& zip ./conf_file.zip ./redis.properties"
=======================
v6test

nginx:
192.168.38.182:/var/ftp/pub/v6_front/image   /usr/local/nginx/html/static/image
192.168.38.182:/var/ftp/pub/v6_admin/image   /usr/local/nginx/html/static/admin/image 
192.168.38.182:/var/ftp/pub/download         /usr/local/nginx/html/static/download 

front:
192.168.38.182:/var/ftp/pub/v6_front/image   /opt/webserver/v6_tomcat/webapps/ROOT/static/image 
192.168.38.182:/var/ftp/pub/v6_admin/image   /opt/webserver/v6_tomcat/webapps/ROOT/static/admin/image 

mobile



192.168.38.182:/var/ftp/pub/v6_front/image   /opt/webserver/v6_tomcat/webapps/v5_mobile/static/image 
192.168.38.182:/var/ftp/pub/v6_admin/image   /opt/webserver/v6_tomcat/webapps/v5_mobile/static/admin/image 

webapp
192.168.38.182:/var/ftp/pub/v6_front/image   /opt/webserver/v6_tomcat/webapps/m/static/image
=============================
v6stage
nginx
192.168.31.250:/volume1/v6/v6_front/image    /usr/local/nginx/html/static/image type nfs (rw,vers=4,addr=192.168.31.250,clientaddr=192.168.31.24)
192.168.31.250:/volume1/v6/v6_admin/image    /usr/local/nginx/html/static/admin/image type nfs (rw,vers=4,addr=192.168.31.250,clientaddr=192.168.31.24)
192.168.31.250:/volume1/v6/download          /usr/local/nginx/html/static/download type nfs (rw,vers=4,addr=192.168.31.250,clientaddr=192.168.31.24)

webapp
192.168.31.250:/volume1/v6/v6_front/image    /opt/webserver/v6_tomcat/webapps/m/static/image

===================

unzip -o /tmp/v6/install_packet/conf_file.zip -d /opt/webserver/v6_tomcat/webapps/xxdai_sys_admin/WEB-INF/classes/

sudo salt -N 'test_v6tomcat' cmd.run "find /opt/webserver/ -type d  -print0 |  grep -EzZ  'classes/com$' |sed 's/WEB-INF\/classes\/com//g';echo"
sudo salt -N 'stage_v6tomcat'  cmd.run "find /opt/webserver/ -type d  -print0 |  grep -EzZ  'classes/com$' |sed 's/WEB-INF\/classes\/com//g';echo;salt-call --local  grains.item  tomcat_app_dir | grep opt | xargs -I {}  sh -c 'echo {};[ -d {} ] && echo found  || echo not found  '  "


test_v6_seo:
    /opt/webserver/v6_tomcat/webapps/ROOT/
test_v6_webapp_front01:
    /opt/webserver/v6_tomcat/webapps/m/
test_v6_tomcat_admin01:
    /opt/webserver/v6_tomcat/webapps/v6_admin/
test_v6_tomcat_batch01:
    /opt/webserver/v6_tomcat/webapps/xxdai_batch/
test_v6_tomcat_mobile02:
    /opt/webserver/v6_tomcat/webapps/v5_mobile/
test_v6_tomcat_front02:
    /opt/webserver/v6_tomcat/webapps/ROOT/
test_v6_tomcat_admin02:
    /opt/webserver/v6_tomcat/webapps/v6_admin/
test_v6_tomcat_batch02:
    /opt/webserver/v6_tomcat/webapps/xxdai_batch/
test_v6_tomcat_front01:
    /opt/webserver/v6_tomcat/webapps/ROOT/
test_v6_tomcat_tradew01:
    /opt/webserver/v6_tomcat/webapps/v6_tradews/
test_v6_tomcat_mobile01:
    /opt/webserver/v6_tomcat/webapps/v5_mobile/
test_v6_tomcat_webservice01:
    /opt/webserver/v6_tomcat/webapps/v6_webservice/
test_v6_tomcat_tradew02:
    /opt/webserver/v6_tomcat/webapps/v6_tradews/
test_v6_tomcat_webservice02:
    /opt/webserver/v6_tomcat/webapps/v6_webservice/

======log section==========
tar  -zcvf /tmp/wutest.tar.gz   `find . -type f -newerct "2016-03-07  15:00:00"   ! -newerct "2016-03-07 17:00:00"   -regex ".*mobile.*\|.*webser.*" `
find . -type f -newerct "2016-02-07  00:00:00"   ! -newerct "2016-03-08 00:00:00"   | xargs -I {}  sh -c 'echo == {} ==;zcat {} | grep -anE -C 20 "AccountRechargeServiceImpl...updateAccountRecharge...BankCode=" ' >/tmp/webservice_yangzhe_0207-0307.txt

ansible stage_tomcat_webservice    -m fetch  -a "src=/opt/webserver/v6_tomcat/logs/catalina.out   dest=/tmp/{{ inventory_hostname  }}.out  flat=yes"
zip catalina.zip *.out
ansible stage_tomcat_webservice    -m fetch  -a "src=/opt/webserver/v6_tomcat/logs/localhost.2016-06-24.log   dest=/tmp/{{ inventory_hostname  }}.log  flat=yes
============

sudo  salt 'ct-production-tomcat-partener02.xinxindai.com' cp.push '/opt/webserver/tomcat/webapps/v6_partener/WEB-INF/classes/discuz.properties'
sudo  find /var/cache/salt/master/minions/    -name discuz.properties   -exec cp {} /srv/salt/wujunrong/discuz.properties   \;
sudo  salt     'ct-production-tomcat-partener01.xinxindai.com'     file.manage_file /opt/webserver/tomcat/webapps/v6_partener/WEB-INF/classes/discuz.properties    '' '{}' salt://wujunrong/discuz.properties     '{hash_type: 'md5', 'hsum': <md5sum>}' root root '644' base 'minion'

堡垒：super/YXauth!@#xxd001

===deploy section========
diff -r /opt/ci/rsync/v6_static_rsync/    /opt/ci/workspace/dev_v6.4.4_20160308/v6_front/trunk/target/v6_front/static/
find   /opt/ci/workspace/dev_v6.4.4_20160308/  -name pom.xml -exec sh -c 'path_dir=`dirname {}`; cd $path_dir;mvn clean package;' \;
````
rm -fr /opt/ci/rsync/v6_static_rsync/*
cp -r  /opt/ci/workspace/dev_v6.4.4_20160308/v6_front/trunk/target/v6_front/static/*  /opt/ci/rsync/v6_static_rsync/
````
========elasticsearch section=====
31.160和31.162 上有fluem服务，在lib目录 /opt/cloudera/parcels/CDH-5.5.0-1.cdh5.5.0.p0.8/lib/flume-ng/lib中增加 elasticsearch-1.7.1.jar 文件

salt 'stage_v6_admin01' state.single   file.recurse  source='salt://wujunrong/20160321config/abc'  name='/opt/webserver/v6_tomcat/webapps/xxdai_sys_admin/image/data/xheditor_Upload'




touch /home/wujunrong/tmp/error.txt;find  ./  -type d  -path ./v6_batch/trunk/batch-schedule -prune -o  -name pom.xml   -print | grep -v "v6_batch/trunk/pom.xml" |  xargs -I {}  sh -c 'path_dir=`dirname {}`&& cd $path_dir && mvn clean package ; return_val=$?  | tee ./output.txt; (if [ "$return_val" -ne 0 ];then ( cat ./output.txt  >>/home/wujunrong/tmp/error.txt)  ;else  cp  ./target/*.war   /home/wujunrong/tmp/;fi)   '

主机名规范：环境-(业务)-运行服务-ip.地点
           test-v6_front01-tomcat-192.168.38.1.yh
           base-svn_master-192.168.1.1.yh
           stage-hypervisor_kvm01-192.168.1.2.yh
		   


==============生产================
hyper-pingtai.xinxindai.com
         dns                                            192.168.110.101    52:54:00:8B:46:49
         ks_cobbler                                     192.168.110.104    52:54:00:CA:72:39  
         monitor                                        192.168.110.60     52:54:00:85:96:A2
         salt                                           192.168.110.102    52:54:00:49:31:C7
         discuz001                                      192.168.10.14
         v5_sc_mysql                                    192.168.110.106    52:54:00:CA:FD:8C
         yw_redis01                                     192.168.110.107    52:54:00:00:A6:C0
         log                                            192.168.110.105    52:54:00:27:67:FF
         v6_front04                                     192.168.110.78     52:54:00:27:61:F1
         saltweb.xinxindai.com                          192.168.110.108    52:54:00:A8:6C:5D
         ansible
         xinxindai-rtx


hyper-v6_pro03.xinxindai.com
      v6_nginx03                                     192.168.110.24     52:54:00:7D:0C:00
      v6_front03                                     192.168.110.28     52:54:00:2D:CE:C8
      v6_webservice03                                192.168.110.42     52:54:00:E2:EE:04
      v6_tradew03                                    192.168.110.45     52:54:00:DA:57:34
      v6_mobile02                                    192.168.110.71     52:54:00:06:60:0D
      v6_zookeeper03                                 192.168.110.50     52:54:00:77:2D:BA
      v6_webapp_front02                              192.168.110.73     52:54:00:F2:0E:48
      ct-production-tomcat-partener02.xinxindai.com  192.168.110.80     52:54:00:39:31:56
      sftp_slave_192.168.110.91                      192.168.110.91     52:54:00:7E:BF:44

hyper-v6_pro01.xinxindai.com
      v6_lvs01                                       192.168.110.20     52:54:00:13:72:E3
                                                     192.168.110.25     52:54:00:13:72:E3
      v6_nginx01                                     192.168.110.22     52:54:00:9A:67:E7
      v6_front01                                     192.168.110.26     52:54:00:BC:0D:5E
      v6_admin01                                     192.168.110.29     52:54:00:A2:60:6A
      v6_nginxws01                                   192.168.110.31     52:54:00:C6:A2:55
                                                     192.168.110.33     52:54:00:C6:A2:55
      v6_webservice01                                192.168.110.40     52:54:00:F2:DE:17
      v6_tradew01                                    192.168.110.43     52:54:00:5A:42:9E
      v6_batch01                                     192.168.110.46     52:54:00:14:8C:F8
      v6_redis01                                     192.168.110.51     52:54:00:A8:89:4B
                                                     192.168.110.53     52:54:00:A8:89:4B
      v6_zookeeper01                                 192.168.110.48     52:54:00:EF:9F:33
      v6_seo01                                       192.168.110.76     52:54:00:8A:DB:1F
      v6_credit_app01                                192.168.110.81     52:54:00:9F:D0:11

hyper-v6_pro02.xinxindai.com
      v6_lvs02                                       192.168.110.21     52:54:00:8D:AB:67
      v6_nginx02                                     192.168.110.23     52:54:00:31:5D:D4
      v6_front02                                     192.168.110.27     52:54:00:10:68:1D
      v6_admin02                                     192.168.110.30     52:54:00:1C:DA:41
      v6_nginxws02                                   192.168.110.32     52:54:00:A0:15:A2
      v6_webservice02                                192.168.110.41     52:54:00:6D:D2:5B
      v6_tradew02                                    192.168.110.44     52:54:00:A9:7F:C7
      v6_batch02                                     192.168.110.47     52:54:00:B9:25:A0
      v6_redis02                                     192.168.110.52     52:54:00:41:CB:0E
      v6_mobile01                                    192.168.110.70     52:54:00:7C:80:42
      v6_zookeeper02                                 192.168.110.49     52:54:00:B2:1F:36
      v6_webapp_front01                              192.168.110.72     52:54:00:38:5D:B8
      v6_seo02                                       192.168.110.77     52:54:00:E9:C7:C4
      ct-production-tomcat-partener01.xinxindai.com  192.168.110.79     52:54:00:C8:27:86
      v6_credit_app02                                192.168.110.82     52:54:00:35:DF:42
      sftp_master_192.168.110.90                     192.168.110.90     52:54:00:7A:76:BB


hyper-host2
       nginx02 192.168.50.62 52:54:00:49:4F:87
       auth02  192.168.50.64 52:54:00:66:31:35
       app2    192.168.50.71 52:54:00:A2:2D:B7
       fk002   192.168.50.56 52:54:00:9C:0E:3A

hyper-host1
      nginx01  192.168.50.60 52:54:00:93:B9:83
               192.168.50.61 52:54:00:93:B9:83
      auth01   192.168.50.63 52:54:00:08:0E:E3    key认证
      app1     192.168.50.70 52:54:00:25:A5:A0    OA
      fk001    192.168.50.55 52:54:00:37:B5:47
      xinxindai-rtx-20160620
	  
	  
hyper-cloudera-pro
      cloudera-appserver                                                     52:54:00:9F:27:49
      cloudera-dashboard                                  192.168.32.21      52:54:00:10:05:F2
	  cloudera-flume01                                    192.168.32.19      52:54:00:27:FB:06
      cloudera-flume02                                    192.168.32.20
      cloudera-redis01                                    192.168.32.50      52:54:00:3B:C2:3E
      cloudera-redis02                                    192.168.32.51      52:54:00:38:66:18
      ct-production-hadoop-dashboard_mysql.xinxindai.com  192.168.32.252     52:54:00:C7:54:96
      cloudera-ace-ws                                     192.168.32.23      52:54:00:DC:BD:B9
==============办公室==============

v6_stage3:hyper-YH-Virtualization-KVM08-192.168.31.17.xinxindai.com
        jenkins                                                  192.168.31.81      52:54:00:0B:72:59
        stage_oa.xinxindai.com                                   192.168.31.40      52:54:00:A0:CE:68
        stage_oa2.xinxindai.com                                  192.168.31.41      52:54:00:DD:63:58
        stage_oa3.xinxindai.com
        stage_v6_admin03                                         192.168.31.31      52:54:00:A7:91:48
        stage_v6_front03                                         192.168.31.28      52:54:00:E4:49:1A
        stage_v6_mobile01.xinxindai.com                          192.168.31.70      52:54:00:8F:19:66
        stage_v6_nginx03                                         192.168.31.24      52:54:00:BE:B6:0C
        stage_v6_redis02                                         192.168.31.66      52:54:00:4E:1E:F2
        stage_v6_seo002                                          192.168.31.88      52:54:00:02:FC:E4
        stage_v6_tradew03                                        192.168.31.56      52:54:00:B5:F2:EC
        stage_v6_webapp02                                        192.168.31.73      52:54:00:BC:45:91
        stage_v6_webservice03                                    192.168.31.52      52:54:00:0E:2D:56
        stage_v6_zookeeper03                                     192.168.31.62      52:54:00:EB:7C:BC
v6_stage1:hyper-YH-Virtualization-KVM06-192.168.31.15.xinxindai.com
        ct-stage-tomcat-partener01.xinxindai.com                 192.168.31.90      52:54:00:AC:3F:87
        stage_fk001                                              192.168.31.85      52:54:00:E3:B4:7E
        stage_fk_nginx01                                         192.168.31.82      52:54:00:E9:3F:F5
                                                                 192.168.31.83      52:54:00:E9:3F:F5
        stage_v6_admin01                                         192.168.31.29      52:54:00:EC:87:DF
        stage_v6_batch02                                         192.168.31.57      52:54:00:34:FD:30
        stage_v6_front01                                         192.168.31.26      52:54:00:2F:0A:E3
        stage_v6_lvs01                                           192.168.31.20      52:54:00:3E:28:62
                                                                 192.168.31.25      52:54:00:3E:28:62
        stage_v6_nginx01                                         192.168.31.22      52:54:00:41:9C:3B
        stage_v6_nginxws01                                       192.168.31.32      52:54:00:56:D5:EE
                                                                 192.168.31.34      52:54:00:56:D5:EE
        stage_v6_tradew01                                        192.168.31.54      52:54:00:8D:9C:9D
        stage_v6_webservice01                                    192.168.31.50      52:54:00:A9:DF:AF
        stage_v6_zookeeper01                                     192.168.31.60      52:54:00:89:72:F0
        stage-v6_credit_app01-tomcat-192.168.31.89.yh            192.168.31.89      52:54:00:12:EB:1E
		
v6_stage2:hyper-YH-Virtualization-KVM07-192.168.31.16.xinxindai.com
        ct-stage-tomcat-partener02.xinxindai.com                 192.168.31.91      52:54:00:77:6B:12
        stage_fk002                                              192.168.31.86      52:54:00:97:A8:A8
        stage_fk_nginx02                                         192.168.31.84      52:54:00:72:12:6E
        stage_v6_admin02                                         192.168.31.30      52:54:00:65:50:95
        stage_v6_batch02                                         192.168.31.58      52:54:00:11:01:89
        stage_v6_front02                                         192.168.31.27      52:54:00:11:FB:BC
        stage_v6_lvs02                                           192.168.31.21      52:54:00:95:20:E4
        stage_v6_mobile02                                        192.168.31.71      52:54:00:07:B0:8A
        stage_v6_nginx02                                         192.168.31.23      52:54:00:E6:77:0F
        stage_v6_nginxws02                                       192.168.31.33      52:54:00:8A:C9:A7
        stage_v6_redis01                                         192.168.31.65      52:54:00:E3:AC:88
                                                                 192.168.31.67      52:54:00:E3:AC:88
        stage_v6_seo001                                          192.168.31.87      52:54:00:B8:3C:E9
        stage_v6_tradew02                                        192.168.31.55      52:54:00:62:18:78
        stage_v6_webapp01                                        192.168.31.72      52:54:00:FF:E9:99
        stage_v6_webservice02                                    192.168.31.51      52:54:00:BD:6F:02
        stage_v6_zookeeper02                                     192.168.31.61      52:54:00:77:79:32
hypervisor_v6_test2:hyper-YH-Virtualization-KVM04-192.168.38.200.xinxindai.com
        test_fkoracle                                            192.168.38.156     52:54:00:08:9C:96
        oracle_test                                              192.168.38.207     52:54:00:97:9D:E8
        ct-test-tomcat-partener01.xinxindai.com                  192.168.38.183     52:54:00:33:B6:2A
        dev_zookeeper                                            192.168.38.163     52:54:00:77:CD:29
        dev_zookeeper02                                          192.168.38.164     52:54:00:C8:70:1E
        dev_zookeeper03                                          192.168.38.165     52:54:00:E2:36:CC
        lvs01                                                    192.168.38.170     52:54:00:A9:0C:8B
                                                                 192.168.38.175     52:54:00:A9:0C:8B
        nginx01                                                  192.168.38.172     52:54:00:BC:A9:74
        redis01                                                  192.168.38.176     52:54:00:66:2B:5B
                                                                 192.168.38.178     52:54:00:66:2B:5B
        test_fk001                                               192.168.38.155     52:54:00:F9:FC:9C
        test_fk002                                               192.168.38.157     52:54:00:90:8C:C2
        test_fk_nginx                                            192.168.38.154     52:54:00:BE:56:9F
        test_v6_tomcat_mobile01                                  192.168.38.158     52:54:00:26:57:12
        tomcat_admin02                                           192.168.38.141     52:54:00:F2:CA:D6
        tomcat_batch02                                           192.168.38.147     52:54:00:5F:C9:A6
        tomcat_tran02                                            192.168.38.153     52:54:00:8E:CC:8C
        tomcat_webservice02                                      192.168.38.143     52:54:00:1D:EF:BB
        v6tomcat1                                                192.168.38.160     192.168.38.161     52:54:00:25:62:AF
        zookeeper01                                              192.168.38.180     52:54:00:67:00:4C
        test-v6_credit_app01-192.168.38.160.yh
        testmxli

hypervisor_v6_test1:hyper-YH-Virtualization-KVM03-192.168.38.190.xinxindai.com
        svnbackup                                                192.168.38.229     52:54:00:29:54:D7
        oracletech                                               192.168.38.219     52:54:00:12:B4:BA
        oracle01                                                 192.168.38.107     52:54:00:BB:FE:1F
        v6nfs                                                    192.168.38.182     52:54:00:72:C4:0E
        ct-test-tomcat-partener02.xinxindai.com                  192.168.38.184     52:54:00:86:6A:E8
        donglian                                                 192.168.38.218     52:54:00:C4:EF:A1
        lvs02                                                    192.168.38.171     52:54:00:C8:D7:0B
        monitor                                                  192.168.38.169     52:54:00:1F:EF:55
        nginx02                                                  192.168.38.173     52:54:00:20:B0:2A
        nginx_service01                                          192.168.38.148     52:54:00:EB:C5:1C
                                                                 192.168.38.152     52:54:00:EB:C5:1C
        nginx_service02                                          192.168.38.149     52:54:00:F6:AE:5E
        redis02                                                  192.168.38.177     52:54:00:C4:54:3F
        rewrite                                                  192.168.38.151     52:54:00:C6:50:3A
        test_v6_tomcat_mobile02                                  192.168.38.159     52:54:00:8B:AD:12
        test_v6_webapp_front01                                   192.168.38.206     52:54:00:BA:E0:C1
        test_v6_webapp_webservice01                              192.168.38.208     52:54:00:52:A3:9E
        tomcat_admin01                                           192.168.38.140     52:54:00:61:73:98
        tomcat_batch01                                           192.168.38.146     52:54:00:3B:AC:C3
        tomcat_tran01                                            192.168.38.144     52:54:00:F0:2E:83
        tomcat_webservice01                                      192.168.38.142     52:54:00:A0:F0:29
        v6tomcat2                                                192.168.38.162     52:54:00:47:BC:56
        zookeeper02                                              192.168.38.181     52:54:00:D4:16:A7
        saltstack                                                192.168.38.10      52:54:00:BB:32:1F

cloudera_test:ct-production-hadoop-dashboard_mysql.xinxindai.com
        cloudera-ace-ws
        hyper-hadoop-test
        cloudera_mysql                                           192.168.31.150     52:54:00:0D:11:2C
        fileshare-v6-logserver                                   192.168.31.155     52:54:00:8F:8C:2D
        h-hadoop-data03                                          192.168.31.164     52:54:00:77:F1:E3
        h_hadoop_dashboard                                       192.168.31.168     52:54:00:7C:86:61
        h_hadoop_data01                                          192.168.31.162     52:54:00:B9:81:1C
        h_hadoop_data02                                          192.168.31.163     52:54:00:5C:7A:35
        h_hadoop_master                                          192.168.31.160     52:54:00:80:C4:07
        h_hadoop_slave                                           192.168.31.161     52:54:00:DD:85:AC
平台_backup:hyper-YH-Virtualization-KVM02-192.168.38.100.xinxindai.com	
        git                                                      192.168.38.12      52:54:00:C3:4A:E8
        jumpserver                                               192.168.38.121     52:54:00:0F:D2:3F
        logstash                                                 192.168.38.11      52:54:00:AF:D3:C5
        luo_test                                                 192.168.38.205     52:54:00:84:8D:D9
        mysql_testdb                                             192.168.38.235     52:54:00:4E:29:EB
        openldap-master                                          192.168.38.120     52:54:00:6F:F8:EA
        saltweb                                                  192.168.38.122     52:54:00:FE:D3:6D
        v4_test001                                               192.168.38.232     52:54:00:A9:ED:5F
        v5_sc_mysql                                              192.168.38.111     52:54:00:96:E8:0C
        v5_stage001                                              192.168.38.145     52:54:00:71:0F:F9
                                                                 192.168.38.236     52:54:00:71:0F:F9
        redmine                                                  192.168.38.239     52:54:00:36:6B:DB
        YH-base-ansible01-192.168.38.13.xinxindai.com            192.168.38.13      52:54:00:2C:69:A1		
		
v6_fix:YH-Virtualization-KVM05-192.168.31.200.xinxindai.com
        test_lvs001                                              192.168.31.101     52:54:00:2A:9D:D6
        test_lvs002                                              192.168.31.100     52:54:00:AF:61:7A
                                                                 192.168.31.102     52:54:00:AF:61:7A
        v6mon_front002                                           192.168.31.107     52:54:00:AB:66:72
        v6mon_redies001                                          192.168.31.127     52:54:00:0C:44:51
        v6mon_redies002                                          192.168.31.126     52:54:00:2F:32:EC
                                                                 192.168.31.128     52:54:00:2F:32:EC
        v6mon_redies003                                          192.168.31.129     52:54:00:F2:7C:3C
        v6mon_zookeeper001                                       192.168.31.130     52:54:00:E2:B5:96
        test_nginx001                                            192.168.31.103     52:54:00:1A:5A:BB
        test_nginx002                                            192.168.31.104     52:54:00:BD:7D:A8
        v6_admin001                                              192.168.31.205     52:54:00:91:0E:04
        v6_batch001                                              192.168.31.213     52:54:00:35:85:8E
        v6_front001                                              192.168.31.203     52:54:00:7C:19:91
        v6_redies001                                             192.168.31.215     52:54:00:95:2D:35
        v6_trader001                                             192.168.31.211     52:54:00:88:20:B4
        v6_webservice001                                         192.168.31.209     52:54:00:6C:5B:1F
        v6_zookeeper001                                          192.168.31.217     52:54:00:76:D5:9A
        v6mon_admin001                                           192.168.31.109     52:54:00:B5:22:A8
        v6mon_batch001                                           192.168.31.124     52:54:00:84:95:2C
        v6mon_front001                                           192.168.31.106     52:54:00:32:75:37
        v6mon_mobile001                                          192.168.31.112     52:54:00:F9:DB:1F
        v6mon_trader001                                          192.168.31.121     52:54:00:91:DF:94
        v6mon_webapp001                                          192.168.31.115     52:54:00:B0:EE:09
        v6mon_webservice001                                      192.168.31.118     52:54:00:A7:CA:6F
        CT-YH-v6test2-credit_app01-192.168.31.206-xinxindai.com  192.168.31.206     52:54:00:D6:14:6B

平台:hyper-YH-Virtualization-KVM01-192.168.38.250.xinxindai.com
        windows_kaoqin                                            192.168.38.237  52:54:00:8D:A5:DA
        windows2008                                               192.168.38.238  52:54:00:5A:DB:DA
        maven                                                     192.168.38.240  52:54:00:E8:BC:49
        svn                                                       192.168.38.230  52:54:00:01:38:C0
        discuz001                                                 192.168.38.249  52:54:00:98:06:15
        dns                                                       192.168.38.118  52:54:00:35:A0:2C
        kaoqin                                                    192.168.38.127  52:54:00:80:B1:C7
        file001_sms   短信                                        192.168.38.209  52:54:00:AB:2D:7B
        cobbler                                                   192.168.38.119  52:54:00:81:15:ED

		
 find ./grains.stage_* | xargs -I {} sed  -i  "s/ci\/files\/stage_v6_admin/wujunrong\/xxd_configure\/stage\/v6\/admin/g"   {}
 find ./grains.stage_* | xargs -I {} sed  -i  "s/ci\/files\/stage_v6_batch/wujunrong\/xxd_configure\/stage\/v6\/batch/g"   {}        
 find ./grains.stage_* | xargs -I {} sed  -i  "s/ci\/files\/stage_v6_credit_app/wujunrong\/xxd_configure\/stage\/v6\/credit/g"   {}  
 find ./grains.stage_* | xargs -I {} sed  -i  "s/ci\/files\/stage_v6_front/wujunrong\/xxd_configure\/stage\/v6\/front/g"   {}        
 find ./grains.stage_* | xargs -I {} sed  -i  "s/ci\/files\/stage_v6_mobile/wujunrong\/xxd_configure\/stage\/v6\/mobile/g"   {}
 find ./grains.stage_* | xargs -I {} sed  -i  "s/ci\/files\/stage_v6_seo/wujunrong\/xxd_configure\/stage\/v6\/seo/g"   {}
 find ./grains.stage_* | xargs -I {} sed  -i  "s/ci\/files\/stage_v6_tradew/wujunrong\/xxd_configure\/stage\/v6\/tradews/g"   {}
 find ./grains.stage_* | xargs -I {} sed  -i  "s/ci\/files\/stage_v6_webapp/wujunrong\/xxd_configure\/stage\/v6\/webapp/g"   {}
 find ./grains.stage_* | xargs -I {} sed  -i  "s/ci\/files\/stage_v6_webservice/wujunrong\/xxd_configure\/stage\/v6\/webservice/g"   {}
 
 
 find ./grains.fix_* | xargs -I {} sed  -i  "s/ci\/files\/data_v6_admin/wujunrong\/xxd_configure\/fix\/v6\/admin/g"   {}
 find ./grains.fix_* | xargs -I {} sed  -i  "s/ci\/files\/data_v6_batch/wujunrong\/xxd_configure\/fix\/v6\/batch/g"   {}        
 find ./grains.fix_* | xargs -I {} sed  -i  "s/ci\/files\/data_v6_credit_app/wujunrong\/xxd_configure\/fix\/v6\/credit/g"   {}  
 find ./grains.fix_* | xargs -I {} sed  -i  "s/ci\/files\/data_v6_front/wujunrong\/xxd_configure\/fix\/v6\/front/g"   {}        
 find ./grains.fix_* | xargs -I {} sed  -i  "s/ci\/files\/data_v6_mobile/wujunrong\/xxd_configure\/fix\/v6\/mobile/g"   {}
 find ./grains.fix_* | xargs -I {} sed  -i  "s/ci\/files\/data_v6_seo/wujunrong\/xxd_configure\/fix\/v6\/seo/g"   {}
 find ./grains.fix_* | xargs -I {} sed  -i  "s/ci\/files\/data_v6_tradew/wujunrong\/xxd_configure\/fix\/v6\/tradews/g"   {}
 find ./grains.fix_* | xargs -I {} sed  -i  "s/ci\/files\/data_v6_webapp/wujunrong\/xxd_configure\/fix\/v6\/webapp/g"   {}
 find ./grains.fix_* | xargs -I {} sed  -i  "s/ci\/files\/data_v6_webservice/wujunrong\/xxd_configure\/fix\/v6\/webservice/g"   {}
 
 find ./grains.temp_* | xargs -I {} sed  -i  "s/ci\/files\/temp_v6_admin/wujunrong\/xxd_configure\/test2\/v6\/admin/g"   {}
 find ./grains.temp_* | xargs -I {} sed  -i  "s/ci\/files\/temp_v6_batch/wujunrong\/xxd_configure\/test2\/v6\/batch/g"   {}        
 find ./grains.temp_* | xargs -I {} sed  -i  "s/ci\/files\/temp_v6_credit_app/wujunrong\/xxd_configure\/test2\/v6\/credit/g"   {}  
 find ./grains.temp_* | xargs -I {} sed  -i  "s/ci\/files\/temp_v6_front/wujunrong\/xxd_configure\/test2\/v6\/front/g"   {}        
 find ./grains.temp_* | xargs -I {} sed  -i  "s/ci\/files\/temp_v6_mobile/wujunrong\/xxd_configure\/test2\/v6\/mobile/g"   {}
 find ./grains.temp_* | xargs -I {} sed  -i  "s/ci\/files\/temp_v6_seo/wujunrong\/xxd_configure\/test2\/v6\/seo/g"   {}
 find ./grains.temp_* | xargs -I {} sed  -i  "s/ci\/files\/temp_v6_tradew/wujunrong\/xxd_configure\/test2\/v6\/tradews/g"   {}
 find ./grains.temp_* | xargs -I {} sed  -i  "s/ci\/files\/temp_v6_webapp/wujunrong\/xxd_configure\/test2\/v6\/webapp/g"   {}
 find ./grains.temp_* | xargs -I {} sed  -i  "s/ci\/files\/temp_v6_webservice/wujunrong\/xxd_configure\/test2\/v6\/webservice/g"   {}
 
 
 find ./grains.perf_* | xargs -I {} sed  -i  "s/ci\/files\/temp_v6_admin/wujunrong\/xxd_configure\/test2\/v6\/admin/g"   {}
 find ./grains.perf_* | xargs -I {} sed  -i  "s/ci\/files\/temp_v6_batch/wujunrong\/xxd_configure\/test2\/v6\/batch/g"   {}        
 find ./grains.perf_* | xargs -I {} sed  -i  "s/ci\/files\/temp_v6_credit_app/wujunrong\/xxd_configure\/test2\/v6\/credit/g"   {}  
 find ./grains.perf_* | xargs -I {} sed  -i  "s/ci\/files\/temp_v6_front/wujunrong\/xxd_configure\/test2\/v6\/front/g"   {}        
 find ./grains.perf_* | xargs -I {} sed  -i  "s/ci\/files\/temp_v6_mobile/wujunrong\/xxd_configure\/test2\/v6\/mobile/g"   {}
 find ./grains.perf_* | xargs -I {} sed  -i  "s/ci\/files\/temp_v6_seo/wujunrong\/xxd_configure\/test2\/v6\/seo/g"   {}
 find ./grains.perf_* | xargs -I {} sed  -i  "s/ci\/files\/temp_v6_tradew/wujunrong\/xxd_configure\/test2\/v6\/tradews/g"   {}
 find ./grains.perf_* | xargs -I {} sed  -i  "s/ci\/files\/temp_v6_webapp/wujunrong\/xxd_configure\/test2\/v6\/webapp/g"   {}
 find ./grains.perf_* | xargs -I {} sed  -i  "s/ci\/files\/temp_v6_webservice/wujunrong\/xxd_configure\/test2\/v6\/webservice/g"   {}
 
 
 查询： ls grains.perf_* | xargs -I {} grep xxd_configure {}
        ls grains.perf_* | xargs -I {} sed -i "s/perf_/xn\//g"  {}
        ls grains.perf_* | xargs -I {} sed "s/v6_/v6\//g" {}
		
 查询连接到redis的服务器
 netstat  -antup | grep redis-server | grep -v LISTEN  | awk '{print $5}'|cut -d ":"  -f -1 | xargs -I {} grep {} /tmp/work_memo.txt
 
 
[root@centos7-laptop ~]# ansible product*tomcat  -m shell -a "mount" |grep -B 20 share   | grep "product\|share"
product_v6_pro01_v6_front01_tomcat | SUCCESS | rc=0 >>
192.168.60.34:/vx/XINXINDAI_FS/share on /share type nfs (rw,addr=192.168.60.34)
product_v6_pro02_v6_front02_tomcat | SUCCESS | rc=0 >>
192.168.60.34:/vx/XINXINDAI_FS/share on /share type nfs (rw,addr=192.168.60.34)
product_v6_pro03-v6_front03_tomcat | SUCCESS | rc=0 >>
192.168.60.34:/vx/XINXINDAI_FS/share on /share type nfs (rw,addr=192.168.60.34)
product_v6_pro01_v6_webservice01_tomcat | SUCCESS | rc=0 >>
192.168.60.34:/vx/XINXINDAI_FS/share on /share type nfs (rw,addr=192.168.60.34)
product_v6_pro02_v6_webservice02_tomcat | SUCCESS | rc=0 >>
192.168.60.34:/vx/XINXINDAI_FS/share on /share type nfs (rw,addr=192.168.60.34)
product_v6_pro03-v6_webservice03_tomcat | SUCCESS | rc=0 >>
192.168.60.34:/vx/XINXINDAI_FS/share on /share type nfs (rw,addr=192.168.60.34)
product-v6_pro01_v6_admin01_tomcat | SUCCESS | rc=0 >>
192.168.60.34:/vx/XINXINDAI_FS/share on /share type nfs (rw,addr=192.168.60.34)
product-v6_pro02_v6_admin02_tomcat | SUCCESS | rc=0 >>
192.168.60.34:/vx/XINXINDAI_FS/share on /share type nfs (rw,addr=192.168.60.34


scp *  root@192.168.38.10:/opt/ci/packages


=================================================
[yxaccount@salt ~]$  sudo  salt -L  "$minion_id"    cmd.run "mount | grep 60.34 ;cat /etc/salt/grains|grep 'imag' "
v6_nginx01:
    192.168.60.34:/vx/XINXINDAI_FS/sitemap on /usr/local/nginx/html/sitemap type nfs (rw,vers=3,addr=192.168.60.34)
    192.168.60.34:/vx/XINXINDAI_FS/download on /usr/local/nginx/html/static/download type nfs (rw,vers=3,addr=192.168.60.34)
    192.168.60.34:/vx/XINXINDAI_FS/v6_admin/image on /usr/local/nginx/html/static/admin/image type nfs (rw,vers=3,addr=192.168.60.34)
    192.168.60.34:/vx/XINXINDAI_FS/v6_front/image on /usr/local/nginx/html/static/image type nfs (rw,vers=3,addr=192.168.60.34)
    image_dir: /usr/local/nginx/html/static/image
    admin_image_dir: /usr/local/nginx/html/static/admin/image
    storage_image_dir: /volume1/v6/v6_front/image
    storage_admin_image_dir: /volume1/v6/v6_admin/image
v6_nginx02:
    192.168.60.34:/vx/XINXINDAI_FS/sitemap on /usr/local/nginx/html/sitemap type nfs (rw,vers=3,addr=192.168.60.34)
    192.168.60.34:/vx/XINXINDAI_FS/download on /usr/local/nginx/html/static/download type nfs (rw,vers=3,addr=192.168.60.34)
    192.168.60.34:/vx/XINXINDAI_FS/v6_admin/image on /usr/local/nginx/html/static/admin/image type nfs (rw,vers=3,addr=192.168.60.34)
    192.168.60.34:/vx/XINXINDAI_FS/v6_front/image on /usr/local/nginx/html/static/image type nfs (rw,vers=3,addr=192.168.60.34)
    image_dir: /usr/local/nginx/html/static/image
    admin_image_dir: /usr/local/nginx/html/static/admin/image
    storage_image_dir: /volume1/v6/v6_front/image
    storage_admin_image_dir: /volume1/v6/v6_admin/image
v6_front02:
    192.168.60.34:/vx/XINXINDAI_FS/share on /share type nfs (rw,addr=192.168.60.34)
    192.168.60.34:/vx/XINXINDAI_FS/v6_front/image on /opt/webserver/v6_tomcat/webapps/ROOT/static/image type nfs (rw,vers=3,addr=192.168.60.34)
    192.168.60.34:/vx/XINXINDAI_FS/v6_admin/image on /opt/webserver/v6_tomcat/webapps/ROOT/static/admin/image type nfs (rw,vers=3,addr=192.168.60.34)
    image_dir: /opt/webserver/v6_tomcat/webapps/ROOT/static/image
    admin_image_dir: /opt/webserver/v6_tomcat/webapps/ROOT/static/admin/image
    storage_image_dir: /volume1/v6/v6_front/image
    storage_admin_image_dir: /volume1/v6/v6_admin/image
v6_webservice01:
    192.168.60.34:/vx/XINXINDAI_FS/share on /share type nfs (rw,addr=192.168.60.34)
v6_webservice03:
    192.168.60.34:/vx/XINXINDAI_FS/share on /share type nfs (rw,addr=192.168.60.34)
v6_tradew02:
v6_tradew01:
v6_front03:
    192.168.60.34:/vx/XINXINDAI_FS/share on /share type nfs (rw,addr=192.168.60.34)
    192.168.60.34:/vx/XINXINDAI_FS/v6_front/image on /opt/webserver/v6_tomcat/webapps/ROOT/static/image type nfs (rw,vers=3,addr=192.168.60.34)
    192.168.60.34:/vx/XINXINDAI_FS/v6_admin/image on /opt/webserver/v6_tomcat/webapps/ROOT/static/admin/image type nfs (rw,vers=3,addr=192.168.60.34)
    image_dir: /opt/webserver/v6_tomcat/webapps/ROOT/static/image
    admin_image_dir: /opt/webserver/v6_tomcat/webapps/ROOT/static/admin/image
    storage_image_dir: /volume1/v6/v6_front/image
    storage_admin_image_dir: /volume1/v6/v6_admin/image
v6_webservice02:
    192.168.60.34:/vx/XINXINDAI_FS/share on /share type nfs (rw,addr=192.168.60.34)
v6_admin02:
    192.168.60.34:/vx/XINXINDAI_FS/share on /share type nfs (rw,addr=192.168.60.34)
    192.168.60.34:/vx/XINXINDAI_FS/v6_admin/images on /opt/webserver/v6_tomcat/webapps/xxdai_sys_admin/images type nfs (rw,vers=3,addr=192.168.60.34)
    192.168.60.34:/vx/XINXINDAI_FS/v6_admin/image on /opt/webserver/v6_tomcat/webapps/xxdai_sys_admin/image type nfs (rw,vers=3,addr=192.168.60.34)
    admin_image_dir: /opt/webserver/v6_tomcat/webapps/xxdai_sys_admin/image
    admin_images_dir: /opt/webserver/v6_tomcat/webapps/xxdai_sys_admin/images
    storage_admin_image_dir: /volume1/v6/v6_admin/image
    storage_admin_images_dir: /volume1/v6/v6_admin/images
v6_admin01:
    192.168.60.34:/vx/XINXINDAI_FS/share on /share type nfs (rw,addr=192.168.60.34)
    192.168.60.34:/vx/XINXINDAI_FS/v6_admin/images on /opt/webserver/v6_tomcat/webapps/xxdai_sys_admin/images type nfs (rw,vers=3,addr=192.168.60.34)
    192.168.60.34:/vx/XINXINDAI_FS/v6_admin/image on /opt/webserver/v6_tomcat/webapps/xxdai_sys_admin/image type nfs (rw,vers=3,addr=192.168.60.34)
    admin_image_dir: /opt/webserver/v6_tomcat/webapps/xxdai_sys_admin/image
    admin_images_dir: /opt/webserver/v6_tomcat/webapps/xxdai_sys_admin/images
    storage_admin_image_dir: /volume1/v6/v6_admin/image
    storage_admin_images_dir: /volume1/v6/v6_admin/images
v6_front01:
    192.168.60.34:/vx/XINXINDAI_FS/share on /share type nfs (rw,addr=192.168.60.34)
    192.168.60.34:/vx/XINXINDAI_FS/v6_front/image on /opt/webserver/v6_tomcat/webapps/ROOT/static/image type nfs (rw,vers=3,addr=192.168.60.34)
    192.168.60.34:/vx/XINXINDAI_FS/v6_admin/image on /opt/webserver/v6_tomcat/webapps/ROOT/static/admin/image type nfs (rw,vers=3,addr=192.168.60.34)
    image_dir: /opt/webserver/v6_tomcat/webapps/ROOT/static/image
    admin_image_dir: /opt/webserver/v6_tomcat/webapps/ROOT/static/admin/image
    storage_image_dir: /volume1/v6/v6_front/image
    storage_admin_image_dir: /volume1/v6/v6_admin/image
v6_batch02:
    192.168.60.34:/vx/XINXINDAI_FS/sitemap on /static/data/sitemap type nfs (rw,addr=192.168.60.34)
v6_tradew03:
v6_batch01:
    192.168.60.34:/vx/XINXINDAI_FS/sitemap on /static/data/sitemap type nfs (rw,addr=192.168.60.34)
v6_nginx03:
    192.168.60.34:/vx/XINXINDAI_FS/sitemap on /usr/local/nginx/html/sitemap type nfs (rw,vers=3,addr=192.168.60.34)
    192.168.60.34:/vx/XINXINDAI_FS/download on /usr/local/nginx/html/static/download type nfs (rw,vers=3,addr=192.168.60.34)
    192.168.60.34:/vx/XINXINDAI_FS/v6_admin/image on /usr/local/nginx/html/static/admin/image type nfs (rw,vers=3,addr=192.168.60.34)
    192.168.60.34:/vx/XINXINDAI_FS/v6_front/image on /usr/local/nginx/html/static/image type nfs (rw,vers=3,addr=192.168.60.34)
    image_dir: /usr/local/nginx/html/static/image
    admin_image_dir: /usr/local/nginx/html/static/admin/image
    storage_image_dir: /volume1/v6/v6_front/image
    storage_admin_image_dir: /volume1/v6/v6_admin/image
v6_webapp01:
    192.168.60.34:/vx/XINXINDAI_FS/v6_front/image on /opt/webserver/v6_tomcat/webapps/m/static/image type nfs (rw,vers=3,addr=192.168.60.34)
    image_dir: /opt/webserver/v6_tomcat/webapps/m/static/image
    storage_image_dir: /volume1/v6/v6_front/image
	
v6_mobile01:
    192.168.60.34:/vx/XINXINDAI_FS/v6_front/image on /opt/webserver/v6_tomcat/webapps/v5_mobile/static/image type nfs (rw,vers=3,addr=192.168.60.34)
    192.168.60.34:/vx/XINXINDAI_FS/v6_admin/image on /opt/webserver/v6_tomcat/webapps/v5_mobile/static/admin/image type nfs (rw,vers=3,addr=192.168.60.34)
    image_dir: /opt/webserver/v6_tomcat/webapps/v5_mobile/static/image
    admin_image_dir: /opt/webserver/v6_tomcat/webapps/v5_mobile/static/admin/image
    storage_image_dir: /volume1/v6/v6_front/image
    storage_admin_image_dir: /volume1/v6/v6_admin/image
+--------------------+-----------+
| Database           | Size (MB) |
+--------------------+-----------+
| airpal             |      0.80 |
| amon               |      4.61 |
| dashboard          |      2.94 |
| dashboard_dev      |    351.49 |
| dashboard_stage    |   2851.89 |
| dashboard_test     |   3047.10 |
| hive               |     55.41 |
| hue                |      4.42 |
| information_schema |      0.01 |
| mysql              |      0.66 |
| oozie              |     21.05 |
| performance_schema |      0.00 |
| scm                |     24.31 |
| zoo                |    544.58 |
+--------------------+-----------+

mvn deploy:deploy-file  -DgroupId=com.xxdai  -DartifactId=xxdai-utils  -Dversion=1.0   -DgeneratePom=false  -Dpackaging=jar -DrepositoryId=nexus "-Durl=http://192.168.31.178:8081/repository/xxdai_release"   -DpomFile=/tmp/xxdai_release/com/xxdai/xxdai-utils/1.0/xxdai-utils-1.0.pom   -Dfile=/tmp/xxdai_release/com/xxdai/xxdai-utils/1.0/xxdai-utils-1.0.jar
mvn deploy:deploy-file  -DgroupId=com.xxdai  -DartifactId=v6_flow  -Dversion=1.0   -DgeneratePom=false  -Dpackaging=jar -DrepositoryId=nexus "-Durl=http://192.168.31.178:8081/repository/xxdai_release"   -DpomFile=/tmp/xxdai_release/com/xxdai/v6_flow/1.0/v6_flow-1.0.pom   -Dfile=/tmp/xxdai_release/com/xxdai/v6_flow/1.0/v6_flow-1.0.jar
mvn deploy:deploy-file  -DgroupId=com.xxdai  -DartifactId=xxdai-appframe  -Dversion=1.2   -DgeneratePom=false  -Dpackaging=jar -DrepositoryId=nexus "-Durl=http://192.168.31.178:8081/repository/xxdai_release"   -DpomFile=/tmp/xxdai_release/com/xxdai/xxdai-appframe/1.2/xxdai-appframe-1.2.pom   -Dfile=/tmp/xxdai_release/com/xxdai/xxdai-appframe/1.2/xxdai-appframe-1.2.jar
mvn deploy:deploy-file  -DgroupId=com.xxdai  -DartifactId=xxdai-dbutils  -Dversion=1.0   -DgeneratePom=false  -Dpackaging=jar -DrepositoryId=nexus "-Durl=http://192.168.31.178:8081/repository/xxdai_release"   -DpomFile=/tmp/xxdai_release/com/xxdai/xxdai-dbutils/1.0/xxdai-dbutils-1.0.pom  -Dfile=/tmp/xxdai_release/com/xxdai/xxdai-dbutils/1.0/xxdai-dbutils-1.0.jar
mvn deploy:deploy-file  -DgroupId=com.xxdai  -DartifactId=oa_dynamicode    -Dversion=1.3  -DgeneratePom=false  -Dpackaging=jar -DrepositoryId=nexus "-Durl=http://192.168.31.178:8081/repository/xxdai_release"   -DpomFile=/tmp/xxdai_release/com/xxdai/oa_dynamicode/1.3/oa_dynamicode-1.3.pom    -Dfile=/tmp/xxdai_release/com/xxdai/oa_dynamicode/1.3/oa_dynamicode-1.3.jar
mvn deploy:deploy-file  -DgroupId=com.oracle  -DartifactId=ojdbc6_g    -Dversion=11.2  -DgeneratePom=false  -Dpackaging=jar -DrepositoryId=nexus "-Durl=http://192.168.31.178:8081/repository/xxdai_release"   -DpomFile=/tmp/xxdai_release/com/oracle/ojdbc6_g/11.2/ojdbc6_g-11.2.pom    -Dfile=/tmp/xxdai_release/com/oracle/ojdbc6_g/11.2/ojdbc6_g-11.2.jar
mvn deploy:deploy-file  -DgroupId=com.cloopen  -DartifactId=ccp_rest_sdk_java    -Dversion=2.6  -DgeneratePom=false  -Dpackaging=jar -DrepositoryId=nexus "-Durl=http://192.168.31.178:8081/repository/xxdai_release"   -DpomFile=/tmp/xxdai_release/com/cloopen/ccp_rest_sdk_java/2.6/ccp_rest_sdk_java-2.6.pom    -Dfile=/tmp/xxdai_release/com/cloopen/ccp_rest_sdk_java/2.6/ccp_rest_sdk_java-2.6.jar


mvn deploy:deploy-file  -DgroupId=com.xxdai  -DartifactId=xxdai-utils  -Dversion=1.1-SNAPSHOT   -DgeneratePom=false  -Dpackaging=jar -DrepositoryId=nexus "-Durl=http://192.168.31.178:8081/repository/xxdai_snapshot"   -DpomFile=/tmp/xxdai_snapshot/com/xxdai/xxdai-utils/1.1-SNAPSHOT/xxdai-utils-1.1.pom   -Dfile=/tmp/xxdai_snapshot/com/xxdai/xxdai-utils/1.1-SNAPSHOT/xxdai-utils-1.1.jar
mvn deploy:deploy-file  -DgroupId=com.xxdai  -DartifactId=xxdai-dbutils      -Dversion=1.1-SNAPSHOT   -DgeneratePom=false  -Dpackaging=jar -DrepositoryId=nexus "-Durl=http://192.168.31.178:8081/repository/xxdai_snapshot"   -DpomFile=/tmp/xxdai_snapshot/com/xxdai/xxdai-dbutils/1.1-SNAPSHOT/xxdai-dbutils-1.1-20150629.080248-1.pom   -Dfile=/tmp/xxdai_snapshot/com/xxdai/xxdai-dbutils/1.1-SNAPSHOT/xxdai-dbutils-1.1-20150629.080248-1.jar
mvn deploy:deploy-file  -DgroupId=com.xxdai  -DartifactId=xxdai-appframe    -Dversion=1.2.5-SNAPSHOT   -DgeneratePom=false  -Dpackaging=jar -DrepositoryId=nexus "-Durl=http://192.168.31.178:8081/repository/xxdai_snapshot"   -DpomFile=/tmp/xxdai_snapshot/com/xxdai/xxdai-appframe/1.2.5-SNAPSHOT/xxdai-appframe-1.2.5-20160405.100429-5.pom   -Dfile=/tmp/xxdai_snapshot/com/xxdai/xxdai-appframe/1.2.5-SNAPSHOT/xxdai-appframe-1.2.5-20160405.100429-5.jar
 mvn deploy:deploy-file  -DgroupId=com.xxdai  -DartifactId=xxdai-appframe    -Dversion=1.2.4-SNAPSHOT   -DgeneratePom=false  -Dpackaging=jar -DrepositoryId=nexus "-Durl=http://192.168.31.178:8081/repository/xxdai_snapshot"   -DpomFile=/tmp/xxdai_snapshot/com/xxdai/xxdai-appframe/1.2.4-SNAPSHOT/xxdai-appframe-1.2.4-20160105.055001-3.pom    -Dfile=/tmp/xxdai_snapshot/com/xxdai/xxdai-appframe/1.2.4-SNAPSHOT/xxdai-appframe-1.2.4-20160105.055001-3.jar
 
 
mvn deploy:deploy-file  -DgroupId=com.xxdai  -DartifactId=v6_flow    -Dversion=1.1-SNAPSHOT   -DgeneratePom=false  -Dpackaging=jar -DrepositoryId=nexus "-Durl=http://192.168.31.178:8081/repository/xxdai_snapshot"   -DpomFile=/tmp/xxdai_snapshot/com/xxdai/v6_flow/1.1-SNAPSHOT/v6_flow-1.1-20150629.080333-1.pom    -Dfile=/tmp/xxdai_snapshot/com/xxdai/v6_flow/1.1-SNAPSHOT/v6_flow-1.1-20150629.080333-1.jar

curl -v -u admin:admin123 --upload-file  /tmp/xxdai_snapshot/com/xxdai/v6_parent/1.2.4-SNAPSHOT/v6_parent-1.2.4-SNAPSHOT.pom  http://192.168.31.178:8081/repository/xxdai_snapshot/com/xxdai/v6_parent/1.2.4-SNAPSHOT/v6_parent-1.2.4-SNAPSHOT.pom
curl -v -u admin:admin123 --upload-file  /tmp/xxdai_snapshot/com/xxdai/v6_parent/1.2.5-SNAPSHOT/v6_parent-1.2.5-20160504.054526-3.pom   http://192.168.31.178:8081/repository/xxdai_snapshot/com/xxdai/v6_parent/1.2.5-SNAPSHOT/v6_parent-1.2.5-SNAPSHOT.pom
curl -v -u admin:admin123 --upload-file  ./xxdai_snapshot/com/xxdai/v6_parent/1.2.3-SNAPSHOT/v6_parent-1.2.3-20151222.022835-1.pom    http://192.168.31.178:8081/repository/xxdai_snapshot/com/xxdai/v6_parent/1.2.3-SNAPSHOT/v6_parent-1.2.3-SNAPSHOT.pom

curl -v -u admin:admin123 --upload-file  /tmp/xxdai_release/com/xxdai/v6_third_lib/1.1/v6_third_lib-1.1.pom  http://192.168.31.178:8081/repository/xxdai_release/com/xxdai/v6_third_lib/1.1/v6_third_lib-1.1.pom
curl -v -u admin:admin123 --upload-file  /tmp/xxdai_release/com/xxdai/v6_third_lib/1.0/v6_third_lib-1.0.pom    http://192.168.31.178:8081/repository/xxdai_release/com/xxdai/v6_third_lib/1.0/v6_third_lib-1.0.pom


rsync  -avh   ./admin/          192.168.110.102:/srv/salt/xinxindai_tomcat/files/v6_admin/        
rsync  -avh   ./batch/          192.168.110.102:/srv/salt/xinxindai_tomcat/files/v6_batch/        
rsync  -avh   ./front/          192.168.110.102:/srv/salt/xinxindai_tomcat/files/v6_front/        
rsync  -avh   ./partener/       192.168.110.102:/srv/salt/xinxindai_tomcat/files/v6_partener/     
rsync  -avh   ./tradews/        192.168.110.102:/srv/salt/xinxindai_tomcat/files/v6_tradews/      
rsync  -avh   ./webservice/     192.168.110.102:/srv/salt/xinxindai_tomcat/files/v6_webservice/   


硬件信息：
ansible qinghua -m raw -a " psrinfo -pv| grep  2200|head -n 1 | tr -d  '\n' ; prtconf | grep Memory"  | grep -v facebook | grep -v ^$
ansible qinghua -f 1 -m raw -a " psrinfo -pv| grep  2200|head -n 1 "  | grep -v face|   grep -v ^$
内存：ansible qinghua -f 1 -m raw -a " prtconf | grep Memory "  | grep -v face|   grep -v ^$
ansible qinghua -f 1 -m raw -a " format < /dev/null"

ipmitool -I lan -H 10.75.1.210 -U USERID -P admin power status



zfs list
NAME                                               USED  AVAIL  REFER  MOUNTPOINT
zones                                              217G  10.2T   585K  /zones
zones/677fd345-2719-e219-e81c-96e36468ebaf        60.8K  10.0G  60.8K  /zones/677fd345-2719-e219-e81c-96e36468ebaf
zones/677fd345-2719-e219-e81c-96e36468ebaf-disk0    10G  10.3T  1.71G  -


===========smartos section leofs section==========


            tee /opt/smartos_vm.json <<-'EOF'            
            {
             "brand": "joyent",
             "image_uuid": "13f711f4-499f-11e6-8ea6-2b9fb858a619",
             "alias": "pxe",
             "hostname": "pxe",
             "max_physical_memory": 1024,
             "quota": 50,
             "resolvers": ["172.17.1.10", "114.114.114.114"],
             "nics": [
                {
                     "nic_tag": "admin",
                     "ips": ["dhcp"]
                }
             ],
             "internal_metadata": {
              "root_pw": "root",
              "admin_pw": "admin"
              },
             "customer_metadata": {
                 "user-script" : "/opt/local/bin/sed -i.bak 's/PermitRootLogin without-password/PermitRootLogin yes/g'   /etc/ssh/sshd_config; /usr/sbin/svcadm restart svc:/network/ssh:default"

              }
            }			
EOF
            vmadm  create -f /opt/smartos_vm.json
            
````````````````````````````


imgadm sources -d  https://images.joyent.com
imgadm sources -a  http://images.briphant.com
imgadm update
imgadm import 1bd84670-055a-11e5-aaa2-0346bb21d5a1
imgadm list | grep 1bd84670-055a-11e5-aaa2-0346bb21d5a1



/opt/centos-lx-brand-image-builder/create-lx-image -t  `ls /opt/centos-lx-brand-image-builder/*.gz`  -k 3.13.0 -m 20160117T201601Z -i test-lx-centos-7.2 -d "CentOS 7.2 64-bit lx-brand image." -u https://docs.joyent.com/images/container-native-linux


./install -d /data/chroot -m http://mirror.centos.org/centos/7/os/x86_64/Packages/ -r centos-release-7-2.1511.el7.centos.2.10.x86_64.rpm  -i test-lx-centos-7.2 -p "CentOS 7.2 LX Brand" -D "CentOS 7.2 64-bit lx-brand image." -u https://docs.joyent.com/images/container-native-linux
```````````````````````````
cd /opt
vi leo-zone1.json
vi leo-zone2.json
vmadm create -f leo-zone1.json
vmadm create -f leo-zone2.json

```````````````````
zlogin <leo-zone1-uuid>

curl -O https://project-fifo.net/fifo.gpg
gpg --primary-keyring /opt/local/etc/gnupg/pkgsrc.gpg --import < fifo.gpg
gpg --keyring /opt/local/etc/gnupg/pkgsrc.gpg --fingerprint


`````````
用官方的源：
VERSION=rel
cp /opt/local/etc/pkgin/repositories.conf /opt/local/etc/pkgin/repositories.conf.original
echo "http://release.project-fifo.net/pkg/${VERSION}" >> /opt/local/etc/pkgin/repositories.conf
用公司内部源：
步骤一：
vi /opt/local/etc/pkgin/repositories.conf
#http://pkgsrc.joyent.com/packages/SmartOS/2014Q4/x86_64/All

步骤二：
echo "http://pkgs.briphant.com/pkgsrc-fifo/pkg/rel"                      >>/opt/local/etc/pkgin/repositories.conf
echo "http://pkgs.briphant.com/pkgsrc-joyent/SmartOS/2014Q4/x86_64/All"  >>/opt/local/etc/pkgin/repositories.conf



pkgin -fy up
pkgin install coreutils sudo gawk gsed
pkgin install leo_manager leo_gateway leo_storage
```````
配置软件源
vi /opt/local/etc/pkgin/repositories.conf
#http://pkgsrc.joyent.com/packages/SmartOS/2014Q4/x86_64/All
#http://release.project-fifo.net/pkg/rel
http://pkgs.briphant.com/pkgsrc-joyent/SmartOS/2014Q4/x86_64/All
http://pkgs.briphant.com/pkgsrc-fifo/pkg/rel

````````leofs manager`````````
vi /opt/local/leo_manager/etc/leo_manager.conf
nodename = manager0@10.1.1.21
manager.mode = master
distributed_cookie = QvTnSK0vrCohKMkw
manager.partner = manager1@10.1.1.22
consistency.num_of_replicas = 1
consistency.write = 1
consistency.read = 1
consistency.delete = 1



Start Manager A: 
svcadm enable epmd
svcadm enable leofs/manager

````````leofs gateway`````

vi /opt/local/leo_gateway/etc/leo_gateway.conf

distributed_cookie = QvTnSK0vrCohKMkw
managers = [manager0@10.1.1.21, manager1@10.1.1.22]
http.port = 80
http.ssl_port     = 443


Start the Gateway:
svcadm enable leofs/gateway
leofs-adm status


```leofs storage```````
vi /opt/local/leo_storage/etc/leo_storage.conf
distributed_cookie = QvTnSK0vrCohKMkw
managers = [manager0@10.1.1.21, manager1@10.1.1.22]

Start Manager B: 
svcadm enable epmd
svcadm enable leofs/manager

`````
Start Storage :
Zlogin to Zone 1:

svcadm enable leofs/storage
leofs-adm status
leofs-adm start

`````
Starting the LeoFS Cluster:
sniffle-admin init-leofs 10.1.1.21.xip.io


svcadm clear svc:/leofs/gateway:default
svcadm clear svc:/leofs/storage:default
svcadm clear svc:/leofs/manager:default

svcadm enable svc:/leofs/manager:default
svcadm enable svc:/leofs/gateway:default
svcadm enable svc:/leofs/storage:default

svcadm restart svc:/leofs/manager:default
svcadm restart svc:/leofs/storage:default
svcadm restart svc:/leofs/gateway:default

svcs -p leofs/manager:default
svcs -p leofs/storage:default

======fifo section============================
imgadm sources -d  https://images.joyent.com
imgadm sources -a  http://images.briphant.com
cd /opt/
vi setupfifo.json
vmadm create -f setupfifo.json

zlogin 92d63038-6fdf-c957-f161-ae22fbd9600c
zfs set mountpoint=/data zones/$(zonename)/data

cd /data
curl -O https://project-fifo.net/fifo.gpg
gpg --primary-keyring /opt/local/etc/gnupg/pkgsrc.gpg --import < fifo.gpg
gpg --keyring /opt/local/etc/gnupg/pkgsrc.gpg --fingerprint


```````
配置软件源
vi /opt/local/etc/pkgin/repositories.conf
#http://pkgsrc.joyent.com/packages/SmartOS/2014Q4/x86_64/All
#http://release.project-fifo.net/pkg/rel
http://pkgs.briphant.com/pkgsrc-joyent/SmartOS/2014Q4/x86_64/All
http://pkgs.briphant.com/pkgsrc-fifo/pkg/rel

`````````````````

pkgin -fy up
pkgin install fifo-snarl fifo-sniffle fifo-howl fifo-cerberus

svcadm enable epmd
svcadm enable snarl
svcadm enable sniffle
svcadm enable howl
svcs epmd snarl sniffle howl

`````````````````````
snarl-admin init default MyOrg Users admin admin
   Created user 'admin' with id 0135d508-3b19-4b15-8827-8d14cea5af21.
   Granted full permissions to admin.
   Set password for to admin.
   Created org 'MyOrg' with id 03500be5-f49a-42b4-be8d-86382e6c02c1.
   Granted permission on org MyOrg to admin.
   Joined admin to MyOrg.
   Selected MyOrg as active org for admin.
   Added default role Users (02b5176f-ddca-408f-8806-3434b440a66b).
   Added 'Everything' scope and set it default.
``````````````````
                leofs-adm status
在fifo服务器上：sniffle-admin init-leofs 10.75.1.202.xip.io

[root@fifo-wu ~]# sniffle-admin init-leofs 10.75.1.208.xip.io
Created user fifo:
Access Key: 5265c1c9d6fb400c0a2f
Secret Key: 8dab3a24d909da8a152d5e5a212a39d9d0b7ea24
Created bucket general bucket: fifo
Created bucket image bucket: fifo-images
Created bucket snapshot bucket: fifo-backups
Configuring endpoint as: https://10.75.1.208.xip.io:10020

leofs-adm start


=====Chunter section============

VERSION=rel
cd /opt
curl -O http://release.project-fifo.net/gz/${VERSION}/fifo_zlogin-latest.gz
gunzip fifo_zlogin-latest.gz
sh fifo_zlogin-latest


VERSION=rel
cd /opt
curl -O http://release.project-fifo.net/gz/${VERSION}/chunter-latest.gz
gunzip chunter-latest.gz
sh chunter-latest
```````````

虚拟机迁移：
vmadm list
vmadm stop 03d08f22-b1a0-6494-a58c-f1b769db8cee
vmadm stop 0b9bccdf-7b8f-e01e-bd92-cc59e651357e
zfs snapshot zones/03d08f22-b1a0-6494-a58c-f1b769db8cee@sending
zfs snapshot zones/0b9bccdf-7b8f-e01e-bd92-cc59e651357e@sending

vmadm get 03d08f22-b1a0-6494-a58c-f1b769db8cee > wujunrong-leofs01.json
vmadm get 0b9bccdf-7b8f-e01e-bd92-cc59e651357e > wujunrong-leofs02.json

zfs send -p  zones/03d08f22-b1a0-6494-a58c-f1b769db8cee@sending> wujunrong-leofs01.zfs
zfs send -p  zones/0b9bccdf-7b8f-e01e-bd92-cc59e651357e@sending> wujunrong-leofs02.zfs


zfs receive zones/03d08f22-b1a0-6494-a58c-f1b769db8cee  < ./wujunrong-leofs01.zfs 
zfs receive zones/0b9bccdf-7b8f-e01e-bd92-cc59e651357e  < ./wujunrong-leofs02.zfs

scp *.zfs root@10.75.1.109:/opt/  scp *.json  root@10.75.1.109:/opt/
``````````smartos pxe section`````````````
#先创建虚拟机
            tee /opt/smartos_vm.json <<-'EOF'            
            {
             "brand": "joyent",
             "image_uuid": "1bd84670-055a-11e5-aaa2-0346bb21d5a1",
             "alias": "pxe",
             "hostname": "pxe",
             "max_physical_memory": 1024,
             "quota": 50,
             "resolvers": ["172.17.1.10", "114.114.114.114"],
             "nics": [
                {
                        "nic_tag": "admin",
                        "ip": "172.16.0.2",
                        "netmask": "255.255.255.0",
                        "dhcp_server": "1"，
                        "primary": true
                }
             ],
             "internal_metadata": {
              "root_pw": "root",
              "admin_pw": "admin"
              },
             "customer_metadata": {
                 "user-script" : "/opt/local/bin/sed -i.bak 's/PermitRootLogin without-password/PermitRootLogin yes/g'   /etc/ssh/sshd_config; /usr/sbin/svcadm restart svc:/network/ssh:default"

              }
            }			
EOF
            vmadm  create -f /opt/smartos_vm.json
#vmadm get  4aa92153-f816-465a-f590-a23e0e01994e |json nics
#vmadm update 4aa92153-f816-465a-f590-a23e0e01994e <<EOF
#     {
#       "update_nics": [
#         {
#           "mac": "12:1d:2d:8e:9f:60",
#           "nic_tag": "storage",
#           "ip": "172.16.0.2",
#           "netmask": "255.255.255.0",
#           "gateway": "172.16.0.1",
#           "dhcp_server": "1"
#
#         }
#       ]
#     }
#EOF

```````````````````vmadm update section ````````````````
vmadm update 5fa67481-5e29-4a40-833e-d374a74f17d0 <<EOF
     {
       "update_nics": [
         {
           "mac": "a2:54:fa:53:74:fd",
           "nic_tag": "admin",
           "ip": "dhcp",
           "primary": true
         }
       ]
     }
EOF

vmadm update 5fa67481-5e29-4a40-833e-d374a74f17d0 <<EOF
 {
  "customer_metadata": {
    "user-script": "/usr/bin/sed -i.bak 's/PermitRootLogin without-password/PermitRootLogin yes/g'   /etc/ssh/sshd_config; /usr/sbin/svcadm restart svc:/network/ssh:default"
  }
 }
EOF

````````
#tftp

pkgin -fy up
pkgin -y install tftp-hpa
mkdir /tftpboot
echo "tftp dgram udp wait root /opt/local/sbin/in.tftpd in.tftpd -s /tftpboot" > /tmp/tftp.inetd
svcadm enable inetd
inetconv -i /tmp/tftp.inetd -o /tmp
svccfg import /tmp/tftp-udp.xml
svcadm restart tftp/udp

#dnsmasq
pkgin -y install dnsmasq
mv /opt/local/etc/dnsmasq.conf /opt/local/etc/dnsmasq.conf.bak
tee /opt/local/etc/dnsmasq.conf <<-'EOF'
   #Lease File to track leases
   dhcp-leasefile=/etc/dnsmasq.leases
   # Give out addresses in the 172.16.0.1/24 subnet
   dhcp-range=192.168.1.100,192.168.1.115,255.255.255.0,2h
   # The name of the boot file is pxegrub
   dhcp-boot=pxegrub
EOF
svcadm enable dnsmasq



pkgin -y install wget

wget  -O  /tftpboot/pxegrub    http://192.168.1.128/file-share/pxelinux/pxegrub
#或wget  -O  /tftpboot/pxegrub     http://aszeszo.googlepages.com/pxegrub
   #在centos上 执行
   #wget  -O  /root/latest.iso http://192.168.1.128/file-share/smartos-20160721T174418Z.iso
   #mount -o loop  /root//latest.iso    /root/mnt/

scp -r  xx.xx.xx.xx:/root/mnt/*    /tftpboot/
＃说明：xx.xx.xx.xx是centos的ip 地址
#问题定位，关闭dnsmaq服务，用dnsmasq调试模式：svcadm disable dnsmasq ;/opt/local/sbin/dnsmasq -C /opt/local/etc/dnsmasq.conf -d

``````````````````
#rsync -av /mnt/ .  会生成boot/目录和platform/目录，其中boot/包含 boot/grub/menu.lst，menu.lst使用/tftpboot目录

hash -r
cd /tftpboot
mkdir /mnt
LOFI=$(lofiadm -a /tftpboot/latest.iso)
mount -F hsfs $LOFI /mnt
rsync -av /mnt/ .     
umount /mnt
lofiadm -d $LOFI
``````````````````````````````


mkdir -p /tftpboot/smartos
cd /tftpboot/smartos
wget http://192.168.1.232/file-share/pxelinux/ldlinux.c32
wget http://192.168.1.232/file-share/pxelinux/libcom32.c32
wget http://192.168.1.232/file-share/pxelinux/mboot.c32


````````pxelinux section``````
centos:
yum -y install syslinux
mkdir -p /var/lib/tftpboot/pxelinux/pxelinux.cfg
mkdir -p /var/lib/tftpboot/pxelinux/smartos/

sudo tee /etc/dnsmasq.d/smartos.conf<<-'EOF'
interface=enp0s8
dhcp-range=10.0.1.190,10.0.1.200,6h
#dhcp-host=80:00:27:c6:a1:16,10.0.0.253,svr1,infinite
dhcp-boot=pxelinux/pxelinux.0
enable-tftp
tftp-root=/var/lib/tftpboot
EOF

cp /usr/share/syslinux/pxelinux.0 /var/lib/tftpboot/pxelinux/pxelinux.0

wget -O  /var/lib/tftpboot/pxelinux/smartos/ldlinux.c32   http://salt/file-share/pxelinux/ldlinux.c32
wget -O  /var/lib/tftpboot/pxelinux/smartos/libcom32.c32  http://salt/file-share/pxelinux/libcom32.c32
wget -O  /var/lib/tftpboot/pxelinux/smartos/mboot.c32     http://salt/file-share/pxelinux/mboot.c32

scp -r ./platform/   10.0.1.101:/var/lib/tftpboot/pxelinux/

sudo tee  /var/lib/tftpboot/pxelinux/pxelinux.cfg/default<<-'EOF'
default smartos
prompt 1
timeout 50
 
label smartos
kernel smartos/mboot.c32
append smartos/platform/i86pc/kernel/amd64/unix -B smartos=true,console=text,root_shadow='$5$2HOHRnK3$NvLlm.1KQBbB0WjoP7xcIwGnllhzp2HnT.mDO7DpxYA' --- smartos/platform/i86pc/amd64/boot_archive
 
label smartosrescue
kernel smartos/mboot.c32
append smartos/platform/i86pc/kernel/amd64/unix -B smartos=true,console=text,standalone=true,noimport=true,root_shadow='$5$2HOHRnK3$NvLlm.1KQBbB0WjoP7xcIwGnllhzp2HnT.mDO7DpxYA' --- smartos/platform/i86pc/amd64/boot_archive
EOF

说明：/var/lib/tftpboot/pxelinux/pxelinux.cfg/default 配置文件中需要的mboot.c32文件，此文件版本pxelinux.0的版本必须一致，都是从syslinux-4.05.tar.gz中取出。
[root@NAT_Server ~]# sudo yum list installed| grep syslinux
syslinux.x86_64                      4.05-13.el7                    @base       
[root@NAT_Server ~]# wget https://www.kernel.org/pub/linux/utils/boot/syslinux/syslinux-4.05.tar.gz
[root@NAT_Server ~]# cp ./syslinux-4.05/com32/mboot/mboot.c32    /var/lib/tftpboot/pxelinux/smartos/
[root@NAT_Server ~]# cp /usr/share/syslinux/pxelinux.0 /var/lib/tftpboot/pxelinux/pxelinux.0

```ipxe section```````````````

iPXE> ifstat
iPXE> dhcp
iPXE> route
iPXE> show dns
iPXE> chain http://boot.ipxe.org/demo/boot.php

svcadm restart  dnsmasq
简单方案:
wget -O  /tftpboot/undionly.kpxe   http://cuddletech.com/IPXE-100612_undionly.kpxe
wget -O  /tftpboot/undionly.kpxe   http://boot.ipxe.org/undionly.kpxe
wget -O  /tftpboot/smartos/platform-latest.tgz http://salt/iso/platform-20170216T015949Z.tgz
tar zxvf  ./platform-latest.tgz 
tee smartos.ipxe <<-'EOF' 
#!ipxe
# /var/lib/tftpboot/smartos.ipxe.tpl
kernel /smartos/platform-20170216T015949Z/i86pc/kernel/amd64/unix -B smartos=true
initrd /smartos/platform-20170216T015949Z/i86pc/amd64/boot_archive
boot
EOF

复杂方案：

tee /opt/smartos_pxe.sh <<-'EOF' 
set -e
mkdir -p  /tftpboot/smartos
cd /tftpboot/smartos
#curl https://us-east.manta.joyent.com/Joyent_Dev/public/SmartOS/platform-latest.tgz > /var/tmp/platform-latest.tgz
#(Just now URL https://download.joyent.com/pub/iso/platform-latest.tgz is invalid, 404… )
wget -O /tftpboot/smartos/platform-latest.tgz http://salt/iso/platform-20170216T015949Z.tgz

tar zxvf  ./platform-latest.tgz 
directory=`ls | grep platform- | sort | tail -n1`
release=${directory:9}
mv $directory $release
cd $release
mkdir platform
mv i86pc platform
cd /tftpboot
cat smartos.ipxe.tpl | sed -e"s/\$release/$release/g" > smartos.ipxe
EOF



`````````pxelinux section`````````
scp ./bios/com32/mboot/mboot.c32 ./bios/com32/lib/libcom32.c32 ./bios/com32/elflink/ldlinux/ldlinux.c32  root@172.16.15.3:/tftpboot/smartos

===============smartos dsapid section============
imgadm sources -a  http://datasets.at
imgadm sources -d  https://images.joyent.com
imgadm import a0e719d6-4e21-11e4-92eb-2bf6399552e7
tee /opt/dsapid.json <<-'EOF'
    { 
      "brand": "joyent", 
      "image_uuid": "a0e719d6-4e21-11e4-92eb-2bf6399552e7", 
      "autoboot": true, 
      "alias": "dsapid-server",
      "hostname": "datasets.dsapid", 
      "resolvers": [ "114.114.114.114", "8.8.4.4" ], 
      "max_physical_memory": 1024, 
      "max_swap": 1024,
      "tmpfs": 1024, 
      "quota": 120, 
      "nics": [ { "nic_tag": "admin", "ip": "10.75.1.75", "netmask": "255.255.255.0", "gateway": "10.75.1.1","primary": true } ]
    }
EOF
 vmadm create -f /opt/dsapid.json

`````````
vi /data/config.json
{
    "log_level": "info",
    "base_url": "http://10.20.1.4/",
    "mount_ui": "/opt/dsapid/ui",
    "listen": {
        "http": {
            "address": "0.0.0.0:80",
            "ssl": false
        }
    },
    "datadir": "/data/files",
    "users": "/data/users.json",
    "sync": [
        {
            "name": "official joyent dsapi",
            "active": false,
            "type": "dsapi",
            "provider": "joyent",
            "source": "https://datasets.joyent.com/datasets",
            "delay": "24h"
        },
        {
            "name": "official joyent imgapi",
            "active": false,
            "type": "imgapi",
            "provider": "joyent",
            "source": "https://images.joyent.com/images",
            "delay": "24h"
        },
        {
            "name": "datasets.at repository",
            "active": true,
            "type": "dsapi",
            "provider": "community",
            "source": "http://datasets.at/datasets",
            "delay": "12h"
        }
    ]
}

```````````````
svcadm disable dsapid
svcadm enable dsapid
svcs -p dsapid
/opt/dsapid/server   -config /data/config.json -log_level debug 


sed -i.bak 's/PermitRootLogin without-password/PermitRootLogin yes/g'   /etc/ssh/sshd_config
sed -i.bak 's/PasswordAuthentication no/PasswordAuthentication yes/g'   /etc/ssh/sshd_config
svcadm restart svc:/network/ssh:default

ssh-keygen -t rsa 
scp ~/.ssh/id_rsa.pub    10.75.1.109:/root/.ssh/authorized_keys


在本地添加用户
curl -v -X PUT http://127.0.0.1:80/api/users -d '{ "name": "admin", "token": "admintoken", "type": "user", "roles": [ "s_dataset.upload", "s_dataset.manage", "s_dataset.admin" ] }'
从远程上传dataset
curl -v -u admintoken: http://10.75.1.202:80/api/upload -F manifest=@./manifest.json  -Ffile=@./base-64-lts-15.4.1.zfs.gz -v

=======git section======

git config --global http.sslVerify false
git clone https://github.com/joyent/mibe
cat sync_file_list.txt |xargs -I{} sh -c 'echo scp `basename {}` root@10.0.1.38:{}'  > sync_file.sh
=====pkgin section=======
cd /
curl -k http://192.168.1.128/smartos/bootstrap-2016Q3-x86_64.tar.gz | gzcat | tar -xf -
pkg_admin rebuild
pkgin -y up


salt '*'  cmd.run 'tail  -2 /opt/local/etc/pkgin/repositories.conf'  shell='/usr/bin/bash'
==========vmadm section=======

tee /opt/ansible.jason <<-'EOF'

{
 "brand": "joyent",
 "image_uuid": "13f711f4-499f-11e6-8ea6-2b9fb858a619",
 "alias": "config-management-server",
 "hostname": "ansible-master",
 "max_physical_memory": 1024,
 "quota": 50,
 "resolvers": ["114.114.114.114", "8.8.4.4"],
 "nics": [
    {
      "nic_tag": "admin",
      "ips": ["dhcp"]
    }
 ],
 "internal_metadata": {
  "root_pw": "root",
  "admin_pw": "admin"
  }
}

EOF
vmadm  create -f /opt/ansible.jason
=======smartos command========fifo command=================
vmadm list  | grep wujunrong-leofs | awk '{print $1 }' |xargs -I {} vmadm delete {}
vmadm list  | grep wujunrong-leofs | awk '{print $1 }' |xargs -I {} vmadm reboot {}
vmadm list  | grep alpha_test_leofs | awk '{print $1 }' |xargs -I {} vmadm delete {}
vmadm list  | grep bb50e0d0  | awk '{print $1 }' |xargs -I {} sh -c 'vmadm update {}   indestructible_delegated=false;vmadm delete {}'
vmadm list | grep old |awk '{print $1}' |xargs -I{} vmadm stop {}
address=`/opt/salt/bin/appdata/salt-2016.3.3.solaris-2_11-i86pc_64bit/salt-call --out=json  network.ip_addrs 2>&1 | grep -v WAR | json local[0]`

========smartos salt========================
ansible facebook_machine10 -m raw -a "cd /opt/ && wget http://192.168.1.232/file-share/salt-2016.3.3-esky-smartos.tar.gz"

ansible facebook_machine10 -m raw -a "echo '10.75.1.70 salt'>>/etc/hosts;cd /opt && tar   zxvf   salt-2016.3.3-esky-smartos.tar.gz && /opt/salt/install/install.sh && svcadm enable salt-minion"
====ssh section======
/home/deployer/.ssh/id_rsa:
  file.managed:
    - user: deployer
    - group: deployer
    - mode: 600
    - contents_pillar: userdata:deployer:id_rsa
    
This would populate /home/deployer/.ssh/id_rsa with the contents of pillar['userdata']['deployer']['id_rsa']. An example of this pillar setup would be like so:

userdata:
  deployer:
    id_rsa: |
        -----BEGIN RSA PRIVATE KEY-----
        MIIEowIBAAKCAQEAoQiwO3JhBquPAalQF9qP1lLZNXVjYMIswrMe2HcWUVBgh+vY
        U7sCwx/dH6+VvNwmCoqmNnP+8gTPKGl1vgAObJAnMT623dMXjVKwnEagZPRJIxDy
        B/HaAre9euNiY3LvIzBTWRSeMfT+rWvIKVBpvwlgGrfgz70m0pqxu+UyFbAGLin+
        GpxzZAMaFpZw4sSbIlRuissXZj/sHpQb8p9M5IeO4Z3rjkCP1cxI
        -----END RSA PRIVATE KEY-----
===============
#base-package:
#  service.running:
#    - name: salt:minion
#    - enable: True
#    - require:
#       - cmd: install_mustang
=====salt section====
{% if module_property.type == 'zone-dataset' and module_property.salt_target == salt['grains.get']('fqdn', '') %}
salt  smartos_thinkpad.zhixiang    state.sls_id    upload_repository_dataset_test_mustang    generate_dataset   -t 3600
======
nat，修改thinkpad上的网卡网关，指向centos nat
vmadm update  d272317f-72c5-6ddb-e84d-c439b6de53ba<<EOF
{
         "update_nics": [
             {
               "interface": "net0",
               "mac": "62:53:89:f4:61:94",
               "nic_tag": "storage",
               "gateway": "192.168.2.203",
               "gateways": [
                 "192.168.2.203"
               ],
               "netmask": "255.255.255.0",
               "ip": "192.168.2.51",
               "ips": [
                 "192.168.2.51/24"
               ],
               "primary": true
             }

       ]
  
  
}
EOF





smartos 修改kvm磁盘大小
zfs get volsize zones/${UUID}-disk0 
zfs set volsize=80g  zones/${UUID}-disk0 
vmadm get ${UUID} 
vmadm list
vmadm stop ${UUID} 
vmadm boot ${UUID}  order=cd,once=d cdrom=/opt/gparted-live-0.28.1-1-amd64.iso,ide
ls /opt/gparted-live-0.28.1-1-amd64.iso
cd /zones/${UUID}/root
cp /opt/gparted-live-0.28.1-1-amd64.iso   .
vmadm boot ${UUID}  order=cd,once=d cdrom=/gparted-live-0.28.1-1-amd64.iso,ide
vmadm info ${UUID}  vnc


salt jinhao state.sls_id   create_alpha_test_leofs1_vm        create_smartos_vm  -t 600
salt jinhao state.sls_id   create_alpha_test_leofs2_vm        create_smartos_vm  -t 600
salt jinhao state.sls_id   dataset_install_alpha_test_leofs1    config_smartos_vm -t 3600
salt jinhao state.sls_id   dataset_install_alpha_test_leofs2    config_smartos_vm -t 3600

salt jinhao  state.sls_id    alpha_test_leofs1_config_master     config_leofs_vm
salt jinhao  state.sls_id    alpha_test_leofs2_config_slave      config_leofs_vm

#leofs1:
salt alpha_test_leofs1 cmd.run 'svcadm enable epmd'
salt alpha_test_leofs1 cmd.run 'svcadm enable leofs/manager'
salt alpha_test_leofs1 cmd.run 'svcs   leofs/manager'

#leofs2:
salt alpha_test_leofs2 'svcadm enable epmd'
salt alpha_test_leofs2 'svcadm enable leofs/manager'
salt alpha_test_leofs2 'svcs   leofs/manager'

#leofs1:
salt alpha_test_leofs1 cmd.run 'svcadm enable leofs/storage'
salt alpha_test_leofs1 cmd.run 'svcs   leofs/storage'
sleep 10
salt alpha_test_leofs1 cmd.run 'leofs-adm status'
salt alpha_test_leofs1 cmd.run 'leofs-adm start'

salt alpha_test_leofs1 cmd.run 'svcadm enable leofs/gateway'
salt alpha_test_leofs1 cmd.run 'svcs   leofs/gateway'
sleep 10
salt alpha_test_leofs1 cmd.run 'leofs-adm status'


rsync -avzh 192.168.1.148:/home/frank/docker-share/  /home/frank/docker-share/  --checksum
==========smartos salt section====  
gloable zone 装salt：
方法一：  
echo 10.0.1.38 salt>>/etc/hosts
cd /
curl -k http://salt/smartos/bootstrap-2016Q3-x86_64.tar.gz | gzcat | tar -xf -

pkg_admin rebuild
#pkgin -y up

cd /opt/
wget http://salt/file-share/salt-2016.3.3-esky-smartos.tar.gz   
cd /opt && tar   zxvf   salt-2016.3.3-esky-smartos.tar.gz && /opt/salt/install/install.sh && svcadm enable salt-minion
svcadm restart salt-minion

````
方法二：同时安装pkg和salt
echo 10.0.1.38 salt>>/etc/hosts;cd /;curl -k http://salt/smartos/bootstrap-2016Q3-x86_64.tar.gz | gzcat | tar -xf -;pkg_admin rebuild;cd /opt/;wget http://salt/file-share/salt-2016.3.3-esky-smartos.tar.gz   ;cd /opt ; tar   zxvf   salt-2016.3.3-esky-smartos.tar.gz ;/opt/salt/install/install.sh && svcadm enable salt-minion;svcadm restart salt-minion
通过ansible安装salt
ansible hp  -f 1 -m raw -a "echo 10.0.1.38 salt>>/etc/hosts;cd /;curl -k httpts;cd /;curl -k http://salt/smartos/bootstrap-2016Q3-x86_64.tar.gz | gzcat |/ tar -xf -;pkg_admin rebuild;cd /opt/;wget http://salt/file-share/salt-2016.3.3-esky-smartos.tar.gz   ;cd /opt ; tar   zxvf   salt-2016.3.3-esky-smartos.tar.gz ;/opt/salt/install/install.sh && svcadm enable salt-minion;svcadm restart salt-minion"

``````
after globle zone reboot 重启后
ansible hp  -f 1 -m raw -a "echo 10.0.1.38 salt>>/etc/hosts;svcadm restart salt-minion"
````salt-key section `````
ansible hp   -m raw -a  'echo hp> /opt/salt/etc/minion_id;cat /opt/salt/etc/minion_id;svcadm restart salt-minion'
==========fifo section==
salt -L  'home_fifo,home_fifo2'   cmd.run      'svcadm restart snarl '
salt -L  'home_fifo,home_fifo2'   cmd.run      'svcadm restart sniffle '
salt -L  'home_fifo,home_fifo2'   cmd.run      'svcadm restart howl'
salt -L  'home_fifo,home_fifo2'   cmd.run      'svcadm restart howl;svcadm restart sniffle;svcadm restart snarl'
salt 'ci_cloud_beijing_*' cmd.run "svcs epmd snarl sniffle howl"
salt ci_cloud_beijing_*   cmd.run  "svcadm disable snarl; svcadm disable howl;svcadm disable sniffle"
salt ci_cloud_beijing_*   cmd.run  "svcadm enable snarl; svcadm enable howl;svcadm enable sniffle"


pkg_add: Can't process http://pkgsrc.joyent.com:80/packages/SmartOS/2014Q4/x86_64/All/perl*: Connection timed out
pkg_add: no pkg found for 'perl>=5.0', sorry.
pkg_add: Can't install dependency perl>=5.0
pkg_add: 1 package addition failed
pkg_add: Can't process http://pkgsrc.joyent.com:80/packages/SmartOS/2014Q4/x86_64/All/perl*: Connection timed out

Running install with PRE-INSTALL for perl-5.20.1.
Package perl-5.20.1 registered in /opt/local/pkg/perl-5.20.1
pkg_add: Can't process http://pkgsrc.joyent.com:80/packages/SmartOS/2014Q4/x86_64/All/unixodbc*: Connection timed out
pkg_add: no pkg found for 'unixodbc>=2.0.11nb3', sorry.
pkg_add: Can't install dependency unixodbc>=2.0.11nb3
pkg_add: 1 package addition failed


`````logrotate section````
每小时切割日志，服务器端配置
/var/log/xinxindai/*catalina.out
/var/log/xinxindai/*tomcat_access.out
{
        dateext
        dateformat -%Y-%m-%d-%s
        extension .out 
        nocompress
        missingok
        copytruncate
        size 512k
        sharedscripts
        postrotate
               find /var/log/xinxindai  -type f -exec sh -c 'second=$(echo {}|  sed  -r   -n  "s/.*-([0-9]+)\.out/\1/p") && \
                             [ ! -z "$second" ] && my_second=$(date  -d @$second +"%H_%M_%S")  && \
                             new_file_name=$(echo {}|sed  -r  "s/([0-9]+)\.out/$my_second.log/g") && \
                             mv {}   $new_file_name '  \;
               find /var/log/xinxindai  -mtime 1 -regex  '.*[0-9][0-9]\.log'   -print -exec /bin/rm -fr {} \; 
        endscript

}

vmadm list | grep home_leofs| awk '{print $1}' |xargs -I {} vmadm reboot {}


===sed section==curl section=
curl -s  http://pkgs.briphant.com/pkgsrc-fifo.bak/pkg/rel/ |  sed -r -n 's/.*(fifo.*howl.*0.8.2.*.tgz).*/\1/p'

===smartos kvm migration====
方法一：
UUID=873f7b84-d078-62ab-d069-ce5760072253
vmadm get ${UUID} | json zfs_filesystem disks

vmadm get ${UUID} >/opt/salt.json
cat /opt/salt.json |json -e "for(i in this.disks){this.disks[i].nocreate=true;this.disks[i].block_size=undefined;}this.i=undefined;" >/opt/salt.json
cat /opt/salt.json |json -e "for(i in this.disks){this.disks[i].nocreate=true;this.disks[i].block_size=undefined;}this.i=undefined;"| ssh 10.0.2.159 vmadm create

zfs snapshot zones/${UUID}@sending
zfs send -p zones/${UUID}@sending > /opt/salt.zfs
zfs destroy zones/${UUID}@sending

zfs snapshot zones/${UUID}-disk0@sending
zfs send -p  zones/${UUID}-disk0@sending > /opt/salt-disk0.zfs
zfs destroy zones/${UUID}-disk0@sending
scp /opt/{*.zfs,salt.json}  10.0.1.174:/opt/


UUID=873f7b84-d078-62ab-d069-ce5760072253
zfs receive zones/${UUID} < /opt/salt.zfs
zfs receive zones/${UUID}-disk0 < /opt/salt-disk0.zfs

zfs destroy zones/873f7b84-d078-62ab-d069-ce5760072253@sending
zfs destroy zones/873f7b84-d078-62ab-d069-ce5760072253-disk0@sending


方法二：
UUID=873f7b84-d078-62ab-d069-ce5760072253
zfs snapshot zones/${UUID}@sending
zfs snapshot zones/${UUID}-disk0@sending
zfs send zones/${UUID}-disk0@sending | ssh 10.20.2.159  zfs recv zones/${UUID}-disk0
zfs send zones/${UUID}@sending       | ssh 10.20.2.159  zfs recv zones/${UUID}
····
UUID=873f7b84-d078-62ab-d069-ce5760072253;zfs destroy zones/${UUID}@sending;zfs destroy zones/${UUID}-disk0@sending
scp /opt/{salt.json,salt-disk0.zfs,salt.zfs} 10.0.1.99:/run/media/frank/
``````

修改一：
First edit the *.json file to include “nocreate”: true, in the “disks” subset:

Example:

  "disks": [
    {
      "path": "/dev/zvol/rdsk/zones/47d84c19-2eae-48c6-a976-109a4c9b9db2-disk0",
      "nocreate": true,  
修改二：
diff /opt/salt.json /opt/salt.json.bak 
40a41
>       "block_size": 8192

自动化修改：
cat /opt/salt.json |json -e "for(i in this.disks){this.disks[i].nocreate=true;this.disks[i].block_size=undefined;}this.i=undefined;"| vmadm create
````````
#vmadm get $UUID | json -e "for(i in this.disks){this.disks[i].nocreate=true;}this.i=undefined;" | ssh destination-host vmadm create

vmadm create < /opt/salt.json
vmadm start ${UUID} 



============smartos network section=========
换网卡，并安装fifo

vmadm update 5fa67481-5e29-4a40-833e-d374a74f17d0 <<EOF
     {
       "update_nics": [
         {
           "mac": "a2:54:fa:53:74:fd",
           "nic_tag": "admin",
           "ip": "dhcp",
           "primary": true
         }
       ]
     }
EOF

find / -path  "/opt/local/fifo-*/etc/*.conf" |xargs -I {} sed -i.bak 's/172.99.6.136/10.0.1.19/g'  {}

svcadm clear snarl             
svcadm clear sniffle
svcadm clear howl 

svcadm enable epmd
svcadm enable snarl
svcadm enable sniffle
svcadm enable howl
sleep 60
svcs epmd snarl sniffle howl
sniffle-admin init-leofs 10.0.1.130.xip.io
svcadm restart sniffle
````````
cp /opt/local/fifo-sniffle/etc/sniffle.conf /data/sniffle/etc/sniffle.conf
把/data/sniffle/etc/sniffle.conf中的/data/sniffle/db的全部替换为/var/db/sniffle

====lakala section=======
ansible  hyper_server_for_zhixiang -m raw -a "vmadm list|  /usr/xpg4/bin/grep -E 'abc'| awk '{print \$1}'|xargs -I{} zlogin {} ifconfig "
/opt/local/bin/sed -i.bak 's/PermitRootLogin without-password/PermitRootLogin yes/g'   /etc/ssh/sshd_config; /usr/sbin/svcadm restart svc:/network/ssh:default
```````lakala snapshot ```````````

ansible  global*fifo*  -m raw -a "vmadm list| grep orchestra |awk '{print \$1}' |xargs -I {} zfs snapshot zones/{}@fifo-levedb-repair-20170620-backup"
ansible  global*fifo*  -m raw -a "vmadm list| grep orchestra |awk '{print \$1}' |xargs -I {} zfs list -t snapshot |grep repair"

salt -C 'cloud* and not cloud18'   cmd.run "vmadm list| grep ci_cloud_beijing_  |awk '{print \$1}' |xargs -I {} zfs snapshot zones/{}@zhixiang-cloud-upgrad-snapshot"
salt -C 'cloud* and not cloud18'   cmd.run "vmadm list| grep ci_cloud_beijing_ |awk '{print \$1}' |xargs -I {} zfs list -t snapshot |grep zhix"
````````lakala leveldb repair``````
find /var/db/sniffle/  -path *1233142006497949337234359077604363797834693083136*
修复脚本
/opt/local/fifo-sniffle/erts-7.0/bin/erl

[application:set_env(eleveldb, Var, Val) || {Var, Val} <- [{max_open_files, 2000}, {block_size, 1048576}, {cache_size, 20*1024*1024*1024}, {sync, false}, {data_root, ""}]].
eleveldb:repair("/var/db/sniffle/<vnode_number>", []).
eleveldb:repair("/var/db/sniffle/1039036320289938793410432185759232459286639542272", []).


svcadm enable epmd;svcadm enable snarl; svcadm enable howl;svcadm enable sniffle
检查最后一条出现db_open的日志
ansible native_fifo_zone -m raw -a " grep db_open /var/log/sniffle/console.log |tail -1 "

`````````````lakala chunter```````````
 ansible global_33_dataset25_joyent2015   -m raw -a "ifconfig"
 ansible global_33_dataset25_joyent2015   -m raw -a "sed -i.bak s/127.0.0.1/10.15.11.33/g /opt/chunter/etc/chunter.conf"
 ansible global_33_dataset25_joyent2015   -m raw -a "diff /opt/chunter/etc/chunter.conf.example  /opt/chunter/etc/chunter.conf"
 ansible global_33_dataset25_joyent2015   -m raw -a "svcadm restart chunter"


 ansible fifo_briphant_orchestra117  -m raw -a  "sniffle-admin hypervisors list"

```lakala ssh````

fifo相关的虚拟机所在的global zone    ： global*fifo*
zhixiang相关的虚拟机所在的global zone： global_*_briphant_dedicated_pxe*_fifo*

ansible  hyper_pxe_lakala   -m raw -a "vmadm list|  /usr/xpg4/bin/grep -E 'abc'| awk '{print \$1}'|xargs -I{} zlogin {} grep PermitRootLogin /etc/ssh/sshd_config "
         global_*_briphant_dedicated_pxe*_fifo*
ansible  hyper_pxe_lakala   -m raw -a "vmadm list|  /usr/xpg4/bin/grep -E 'abc'| awk '{print \$1}'|xargs -I{} zlogin {} sed -n 's/PermitRootLogin without-password/PermitRootLogin yes/gp'   /etc/ssh/sshd_config "
ansible  hyper_pxe_lakala   -m raw -a "vmadm list|  /usr/xpg4/bin/grep -E 'abc'| awk '{print \$1}'|xargs -I{} zlogin {} sed -i.bak 's/PermitRootLogin without-password/PermitRootLogin yes/g'   /etc/ssh/sshd_config "

ansible  hyper_pxe_lakala   -m raw -a "vmadm list|  /usr/xpg4/bin/grep -E 'abc'| awk '{print \$1}'|xargs -I{} zlogin {} /usr/sbin/svcadm restart svc:/network/ssh:default"
修改密码
ansible *_fifo   -m raw -a "vmadm list|  /usr/xpg4/bin/grep -E 'abc'| awk '{print \$1}'|xargs -I{} zlogin {} sh -c 'sed -i.bak \"s/PermitRootLogin without-password/PermitRootLogin yes/g\"   /etc/ssh/sshd_config;echo \"root:456_lakala\"|changepass;  /usr/sbin/svcadm restart svc:/network/ssh:default' "


```lakala fifo````
ansible native_fifo_zone -m raw -a "snarl-admin member-status;howl-admin member-status;sniffle-admin member-status"
ansible native_fifo_zone -m raw -a "howl-admin status;sniffle-admin status;snarl-admin status;"
ansible native_fifo_zone -m raw -a "svcs epmd snarl sniffle howl"
ansible native_fifo_zone -m raw -a "prtconf | head -3 | grep Mem"

ansible native_fifo_zone -m raw -a "svcadm disable snarl; svcadm disable howl;svcadm disable sniffle;svcadm disable epmd"
ansible native_fifo_zone -m raw -a "svcadm enable epmd;svcadm enable snarl; svcadm enable howl;svcadm enable sniffle"


ansible global*fifo*        -m raw -a "vmadm list|  /usr/xpg4/bin/grep -E 'briphant_orchestra'| awk '{print \$1}'|xargs -I{} zlogin {} svcs sniffle snarl howl"
ansible hyper_server_lakala -m raw -a "uname -a"  | grep lakala |dos2unix |wc -l #查询主机数目

查fifo zone的内存负荷
ansible global*fifo*  -m raw -a "vmadm list|  /usr/xpg4/bin/grep -E 'briphant_orchestra'| awk '{print \$1}'" 2>&1  |grep -v global| grep -v -i connection|grep -E -v "^[[:space:]]*$" |xargs -I{} grep {} ./zonememstat_lakala.txt
ansible global*fifo*  -m raw -a "vmadm list|  /usr/xpg4/bin/grep -E 'briphant_orchestra'| awk '{print \$1}'" 2>&1  |grep -v global| grep -v -i connection|grep -E -v "^[[:space:]]*$" |dos2unix.exe|xargs -I{} grep {}  ./zonememstat_lakala.txt


ansible fifo_briphant_orchestra117  -m raw -a "fifoadm hypervisors list | grep lakala|sort -k1,1|awk '{print \$1}'|uniq -c"
`````lakala pxe`````
查询pxe zone（alias 名称包含booter）有无做过snapshot
ansible *pxe[1-4]_fifo -m raw -a "zfs list -t    snapshot | grep \`vmadm list| grep booter|awk '{print \$1}'\` "
ansible *pxe-briphant_booter[2-4]* -m raw -a "svcs dnsmasq"

为pxe zone（alias 名称包含booter）做快照：
ansible *pxe[1-4]_fifo -m raw -a " zfs snapshot zones/\`vmadm list| grep booter|awk '{print \$1}'\`@pxe_backup_20170519_by_zhixiang "

```lakala troubleshotting`````
kstat -n sd3,err
iostat -En

last | grep boot
who -b
look at /var/adm/messages file
fmdump

``````lakala log`````
grep -i "compaction"  /var/db/sniffle/*/*LOG*



=====yibaba section===
qemu-img create -f qcow2 /var/lib/libvirt/images/redmine.qcow2 102400M

sudo virt-install  -n tomcat  -r 4096 --disk=path=/var/lib/libvirt/images/tomcat.qcow2   --location ~/CentOS-7-x86_64-Minimal-1611.iso   --nographics  -w bridge:br0  --initrd-inject=/root/anaconda-ks.cfg.centos7 --extra-args "ks=file:/anaconda-ks.cfg.centos7  console=ttyS0,115200"



sudo virt-install  -n svn  -r 4096 --disk=path=/var/lib/libvirt/images/svn.qcow2   --location ~/CentOS-7-x86_64-Minimal-1611.iso   --nographics  -w bridge:br0  --initrd-inject=/root/anaconda-ks.cfg.centos7 --extra-args "ks=file:/anaconda-ks.cfg.centos7  console=ttyS0,115200 ksdevice=eth0 hostname=svn ip=192.168.1.192 netmask=255.255.255.0 dns=192.168.1.1 gateway=192.168.1.1"

sudo virt-install  -n redmine  -r 4096 --disk=path=/var/lib/libvirt/images/redmine.qcow2   --location ~/CentOS-7-x86_64-Minimal-1611.iso   --nographics  -w bridge:br0  --initrd-inject=/root/anaconda-ks.cfg.centos7 --extra-args "ks=file:/anaconda-ks.cfg.centos7  console=ttyS0,115200 ksdevice=eth0 ip=192.168.1.191 netmask=255.255.255.0 dns=192.168.1.1 gateway=192.168.1.1"

````yibaba kvm snapshot````

virsh list | grep run| grep -v wujunrong |awk  '{print  $2}' |xargs  -I{}  virsh snapshot-create-as --domain {}  --name "yibaba-everyday-backup-`date +%F`"   --description "yibaba"  --atomic
virsh list | grep run| grep -v wujunrong |awk  '{print  $2}' |xargs  -I{}  virsh snapshot-list  {}

yum -y install net-tools wget

cat > kvm_snapshot.xml<<EOF
<domainsnapshot>
 <name>agent:tomcat8-installed</name>
 <description>yibaba </description>
</domainsnapshot>
EOF

virsh snapshot-create    tomcat  --atomic kvm_snapshot.xml
virsh snapshot-create-as --domain tomcat --name "`date`-redis-cluster"   --description "yibaba"  --atomic

virsh snapshot-list  tomcat
````yibaba tomcat````
wget http://apache.claz.org/tomcat/tomcat-8/v8.0.44/bin/apache-tomcat-8.0.44.tar.gz
rpm  -ivh jdk-8u131-linux-x64.rpm
rpm -qa| grep jdk
rpm -ql jdk1.8.0_131-1.8.0_131-fcs.x86_64
tar zxvf  /root/apache-tomcat-8.0.44.tar.gz   -C /opt/

````````yibaba redis section````
HA 模式
echo slaveof 127.0.0.1 7000 >> /etc/redis/7001.conf
echo slaveof 127.0.0.1 7000 >> /etc/redis/7002.conf

设置可以远程访问：
protected-mode yes
#bind 127.0.0.1
..................
yibaba cluster
#注意使用redis-cli连到集群时必须有-c
grep cluster-ena  /etc/redis/70*.conf
sed -i  's/# cluster-enabled yes/cluster-enabled yes/'  700*.conf

grep cluster-config-file  /etc/redis/70*.conf
sed -i  's/# cluster-config-file nodes-6379.conf/cluster-config-file nodes-7001.conf/'  /etc/redis/7001.conf

grep cluster-node-timeout /etc/redis/700*.conf
sed -i 's/# cluster-node-timeout 15000/cluster-node-timeout 15000/' /etc/redis/700*.conf

grep ^appendonly /etc/redis/700*.conf
sed -i 's/^appendonly no/appendonly yes/'  /etc/redis/700*.conf

sudo yum install rubygems
gem install redis

/root/redis-4.0.0/src/redis-trib.rb  create --replicas 0 127.0.0.1:7000 127.0.0.1:7001 127.0.0.1:7002 127.0.0.1:7003
/usr/local/bin/redis-cli  -h 192.168.1.190 -p 7000 cluster info
/usr/local/bin/redis-cli  -h 192.168.1.190 -p 7000 cluster nodes
/usr/local/bin/redis-cli -h 192.168.1.190 -p 7000  -c  #注意集群必须有-c
......
重建cluster流程，先cluster forget，然后flushall，再 cluster reset soft,最后/root/redis-4.0.0/src/redis-trib.rb  create --replicas 0 192.168.1.190:7000 192.168.1.190:7001 192.168.1.190:7002

/usr/local/bin/redis-cli  -h 192.168.1.190 -p 7000 cluster nodes
/root/redis-4.0.0/src/redis-trib.rb  create --replicas 0 192.168.1.190:7000  192.168.1.190:7002 192.168.1.190:7003
/usr/local/bin/redis-cli  -h 192.168.1.190 -p 7001 cluster nodes
/usr/local/bin/redis-cli  -h 192.168.1.190 -p 7001 cluster forget 16d75d1e782f626009efcc08b30292578d2b6b5c
/usr/local/bin/redis-cli  -h 192.168.1.190 -p 7001 cluster forget 2b2e6b65db53acd6d32f2eeadf7b75a28bd2de7d
/usr/local/bin/redis-cli  -h 192.168.1.190 -p 7001 cluster forget caec2ba999dd7ad9fc85a9d2156d4d88ce6cb189
/usr/local/bin/redis-cli  -h 192.168.1.190 -p 7001 cluster nodes
/usr/local/bin/redis-cli  -h 192.168.1.190 -p 7002 cluster nodes
/usr/local/bin/redis-cli  -h 192.168.1.190 -p 7002 cluster forget 2b2e6b65db53acd6d32f2eeadf7b75a28bd2de7d
/usr/local/bin/redis-cli  -h 192.168.1.190 -p 7002 cluster forget caec2ba999dd7ad9fc85a9d2156d4d88ce6cb189
/usr/local/bin/redis-cli  -h 192.168.1.190 -p 7002 cluster forget 4a85499f6856da343760f08a5b6e311e9376e9e1
/usr/local/bin/redis-cli  -h 192.168.1.190 -p 7002 cluster nodes
/usr/local/bin/redis-cli  -h 192.168.1.190 -p 7003 cluster nodes
/usr/local/bin/redis-cli  -h 192.168.1.190 -p 7003 cluster forget  16d75d1e782f626009efcc08b30292578d2b6b5c
/usr/local/bin/redis-cli  -h 192.168.1.190 -p 7003 cluster forget  4a85499f6856da343760f08a5b6e311e9376e9e1
/usr/local/bin/redis-cli  -h 192.168.1.190 -p 7003 cluster forget  caec2ba999dd7ad9fc85a9d2156d4d88ce6cb189
/usr/local/bin/redis-cli  -h 192.168.1.190 -p 7003 cluster nodes

127.0.0.1:7000> CLUSTER FORGET caec2ba999dd7ad9fc85a9d2156d4d88ce6cb189
/usr/local/bin/redis-cli  -h 192.168.1.190 -p 7002 cluster forget 2b2e6b65db53acd6d32f2eeadf7b75a28bd2de7d
/usr/local/bin/redis-cli  -h 192.168.1.190 -p 7000  CLUSTER RESET SOFT
/usr/local/bin/redis-cli  -h 192.168.1.190 -p 7001  CLUSTER RESET SOFT
/usr/local/bin/redis-cli  -h 192.168.1.190 -p 7002  CLUSTER RESET SOFT
/usr/local/bin/redis-cli  -h 192.168.1.190 -p 7003  CLUSTER RESET SOFT
/usr/local/bin/redis-cli  -h 192.168.1.190 -p 7003  flushall
/usr/local/bin/redis-cli  -h 192.168.1.190 -p 7003  CLUSTER RESET SOFT
/root/redis-4.0.0/src/redis-trib.rb  create --replicas 0 192.168.1.190:7000 192.168.1.190:7001 192.168.1.190:7002
..................

/usr/local/bin/redis-cli  -h 192.168.1.190 -p 7000
/etc/init.d/redis_7001 restart
/etc/init.d/redis_7002 restart


`````````yibaba docker````````
docker exec -ti nginx_web_1 ls /var/log/nginx/
docker exec -t  nginx_web_1 cat /var/log/nginx/yibaba.error.log |less
docker exec -t nginx_web_1 cat /var/log/nginx/host.access.log  |less
```````````yibaba redmine section```````````
[root@redmine ~]# docker images
REPOSITORY          TAG                 IMAGE ID            CREATED             SIZE
redmine             latest              1ecbd8e5c2ba        13 days ago         641MB
mysql               latest              e799c7f9ae9c        5 weeks ago         407MB
``
docker run -d --name yibaba-redmine-mysql -e MYSQL_ROOT_PASSWORD=yibaba -e MYSQL_DATABASE=redmine mysql
docker run -d --name yibaba-redmine --link  yibaba-redmine-mysql:mysql  -p 8080:3000 redmine
````
docker   exec   yibaba-redmine   cat /usr/src/redmine/app/controllers/users_controller.rb  |less -N
docker   exec   yibaba-redmine   cat /usr/src/redmine/log/production.log  |less
````
  docker exec -it yibaba-redmine-mysql bash
  mysql -uroot -p
  SHOW VARIABLES LIKE  'char%';
  ALTER DATABASE redmine  CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci;
  redmine支持中文,需要一张张表改utf8
  mysql> use redmine
  mysql> ALTER TABLE issue_status        CONVERT TO CHARACTER SET utf8;
·····
在docker中修改/etc/mysql/conf.d/mysql.cnf 文件
root@b10449c35554:/# cp /root/mysql.cnf     /etc/mysql/conf.d/mysql.cnf           
root@b10449c35554:/# echo [mysqld] >> /etc/mysql/conf.d/mysql.cnf
root@b10449c35554:/# echo character-set-server=utf8 >> /etc/mysql/conf.d/mysql.cnf
root@b10449c35554:/# echo collation-server=utf8_general_ci  >>  /etc/mysql/conf.d/mysql.cnf 
root@b10449c35554:/# cat /etc/mysql/conf.d/mysql.cnf

在包含mysqld的那行前面添加default-character-set=utf8
sed -i.bak2 '/mysqld/ i \
default-character-set=utf8'  /etc/mysql/conf.d/mysql.cnf
`````
检查是否是utf-8
python
Python 2.7.5 (default, Sep 15 2016, 22:37:39) 
[GCC 4.8.5 20150623 (Red Hat 4.8.5-4)] on linux2
Type "help", "copyright", "credits" or "license" for more information.
>>> "\xE5\x86\x9B\xE8\x8D\xA3".decode("utf-8")


====zhixiang section=========

ansible liuzhen  -f 1 -m raw -a "echo 10.20.2.200 salt>>/etc/hosts;cd /;curl -k httts;cd /;curl -k http://salt/smartos/bootstrap-2016Q4-x86_64.tar.gz | gzcat |/ tar -xf -;pkg_admin rebuild;cd /opt/;wget http://salt/file-share/salt-2016.3.3-esky-smartos.tar.gz   ;cd /opt ; tar   zxvf   salt-2016.3.3-esky-smartos.tar.gz ;/opt/salt/install/install.sh && svcadm enable salt-minion;svcadm restart salt-minion"

/opt/salt/bin/salt-minion -l debug

sed -i.bak   's/#master: salt/master: salt/' /opt/salt/etc/minion
ansible liuzhen -m raw -a "svcadm restart salt-minion"

rsync -avzh /srv/  10.20.2.200:/srv/   --checksum --dry-run

````zhixiang cloud section````
ansible  'ci*:!ci18'    -m raw -a "vmadm list| grep cloud |awk '{print \$1}' |xargs -I {} zlogin {}  grep 'service =' /data/howl/etc/howl.conf "
ansible  'ci*:!ci18'    -m raw -a "vmadm list| grep cloud |awk '{print \$1}' |xargs -I {} zlogin {}  sed -i.bak  's/^service = b_howl/service = wu_b_how/'  /data/howl/etc/howl.conf "

ansible  'ci*:!ci18'    -m raw -a "vmadm list| grep cloud |awk '{print \$1}' |xargs -I {} zlogin {}  grep 'service =' /data/snarl/etc/snarl.conf "
ansible  'ci*:!ci18'    -m raw -a "vmadm list| grep cloud |awk '{print \$1}' |xargs -I {} zlogin {}  sed -i.bak  's/^service = b_snarl/service = wu_b_snarl/'  /data/snarl/etc/snarl.conf "

ansible  'ci*:!ci18'    -m raw -a "vmadm list| grep cloud |awk '{print \$1}' |xargs -I {} zlogin {}  grep 'service =' /data/sniffle/etc/sniffle.conf "
ansible  'ci*:!ci18'    -m raw -a "vmadm list| grep cloud |awk '{print \$1}' |xargs -I {} zlogin {}  sed -i.bak  's/^service = b_sniffle/service = wu_b_sniffle/'  /data/sniffle/etc/sniffle.conf "



通用修改版本：
ansible  'ci*:!ci18'    -m raw -a "vmadm list| grep cloud |awk '{print \$1}' |xargs -I {} zlogin {}  sed -i.bak  's/^service = b_sniffle/service = wu_b_sniffle/'  /data/*/etc/*.conf "
ansible  'ci*:!ci18'    -m raw -a "vmadm list| grep cloud |awk '{print \$1}' |xargs -I {} zlogin {}  sed -i.bak  's/^service = b_sniffle/service = wu_b_sniffle/'  /data/*/etc/*.conf "

ansible  'ci*:!ci18'    -m raw -a "vmadm list| grep cloud |awk '{print \$1}' |xargs -I {} zlogin {}  sed -i.bak  's/^libsnarl.instance = snarl/libsnarl.instance = wu_snarl/' /data/*/etc/*.conf"
ansible  'ci*:!ci18'    -m raw -a "vmadm list| grep cloud |awk '{print \$1}' |xargs -I {} zlogin {}  sed -i.bak  's/^libhowl.instance = howl/libhowl.instance = wu_howl/'    /data/*/etc/*.conf"
通用查询版本：
ansible  'ci*:!ci18'    -m raw -a "vmadm list| grep cloud |awk '{print \$1}' |xargs -I {} zlogin {}  grep 'service = ' /data/*/etc/*.conf "
ansible  'ci*:!ci18'    -m raw -a "vmadm list| grep cloud |awk '{print \$1}' |xargs -I {} zlogin {}  grep 'li.*.instance' /data/*/etc/*.conf "


chunter:
查询：
ansible  'ci*:!ci18'    -m raw -a "grep 'li.*.instance' /opt/chunter/etc/chunter.conf "
修改
ansible  'ci*:!ci18'    -m raw -a "sed -i.bak   's/libsnarl.instance = snarl/libsnarl.instance = wu_snarl/' /opt/chunter/etc/chunter.conf "
ansible  'ci*:!ci18'    -m raw -a "sed -i.bak2  's/libhowl.instance = howl/libhowl.instance = wu_howl/'    /opt/chunter/etc/chunter.conf  "
ansible  'ci*:!ci18'    -m raw -a "sed -i.bak2  's/libsniffle.instance = sniffle/libsniffle.instance = wu_sniffle/'    /opt/chunter/etc/chunter.conf  "
sed -i ''  's/libsnarl.instance = snarl/libsnarl.instance = wu_snarl/' /opt/chunter/etc/chunter.conf


查询
 ansible  'ci*:!ci18'    -m raw -a "vmadm list| grep cloud |awk '{print \$1}' |xargs -I {} zlogin {}  grep 'create_res_g_pool' /data/*/etc/*.conf "
ansible  'ci*:!ci18'    -m raw -a "vmadm list| grep cloud |awk '{print \$1}' |xargs -I {} zlogin {}  grep 'nfs_dataset' /data/*/etc/*.conf "

```aio section````


salt -C 'cloud* and not cloud18'   cmd.run "imgadm import  c7ddb0a8-5fe0-11e7-b983-f305fb303725"  -t 1800
salt -C 'cloud* and not cloud18'   cmd.run "ps -ef | grep chunter"  -t 1800
salt -C 'cloud* and not cloud18'   cmd.run "pkill -f zlogin"  -t 1800

salt -C 'cloud* and not cloud18'   cmd.run "svcadm clear chunter;svcadm clear zlogin;svcadm clear epmd"  -t 1800
salt -C 'cloud* and not cloud18'   cmd.run "svcadm disable chunter;svcadm disable zlogin;svcadm disable epmd"  -t 1800
salt -C 'cloud* and not cloud18'   cmd.run "svcadm enable epmd;svcadm enable zlogin;svcadm enable chunter"  -t 1800

salt -C 'cloud* and not cloud18'   cmd.run "svcs chunter zlogin epmd"  -t 1800
salt   -C 'cloud* and not cloud18'    cmd.script salt://script/zhixiang_cloud_chunter_0_9_install_salt.cmd.script.sh -t 600



salt -C 'cloud* and not cloud18'   cmd.run "vmadm list| grep ci_cloud_beijing_ |awk '{print \$1}' |xargs -I {} sh -c 'echo {\\\"image_uuid\\\": \\\"c7ddb0a8-5fe0-11e7-b983-f305fb303725\\\"} | vmadm reprovision {}' "  -t 60
#salt -C 'cloud* and not cloud18'   cmd.run "vmadm list| grep ci_cloud_beijing_ |awk '{print \$1}' |xargs -I {} vmadm stop {}"



salt -C 'cloud* and not cloud18'   cmd.run "vmadm list| grep ci_cloud_beijing_ |awk '{print \$1}' |xargs -I {} zlogin {} sed -i.bak \\'\$ d\\' /opt/local/etc/pkgin/repositories.conf " -t 60
salt -C 'cloud* and not cloud18'   cmd.run "vmadm list| grep ci_cloud_beijing_ |awk '{print \$1}' |xargs -I {} zlogin {} cat /opt/local/etc/pkgin/repositories.conf " -t 60


salt-run manage.down removekeys=True


salt -C 'cloud* and not cloud18'   cmd.run "vmadm list| grep ci_cloud_beijing_ |awk '{print \$1}' |xargs -I {}  zlogin {} sh -c 'echo 10.20.2.200 salt>>/etc/hosts;echo http://192.168.31.12:8000/2016Q3/x86_64/All >> /opt/local/etc/pkgin/repositories.conf;echo http://salt/smartos/pkgin2016Q3 >> /opt/local/etc/pkgin/repositories.conf;rm -fr /var/db/pkgin/*;/opt/local/bin/pkgin -fy up;/opt/local/bin/pkgin -y install salt lrzsz;/usr/bin/hostname>/opt/local/etc/salt/minion_id;sleep 10;' " -t 600

salt -C 'cloud* and not cloud18'   cmd.run "vmadm list| grep ci_cloud_beijing_ |awk '{print \$1}' |xargs -I {}  zlogin {} svcs -a | grep salt"
salt -C 'cloud* and not cloud18'   cmd.run "vmadm list| grep ci_cloud_beijing_ |awk '{print \$1}' |xargs -I {}  zlogin {} svcadm enable svc:/pkgsrc/salt:minion"



##修改/data目录下的配置文件，因为是dataset delegate ，所以在global 生命周期内只要修改一次
#salt 'ci_cloud_beijing_*'  cmd.run "ls /data/*/etc/*.conf|xargs -I{} cp {} {}.bak3"
#salt 'ci_cloud_beijing_*' cmd.run  "sed -i.bak2  's/^service = howl/service = wu_howl/'  /data/howl/etc/howl.conf "
#salt 'ci_cloud_beijing_*' cmd.run  "sed -i.bak2  's/^service = snarl/service = wu_snarl/'  /data/snarl/etc/snarl.conf "
#salt 'ci_cloud_beijing_*' cmd.run  "sed -i.bak2  's/^service = sniffle/service = wu_sniffle/'  /data/sniffle/etc/sniffle.conf "
#
#salt 'ci_cloud_beijing_*'  cmd.run "sed -i.bak2  's/^libsnarl.instance = snarl/libsnarl.instance = wu_snarl/' /data/*/etc/*.conf"
#salt 'ci_cloud_beijing_*'  cmd.run "sed -i.bak2  's/^libhowl.instance = howl/libhowl.instance = wu_howl/'     /data/*/etc/*.conf"
#salt 'ci_cloud_beijing_*'  cmd.run "sed -i.bak2  's/^libsniffle.instance = sniffle/libsniffle.instance = wu_sniffle/'     /data/*/etc/*.conf"

salt 'ci_cloud_beijing_*'  cmd.run "sed -i.bak2  '/ldap_dataset/d' /data/*/etc/*.conf "
salt 'ci_cloud_beijing_*'  cmd.run "sed -i.bak2  '/create_res_g_pool/d' /data/*/etc/*.conf "
salt 'ci_cloud_beijing_*'  cmd.run "sed -i.bak2  '/nfs_dataset/d' /data/*/etc/*.conf "
##
salt ci_cloud_beijing_*  cmd.run "svcadm enable epmd;svcadm enable snarl; svcadm enable howl;svcadm enable sniffle"
#salt 'ci_cloud_beijing_*' cmd.run "svcadm restart snarl; svcadm restart howl;svcadm restart sniffle"
#salt ci_cloud_beijing_* cmd.run "svcadm clear snarl; svcadm clear howl;svcadm clear sniffle"

salt ci_cloud_beijing_*  cmd.run "svcs epmd snarl sniffle howl"
salt ci_cloud_beijing_*  cmd.run "mkdir -p /opt/local/etc/nginx/includes;cp /opt/local/www/flower.conf /opt/local/etc/nginx/includes"
salt 'ci_cloud_beijing_*'  state.single      file.managed  template=jinja name='/opt/local/etc/nginx/nginx.conf'   source='salt://file/nginx.conf.zhixiang_cloud'  backup='minion'
salt ci_cloud_beijing_*   state.single      file.managed  template=jinja name='/opt/local/etc/nginx/includes/flower.conf'   source='salt://file/flower.conf.zhixiang_cloud'  backup='minion'
salt ci_cloud_beijing_*   cmd.run           "svcadm enable nginx"
#salt 'ci_cloud_beijing_1' cmd.run "snarl-admin member-status;howl-admin member-status;sniffle-admin member-status"  -t 600

salt ci_cloud_beijing_*  state.sls fifo_0_9_rsyslog5_conf_blockreplace

salt -C 'cloud* and not cloud18'   cmd.run "svcs chunter"  -t 1800
=====openstack section=====




#http://tripleo.org/environments/baremetal.html#instackenv
#To deploy a minimal TripleO cloud with TripleO you need the following baremetal machines:

  * 1 Undercloud
  * 1 Overcloud Controller
  * 1 Overcloud Compute


#步骤1：安装Undercloud，在虚拟机上执行 https://www.rdoproject.org/tripleo/，装ansible远程控制物理机

 ssh root@$VIRTHOST uname -a
 wget https://raw.githubusercontent.com/openstack/tripleo-quickstart/master/quickstart.sh
 sudo bash quickstart.sh --install-deps
 
 export VIRTHOST='my_test_machine.example.com'
 curl -O https://raw.githubusercontent.com/openstack/tripleo-quickstart/master/quickstart.sh
 bash quickstart.sh $VIRTHOST

 执行bash quickstart.sh $VIRTHOST命令后输出如下
 ssh -F /root/.quickstart/ssh.config.ansible undercloud
 http://tripleo.org/basic_deployment/basic_deployment_cli.html#upload-images

#步骤2：在物理机上执行,安装用于控制虚拟机的ipmi组件

#install RDO repository:
sudo yum install -y centos-release-openstack-ocata
sudo yum install -y python-virtualbm

[root@hp stack]# virsh  list
 Id    Name                           State
----------------------------------------------------
 6     docker-test2                   running
 7     docker-test3                   runnin

sudo vbmc add   --port 7000 --username admin --password admin  docker-test2
sudo vbmc add   --port 7001 --username admin --password admin  docker-test3

yum -y install ipmitool
ipmitool -I lanplus -U admin -P admin  -H 10.0.1.99 -p 7001 power status
ipmitool -I lanplus -U admin -P admin  -H 192.168.23.1 -p 7000 power status
步骤3：在undercloud上查看相关信息
ssh -F /root/.quickstart/ssh.config.ansible undercloud #undercloud

物理机配置：
[stack@undercloud ~]$ cat instackenv.json
{
    "nodes":[
        {
            "name":"compute",
            "pm_type":"pxe_ipmitool",
            "pm_user":"admin",
            "pm_password":"admin",
            "pm_addr":"192.168.23.1",
            "pm_port":"7000"
        },
        {
            "name":"controller",
            "pm_type":"pxe_ipmitool",
            "pm_user":"admin",
            "pm_password":"admin",
            "pm_addr":"192.168.23.1",
            "pm_port":"7001"
        }
    ]
}



[stack@undercloud ~]$ source stackrc 
[stack@undercloud ~]$ openstack image list
[stack@undercloud ~]$ neutron subnet-list
                      neutron subnet-update f31f5147-2004-47fd-af79-780864f89a57  --dns-nameservers list=true 8.8.8.8

#步骤4：配置可用于部署云的物理机器

                      #查看物理机
                      openstack baremetal node list
                      #查看物理机地址

                      less /home/stack/instackenv.json
                      openstack baremetal import --json ~/instackenv.json
                      openstack baremetal configure boot

[stack@undercloud ~]$ openstack baremetal node manage 201121dd-fad4-4be7-a94b-b5cbd92369f7
[stack@undercloud ~]$ openstack baremetal node manage 29ae7132-a28d-4db0-af0c-9534e2f30d5f
或                    for node in $(openstack baremetal node list -c UUID -f value) ; do openstack baremetal node manage $node ; done
                      openstack baremetal node delete cfe0d0f8-5424-458b-a44f-4ee181d7c6cd
#introspect
openstack overcloud node introspect 201121dd-fad4-4be7-a94b-b5cbd92369f7 --provide #失败out-of-band introspection有bug，https://bugzilla.redhat.com/show_bug.cgi?id=1395819
openstack baremetal introspection bulk start  #成功

openstack baremetal introspection status 201121dd-fad4-4be7-a94b-b5cbd92369f7         #查看结果
sudo journalctl -u openstack-ironic-inspector -u openstack-ironic-inspector-dnsmasq   #日志
 #日志：/var/log/ironic-inspector/  
systemctl status openstack-ironic-inspector-dnsmasq
                      
ironic node-update compute add properties/capabilities='profile:compute,boot_option:local'
ironic node-update controller add properties/capabilities='profile:control,boot_option:local'
http://192.168.23.1:3000
sudo hiera admin_password

====pwc section===



`````pwc kvm````
qemu-img create -f qcow2  /home/frank/HPDisk/kvm/pwc.qcow2 100G
sudo virt-install  -n pwc  -r 2048 --disk=path=/home/frank/HPDisk/kvm/pwc.qcow2   --location /home/frank/Downloads/CentOS-7-x86_64-Minimal-1611.iso   --nographics  -w bridge:bridge0  --initrd-inject=/home/frank/anaconda-ks.cfg.centos7-kvm-yibaba   --extra-args "ks=file:/anaconda-ks.cfg.centos7-kvm-yibaba console=ttyS0,115200 ksdevice=eth0 ip=10.0.1.198 netmask=255.255.255.0 dns=10.0.1.1 gateway=10.0.1.1"

````````````````````
ssh-keygen -f $HOME/.ssh/id_rsa -t rsa -N '' 
在公司家里都执行：ssh-copy-id -i ~/.ssh/id_rsa.pub frank@139.219.100.43
在公司执行： ssh -R     1977:127.0.0.1:3389   frank@139.219.100.43
            家里访问公司windows用1977端口
在家里执行： ssh -R     3389:127.0.0.1:3389   frank@139.219.100.43
            公司访问家里windows用3389端口

/usr/libexec/qemu-kvm -name guest=undercloud,debug-threads=on -S -object secret,id=masterKey0,format=raw,file=/home/stack/.config/libvirt/qemu/lib/domain-1-undercloud/master-key.aes -machine pc-i440fx-rhel7.3.0,accel=kvm,usb=off -cpu Skylake-Client,+ds,+acpi,+ss,+ht,+tm,+pbe,+dtes64,+monitor,+ds_cpl,+vmx,+smx,+est,+tm2,+xtpr,+pdcm,+osxsave,+tsc_adjust,+clflushopt,+pdpe1gb,-avx,-avx2 -m 12288 -realtime mlock=off -smp 6,sockets=6,cores=1,threads=1 -uuid da53a3ab-1a3c-4330-b4a4-0d3eb5e1fb62 -no-user-config -nodefaults -chardev socket,id=charmonitor,path=/home/stack/.config/libvirt/qemu/lib/domain-1-undercloud/monitor.sock,server,nowait -mon chardev=charmonitor,id=monitor,mode=control -rtc base=utc,driftfix=slew -global kvm-pit.lost_tick_policy=discard -no-hpet -no-shutdown -boot menu=off,strict=on -device piix3-usb-uhci,id=usb,bus=pci.0,addr=0x1.0x2 -drive file=/home/stack/pool/undercloud.qcow2,format=qcow2,if=none,id=drive-virtio-disk0 -device virtio-blk-pci,scsi=off,bus=pci.0,addr=0x5,drive=drive-virtio-disk0,id=virtio-disk0,bootindex=1 -netdev tap,fd=25,id=hostnet0 -device virtio-net-pci,netdev=hostnet0,id=net0,mac=00:aa:2d:6e:e8:5a,bus=pci.0,addr=0x3 -netdev tap,fd=26,id=hostnet1 -device virtio-net-pci,netdev=hostnet1,id=net1,mac=00:aa:2d:6e:e8:58,bus=pci.0,addr=0x4 -chardev pty,id=charserial0 -device isa-serial,chardev=charserial0,id=serial0 -vnc 127.0.0.1:0 -device cirrus-vga,id=video0,bus=pci.0,addr=0x2 -device virtio-balloon-pci,id=balloon0,bus=pci.0,addr=0x6 -msg timestamp=on


`````demo section````pwc demo backup section`````pwc demo section````
·············
host:
          echo 1 > /proc/sys/net/ipv4/ip_forward
jira:
          /opt/atlassian/jira/bin/start-jira.sh
windows:
         cd /cygdrive/c/work/PwC/spark-2.2.0-bin-hadoop2.7
         ./bin/spark-class.cmd org.apache.spark.deploy.history.HistoryServer

         cd /cygdrive/c/work/PwC/hadoop-spark/scala-devops
         /cygdrive/c/work/PwC/spark-2.2.0-bin-hadoop2.7/bin/spark-submit.cmd  --class "SimpleApp"   --master local[4]   target/scala-2.11/simple-project_2.11-1.0.jar>/tmp/       spark.html 
         abc=`cat /tmp/spark.html` ; sed s/template/"$abc"/  /tmp/report_template.html >/tmp/report_windows.html
         #http://192.168.56.17:81/linux-spark.html

spark-linux:
         docker exec -ti spark  /usr/local/spark/sbin/start-history-server.sh

http://192.168.56.17:81/spark-log2.html
············
docker run -d -p 5000:5000 --restart=always --name registry registry:latest
#步骤一：在源机器上执行：
docker inspect 58e872eebcd3 |less -i
docker inspect --format="{{json .Config.ExposedPorts }}"  pwc_jenkins
docker inspect --format="{{json .Config.ExposedPorts }}" spark
docker inspect --format="{{json .Config.ExposedPorts }}" nifty_neumann

docker commit   -c "EXPOSE 50000"   -c "EXPOSE 8080"  e8403a745904   pwc/jenkins:20171011
docker commit   -c "EXPOSE 80"      -c "EXPOSE 8080"  jira           pwc/jira:20171011

docker commit   --change='CMD ["/usr/sbin/init"]'  --change='VOLUME [ "/sys/fs/cgroup" ]' -c "EXPOSE 4040"  -c "EXPOSE 18080"   -c "EXPOSE 8080"  -c "EXPOSE 80"  -c "EXPOSE 22"  spark pwc/spark:20171027

docker commit   -c "EXPOSE 29418"   -c "EXPOSE 8080"  gerrit   pwc/gerrit:20171011


docker tag pwc/gerrit:20171011  10.0.1.198:5000/pwc/gerrit:20171011
docker tag pwc/spark_env:20171011  10.0.1.198:5000/pwc/spark_env:20171011
docker tag pwc/jira:20171011  10.0.1.198:5000/pwc/jira:20171011
docker tag pwc/jenkins:20171011  10.0.1.198:5000/pwc/jenkins:20171011


cat > /etc/docker/daemon.json <<-EOF
        { "insecure-registries":["10.0.1.198:5000"] }
EOF
       systemctl restart  docker.service
docker push 10.0.1.198:5000/pwc/gerrit:20171011
docker push 10.0.1.198:5000/pwc/jira:20171011
docker push 10.0.1.198:5000/pwc/spark_env:20171011
docker push 10.0.1.198:5000/pwc/jenkins:20171011

#步骤二：在10.0.1.198机器上执行：
cat > /etc/docker/daemon.json <<-EOF
        { "insecure-registries":["10.0.1.198:5000"] }
EOF
       systemctl restart  docker.service
docker pull 10.0.1.198:5000/pwc/gerrit:20171011
docker pull 10.0.1.198:5000/pwc/jira:20171011
docker pull 10.0.1.198:5000/pwc/spark_env:20171011
docker pull 10.0.1.198:5000/pwc/jenkins:20171011

docker run --privileged -d -ti -v /sys/fs/cgroup:/sys/fs/cgroup:ro  --name jira      -p 18080:8080         -p 180:80 10.0.1.198:5000/pwc/jira:20171011
docker  exec -ti  jira bash
/opt/atlassian/jira/bin/start-jira.sh


rsync -avzh  --checksum  /var/lib/docker/volumes/jenkins_home/  10.0.1.198:/var/lib/docker/volumes/jenkins_home/ --delete -n
docker run -d -ti --name jenkins -p 8080:8080 -p 50000:50000 --env JAVA_OPTS="-Djava.util.logging.config.file=/var/jenkins_home/log.properties"   -v jenkins_home:/var/jenkins_home -v /etc/resolv.conf:/etc/resolv.conf 10.0.1.198:5000/pwc/jenkins:20171011


docker run --name gerrit -d -v ~/gerrit_volume:/var/gerrit/review_site -p 8081:8080 -p 29418:29418 --restart=always  10.0.1.198:5000/pwc/gerrit:20171011
docker run --name gerrit -d -p 8081:8080 -p 29418:29418  --restart=always 10.0.1.198:5000/pwc/gerrit:20171011
docker run --name gerrit -d -p 8080:8080 -p 29418:29418 -e GERRIT_INIT_ARGS='--install-plugin=replication' --restart=always 10.0.1.198:5000/pwc/gerrit:20171011
docker run --name gerrit -d -p 8080:8080 -p 29418:29418 -e GERRIT_INIT_ARGS='--install-plugin=replication --install-plugin=download-commands' --restart=always pwc/gerrit:20171124

docker run --name spark --privileged -d -ti -v /sys/fs/cgroup:/sys/fs/cgroup:ro  -v /var/share:/var/share -p 18082:18080 -p 81:80 -p 222:22 -p 8082:8080 pwc/spark:20171027
`````````spark scala section for linux````````````````

··········
spark编译
export MAVEN_OPTS="-Xmx2g -XX:ReservedCodeCacheSize=512m"
·············



#在dockers内安装并运行spark demo程序
docker run -ti -v /sys/fs/cgroup:/sys/fs/cgroup:ro local/c7-systemd:latest bash
或 docker exec -it youthful_albattani   bash


#步骤一：安装spark
tar xvf spark-2.2.0-bin-hadoop2.7.tgz
mv spark-2.2.0-bin-hadoop2.7 /usr/local/spark
export PATH=$PATH:/usr/local/spark/bin
source ~/.bashrc


#步骤二：安装sbt编译环境，http://spark.apache.org/docs/latest/quick-start.html
linux:
      curl https://bintray.com/sbt/rpm/rpm | sudo tee /etc/yum.repos.d/bintray-sbt-rpm.repo
      sudo yum -y install sbt
#windows:
      http://www.scala-sbt.org/1.x/docs/Installing-sbt-on-Windows.html

#sbt 在windows上的相关路径，sbt的plugin可以在这个目录下设置，C:\Users\Frank\.sbt\preloaded\oro\oro\2.0.8\docs\oro-javadoc.jar
$ find .
.
./build.sbt
./src
./src/main
./src/main/scala
./src/main/scala/SimpleApp.scala

#compile-scalar-code-by-sbt-for-spark
   sbt package  
#on linux:
  docker exec -ti  spark bash
  cd /root/devops
  /usr/local/spark/bin/spark-submit   --class "SimpleApp"   --master local[4]   target/scala-2.11/simple-project_2.11-1.0.jar


#步骤三：创建image

docker commit --change='CMD ["/usr/sbin/init"]'    -c "EXPOSE 80"    1e5037d756a3      local/c7-systemd-spark:version1
#步骤四：上传到docker私有源
docker tag local/c7-systemd-spark:version1  localhost:5000/spark:version1
docker push localhost:5000/spark:version1
···············spark scala section for windows··············
#on windows:


  
步骤0：
  用sbt编译出来的路径./bin，和用eclipse编译出来的路径./target/scala-2.11/classes/
  $ find ./ -ipath *simpleapp*
  用elipse编译出来的：
  ./bin/SimpleApp$$anonfun$1.class
  ./bin/SimpleApp$$anonfun$2.class
  ./bin/SimpleApp$.class
  ./bin/SimpleApp.class
  ./src/main/scala/SimpleApp.scala
  #用stb编译出来的：
  ./target/scala-2.11/classes/SimpleApp$$anonfun$1.class
  ./target/scala-2.11/classes/SimpleApp$$anonfun$2.class
  ./target/scala-2.11/classes/SimpleApp$.class
  ./target/scala-2.11/classes/SimpleApp.class  

 
3步骤一:
cat ./project/site.sbt
addSbtPlugin("com.typesafe.sbteclipse" % "sbteclipse-plugin" % "5.2.2")
#步骤二:生成eclipse需要的文件
运行sbt eclipse
```````
eclipse问题描述:
scalatest_2.10-1.9.1.jar is cross-compiled with an incompatible version of Scala (2.10).
In case of errorneous report, this check can be disabled in the compiler preference page.

  进入到scala compiler preference page (project->properties) 
  方法一：
  #http://spark.apache.org/developer-tools.html
  #alvinalexander.com/scala/sbt-how-to-configure-work-with-eclipse-projects
  #Project->properties->Scala Compiler->选中Use project Settings，在scala intallation中选中和sbt一致的scala版本
  #  scala版本是运行sbt ecliplse时下载的scala库文件版本，
  #  scala库文件版本在.classpath中
  #  运行sbt ecliplse会下载scala库文件，并生成.classpath和.project文件
  Frank@Frank-PC /cygdrive/c/work/PwC/spark
  $ ls -a
  .  ..  .cache-main  .classpath  .project  .settings  bin  build.sbt  project  spark-warehouse  src  target
  
  方法二：
  #http://blog.csdn.net/liujinkui41/article/details/38314687
  ->Build manager->withVersionClasspathValidator 勾选去掉
  然后 compileorder 默认 Mixed (如果默认错误会仍然显示），选择java then scala 或 scala then java 均可
 ····················
#运行scalar程序
 步骤一：
 cd /cygdrive/c/work/PwC/hadoop-spark/scala-devops
 sbt package  #把class文件打包成jar文件
#步骤二：用eclipse编译
# eclipse build 路径：设置为 target/scala-2.11/simple-project_2.11-1.0.jar
#  If you want to change the location of this directory for whatever reason, right-click any Java project, Project > Properties->select "Build Path" -> "Configure Build Path". The location   of the output directory is specified in the "Source" tab. 
# 步骤三
    cat > /tmp/report_template.html<<EOF
    <style>
    body{
    display: flex;
     justify-content:center;
      align-items:center;
    }
    </style>
    
    <html>
    <h1>template</h1>
    </html>
EOF
 cd /cygdrive/c/work/PwC/hadoop-spark/scala-devops
  /cygdrive/c/work/PwC/spark-2.2.0-bin-hadoop2.7/bin/spark-submit.cmd  --class "SimpleApp"   --master local[4]   target/scala-2.11/simple-project_2.11-1.0.jar>/tmp/spark.html 
  abc=`cat /tmp/spark.html` ; sed s/template/"$abc"/  /tmp/report_template.html >/tmp/report_windows.html
···spark history section for windows···
步骤一：
在windows中设置环境变量
echo %HADOOP_HOME%

  0.  在windows上运行hadoop需要winutils binary in the hadoop binary path.
      https://stackoverflow.com/questions/19620642/failed-to-locate-the-winutils-binary-in-the-hadoop-binary-path
  0.5 运行hadoop，报错17/10/24 15:38:16 INFO SparkContext: Successfully stopped SparkContext  Exception in thread "main" ExitCodeException exitCode=-1073741515:
      https://stackoverflow.com/questions/45947375/why-does-starting-a-streaming-query-lead-to-exitcodeexception-exitcode-1073741
      使用winutils会发现缺少dll文件

  1.运行windows缺文件：C:\Users\Frank>C:\work\PwC\spark-2.2.0-bin-hadoop2.7\bin\winutils.exe chmod -R 777 C:\work\PwC\spark-2.2.0-bin-hadoop2.7
                    C:\work\PwC\spark-2.2.0-bin-hadoop2.7\bin\winutils.exe chmod -R 777 /tmp
  2.缺的dll文件到https://www.microsoft.com/en-us/download/details.aspx?id=14632下载，具体来源见下面网址
    https://www.faqforge.com/windows/fix-the-program-cant-start-because-msvcr100-dll-is-missing-from-your-computer-error-on-windows/
     


步骤二：
   mkdir -p /cygdrive/c/tmp/spark-events

   cp ./spark-2.2.0-bin-hadoop2.7/conf/spark-defaults.conf.template ./spark-2.2.0-bin-hadoop2.7/conf/spark-defaults.conf
   vi /cygdrive/c/work/PwC/spark-2.2.0-bin-hadoop2.7/conf/spark-defaults.conf

        spark.eventLog.enabled          true
        spark.history.fs.logDirectory   file:///tmp/spark-events
步骤三：运行  
   cd /cygdrive/c/work/PwC/spark-2.2.0-bin-hadoop2.7
   ./bin/spark-class.cmd org.apache.spark.deploy.history.HistoryServer



```````spark java section`````
#步骤五：运行java版本的例子
vi ./src/main/java/SimpleApp.java
<project>
  <groupId>edu.berkeley</groupId>
  <artifactId>simple-project</artifactId>
  <modelVersion>4.0.0</modelVersion>
  <name>Simple Project</name>
  <packaging>jar</packaging>
  <version>1.0</version>
  <dependencies>

    <dependency>
        <groupId>org.apache.spark</groupId>
        <artifactId>spark-sql_2.10</artifactId>
        <version>2.2.0</version>
    </dependency>
    
  </dependencies>
  <build>

   <plugins>
     <plugin>
       <groupId>org.apache.maven.plugins</groupId>
          <artifactId>maven-compiler-plugin</artifactId>
             <version>3.7.0</version>
             <configuration>
               <source>1.8</source>
               <target>1.8</target>
             </configuration>
     </plugin>
    </plugins>
   </build>
</project>    

````````````

cat /root/java-devops/src/main/java/SimpleApp.java
/* SimpleApp.java */
import org.apache.spark.sql.SparkSession;
public class SimpleApp {
  public static void main(String[] args) {
    String logFile = "/usr/local/spark/README.md"; // Should be some file on your system
    SparkSession spark = SparkSession.builder().appName("Simple Application").getOrCreate();
    org.apache.spark.sql.Dataset<String> logData = spark.read().textFile(logFile).cache();

    long numAs = logData.filter(s -> s.contains("a")).count();
    long numBs = logData.filter(s -> s.contains("b")).count();

    System.out.println("Lines with a: " + numAs + ", lines with b: " + numBs);

    spark.stop();
  }
}
cd /root/java-devops
mvn package
/usr/local/spark/bin/spark-submit   --class "SimpleApp"   --master local[4]   target/simple-project-1.0.jar




`````spark python section``````````
#步骤六：运行python版本的例子
SPARK_HOME=/usr/local/spark
export PYTHONPATH=$SPARK_HOME/python:$PYTHONPATH
export PYTHONPATH=$SPARK_HOME/python/lib/py4j-0.10.4-src.zip:$PYTHONPATH
#/usr/local/spark/python/lib/py4j-0.10.4-src.zip
····················
cat  /root/python-devops/SimpleApp.py 
"""SimpleApp.py"""
from pyspark.sql import SparkSession

logFile = "/usr/local/spark/README.md"  # Should be some file on your system
appName = "spark-python-wujunrong"
spark = SparkSession.builder.appName(appName).master("local").getOrCreate()
logData = spark.read.text(logFile).cache()

numAs = logData.filter(logData.value.contains('a')).count()
numBs = logData.filter(logData.value.contains('b')).count()

print("Lines with a: %i, lines with b: %i" % (numAs, numBs))

spark.stop()
·········spark python windows··········
windows:
SPARK_HOME=/cygdrive/c/work/PwC/spark-2.2.0-bin-hadoop2.7
cd /cygdrive/c/work/PwC/spark
/cygdrive/c/work/PwC/spark-2.2.0-bin-hadoop2.7/bin/spark-submit.cmd --master local[4] ./SimpleApp.py
```````
SPARK_HOME=/usr/local/spark
export PYTHONPATH=$SPARK_HOME/python:$PYTHONPATH
export PYTHONPATH=$SPARK_HOME/python/lib/py4j-0.10.4-src.zip:$PYTHONPATH
/usr/local/spark/bin/spark-submit  --master local[4] /root/python-devops/SimpleApp.py 

步骤六：制作docker镜像(迁移docker container场景)
docker commit --change='CMD ["/usr/sbin/init"]'    -c "EXPOSE 80" -c "EXPOSE 4040"  1e5037d756a3      local/c7-systemd-spark:example-shell-java-python
docker tag local/c7-systemd-spark:example-shell-java-python  localhost:5000/c7-systemd-spark:example-shell-java-python
ssh -R     5000:127.0.0.1:5000   frank@139.219.100.43


#步骤七：在另一台机器上运行装有spark的docker(迁移docker container场景)
docker pull 139.219.100.43:5000/c7-systemd-spark:example-shell-java-python
查看ropository中有多少image
方法一：
http://139.219.100.43:5000/v2/_catalog
方法二：
/var/lib/docker/volumes/181f8ba1f76f6d6f132ae91f9c9fff2c518d841d00b2137a5457c74de2828b5c/_data/docker/registry/v2/repositories

docker run -ti -p 4040:4040 -p 18080:18080   139.219.100.43:5000/c7-systemd-spark:example-shell-java-python bash
#步骤八
··········spark history section for linux················
通过网页浏览查看历史运行数据(这种配置方式在python模式相关文档中有阐述，在windows下也适用）：

vi  /usr/local/spark/conf/spark-defaults.conf
spark.eventLog.enabled          true
spark.history.fs.logDirectory   file:///tmp/spark-events

mkdir /tmp/spark-events
/usr/local/spark/sbin/start-history-server.sh
/usr/local/spark/bin/spark-submit  --master local[4] --conf spark.eventLog.enabled=true  /root/python-devops/SimpleApp.py
网页地址http://192.168.56.17:38080  #docker run --name spark -ti -p 4040:4040 -p 38080:18080  -p 280:80 pwc/centos7:spark bash

```gerrit section````
搜索文档：
Change-Id site:https://gerrit-documentation.storage.googleapis.com/


······
步骤1：
#vi /var/gerrit/review_site/etc/gerrit.config
gerrit用户设置
Edit your [auth] statement in gerrit.config to look like this:
[auth]
type = development_become_any_account

And then force login as the admin, grant yourself access

步骤2：在gerrit服务器上，在docker中用了机器名称，需要把他改成ip地址，否则用户logout后，会访问http://f018b8793a63:8080/
vi /var/gerrit/review_site/etc/gerrit.config
canonicalWebUrl = http://192.168.56.17:8080/

···gerrit work flow·
1.查看git源地址：
  gerrit上：菜单project->List->选中项目，点Repository Browser列对应的项目

2.设置ssh
  ssh-keygen -f $HOME/.ssh/id_rsa -t rsa -N ''
  cat .ssh/id_rsa.pub
  切换用户，在右上角setting选项，在左边添加ssh public key，  
3.git clone ssh://wujunrog-username@192.168.56.17:29418/hadoop-spark.git
  用git 来提交， git push origin HEAD:refs/for/master  #https://gerrit-documentation.storage.googleapis.com/Documentation/2.14.3/intro-user.html#upload-change
  或用下面的git-review软件来提交

安装git-review  #git review section
步骤一：
sudo yum install git-review

git config --list
    remote.gerrit.url=ssh://frank@192.168.56.17:29418/hadoop-spark.git
    remote.gerrit.fetch=+refs/heads/*:refs/remotes/gerrit/*


步骤二：

用git review来提交

设置git-review
cat > .gitreview <<EOF #这一步貌似不需要
[gerrit]
host=192.168.56.17
port=29418
project=hadoop-spark.git
defaultbranch=master
EOF

#解决下面步骤三的问题We don't know where your gerrit is. Please manually create a remote
git remote -v
git remote rm gerrit
git remote add gerrit  ssh://wujunrog-username@192.168.56.17:29418/hadoop-spark.git  #必须包含用户名称wujunrog-username来源（菜单project->List->选中项目，点Repository Browser列对应的项目）


步骤三：
git review -s  #初始化git review，第一次运行是会让你输入用户名称，这个用户名称用来向git 提交change
    1.Could not connect to gerrit.
      Enter your gerrit username: wujunrong
    2.We don't know where your gerrit is. Please manually create a remote
后续git review -R还会报错，按照提示安装即可“Hint: To automatically insert Change-Id, install the hook:”
·······

1.把frank放到administration组中去，设置电子邮件，否则下面无法执行gerrit command
  cat ~/.ssh/id_rsa.pub | ssh -p 29418 192.168.56.17  gerrit set-account --add-ssh-key - --add-email publicwu@163.com frank  #以frank用户执行
  cat ~/.ssh/id_rsa.pub | ssh -p 29418 frank@192.168.56.17  gerrit set-account --add-ssh-key - --add-email unarywu2@163.com wujr #说明:frank@192.168.56.17用frank用户的publickey去访问gerrit服务器,frank用户是已存在的而且是已存在email地址的管理员用户，wujr是要在gerrit服务器上设置account的用户
  

2.设置frank用户密码,否则下面一步frank用户无法登入
  操作方法：主页右上角，switch account成frank用户，右上角，点setting按钮，在左边选http password活页夹

3.git clone http://192.168.56.17:8080/hadoop-spark.git  #git clone时用的用户，git review提交审核时也是以这个用户的名义提交审核

4.上传gerrit project项目：
4/1)
git config --global user.name "wujunrong"
git config --global user.email "publicwu@163.com"

git add java-devops/pom.xml
git add java-devops/src/main/java/SimpleApp.java

4/2).把root账户的id_rsa.pub添加到gerrit frank用户的SSH public key中去,以便本地root用户能以frank用户ssh到gerrit中去，否则下面一步报错
     操作方法：主页右上角，switch account成frank用户，右上角，点setting按钮，在左边选SSH public key活页夹

4/3).设置git自动生成Change-Id，install the hook，否则下面一步无法执行
        gitdir=$(git rev-parse --git-dir); scp -p -P 29418 frank@192.168.56.17:hooks/commit-msg ${gitdir}/hooks/

4/4).   git commit -m "wujunrong test gerrit"
        或前面的add没有执行的情况下，可执行git commit -a -m "a week before national day"

git show #查看哪些会被push到remote中去
git review -R  #申请review
! [remote rejected] HEAD -> refs/publish/master (no new changes),错误是由于commit的时候没有加-a参数 git commit -a  -m "wujunrong test gerrit tuesday"


步骤三：peer review，从change 到submittable 需要为label打分，label达到一定能分数才能submittable
知识点：
1、查看上述提交的review的界面入口             ： 菜单all->open 查看上述提交的review 
2、配置私有label入口：  菜单project->General->Project command->Edit config
3、默认组有对review label打分的权限（porject菜单->owner->添加组)，对每个reference也可以设置打分权限（porject菜单->reference->选add group）



····gerrit jenkins section·····
I want to add a "verified" label to my Gerrit project to allow Jenkins to verify that the code builds and passes its tests and so on.I know I need to add a section to project.config as below:[label "Verified"]
      function = MaxWithBlock
      value = -1 Fails
      value =  0 No score
      value = +1 Verified

However, how do I get to that file to edit it?


You can configure your project config in the Gerrit UI.You should follow the following steps:


  1. - Launch your Gerrit UI.
  2. - Login as admin.
  3. - Go to projects > and click List.
  4. - Select your project and click Edit config button.
  5. - Paste your content and click save.
  6. - 点publish，此时change处于draft状态，即使review +2分也不能submit
  7. - 到my菜单，选中点publish，此时submit按钮出现
······

步骤四：:配置jenkins plugin
https://wiki.jenkins.io/display/JENKINS/Gerrit+Trigger
1）登入jenkins服务器：
   cp -r /var/jenkins_home/.ssh    /var/lib/docker/volumes/jenkins_home/_data
   #在docker宿主机上，jenkins_home对应的目录是/var/lib/docker/volumes/jenkins_home/_data,可用docker volume inspect jenkins_home命令查询
   #历史命令参考：docker run -p 8080:8080 -p 50000:50000 -v jenkins_home:/var/jenkins_home jenkins/jenkins:lts
2）jenkins上安装Gerrit Trigger插件
   Manage Jenkins->搜索Gerrit Trigger，在配置trigger的时候，要用wujunrog-username，而不是wujunrong(注册用户时候有2个用户名,一个是简称，另一个是真正的ssh用户名)
   测试stream-events
   ssh -p 29418 wujunrog-username@192.168.56.17  gerrit stream-events
3）配置好git后， 要配置$GERRIT_REFSPEC等变量，只有通过stream-events触发genkins，$GERRIT_REFSPEC等变量才会被触发填值

步骤五:
配置maven
系统管理->Global Tool Configuration->JDK
 /var/jenkins_home/workspace/build-spark-enviromnet

步骤六：在maven中安装spark
docker run -p 8080:8080 -p 50000:50000 -v jenkins_home:/var/jenkins_home  -v /etc/resolv.conf:/etc/resolv.conf jenkins

#在Jenkins中的shell代码





    #!/bin/bash
    export PATH=$PATH:/var/jenkins_home/spark-2.2.0-bin-hadoop2.7/bin/
    exec &> >(tee /tmp/wujunrong.log)
    ls
    pwd
    date
    
    #compile java source code using maven
    cd /var/jenkins_home/workspace/build-spark-enviromnet/default/java-devops
    /var/jenkins_home/tools/hudson.tasks.Maven_MavenInstallation/pwc-maven/bin/mvn package
    
    spark-submit  --class "SimpleApp"   --master local[4]   target/simple-project-1.0.jar
    echo ==============run on remote server==============================
    
    #deploy python code and scala code including sbt
    ssh -p 222  root@192.168.56.17 mkdir -p /root/jenkins-spark/python-devops
    ssh -p 222  root@192.168.56.17 mkdir -p /root/jenkins-spark/scala-devops/src/main/scala/
    ssh -p 222  root@192.168.56.17 mkdir -p /root/jenkins-spark/scala-devops/project
    #complie scala code 
    scp -P 222  /var/jenkins_home/workspace/build-spark-enviromnet/default/python-devops/SimpleApp.py   root@192.168.56.17:/root/jenkins-spark/python-devops/
    scp -P 222  /var/jenkins_home/workspace/build-spark-enviromnet/default/scala-devops/src/main/scala/SimpleApp.scala root@192.168.56.17:/root/jenkins-spark/scala-devops/    src/main/scala/SimpleApp.scala 
    scp -P 222  /var/jenkins_home/workspace/build-spark-enviromnet/default/scala-devops/build.sbt root@192.168.56.17:/root/jenkins-spark/scala-devops/build.sbt
    
    
    echo "cd /root/jenkins-spark/scala-devops;sbt package" |ssh -p 222  root@192.168.56.17 
    echo "cd /root/jenkins-spark/scala-devops ;/usr/local/spark/bin/spark-submit   --class "SimpleApp"   --master local[4]   target/scala-2.11/simple-project_2.11-1.0.jar>/    var/www/html/spark.html" | ssh -p 222  root@192.168.56.17
    
    #handle scalar programm report
    cat > /tmp/report_template.html<<EOF
    <style>
    body{
    display: flex;
     justify-content:center;
      align-items:center;
    }
    </style>
    
    <html>
    <h1>template</h1>
    </html>
    EOF
    
    scp -P 222 /tmp/report_template.html root@192.168.56.17:/var/www/html/report_template.html
    
    echo 'abc=`cat /var/www/html/spark.html` ; sed s/template/"$abc"/  /var/www/html/report_template.html >/var/www/html/linux-spark.html' | ssh -p 222  root@192.168.56.17
    
    
    #ssh root@192.168.56.17  docker cp /var/lib/docker/volumes/jenkins_home/_data/workspace/build-spark-enviromnet/python-devops/SimpleApp.py   spark:/root/python-devops/    SimpleApp.py
    #ssh root@192.168.56.17 docker exec -ti  spark  /usr/local/spark/sbin/start-history-server.sh
    #ssh root@192.168.56.17  docker exec spark /usr/local/spark/bin/spark-submit  --master local[4] /root/python-devops/SimpleApp.py
······   
  执行完上述内容后再执行，传输日志文件：

  scp -P 222  /tmp/wujunrong.log  root@192.168.56.17:/var/www/html/spark-log.html
  echo  "tr '\n'  '<' < /var/www/html/spark-log.html |sed 's/</<br\/>/g'  >/var/www/html/spark-log2.html"  | ssh -p 222  root@192.168.56.17
`````
https://github.com/jenkinsci/docker/blob/master/README.md
docker commit   -c "EXPOSE 50000"   -c "EXPOSE 8080"  187b27435067  pwc/jenkins:lts

cat > /var/lib/docker/volumes/jenkins_home/_data/log.properties <<EOF
handlers=java.util.logging.ConsoleHandler
jenkins.level=FINEST
java.util.logging.ConsoleHandler.level=FINEST
EOF
docker run --name pwc_jenkins -p 8080:8080 -p 50000:50000 --env JAVA_OPTS="-Djava.util.logging.config.file=/var/jenkins_home/log.properties"   -v jenkins_home:/var/jenkins_home -v /etc/resolv.conf:/etc/resolv.conf pwc/jenkins:lts
```````

              docker host上   本地目录： /var/lib/docker/volumes/jenkins_home/_data
              docker container内的目录： /var/jenkins_home

docker cp youthful_albattani:/root/spark-2.2.0-bin-hadoop2.7.tgz /root/
cp /root/spark-2.2.0-bin-hadoop2.7.tgz /var/lib/docker/volumes/jenkins_home/_data/


```````````````
jenkins execute shell 中编译spark java代码
#!/bin/bash
export PATH=$PATH:/var/jenkins_home/spark-2.2.0-bin-hadoop2.7/bin/
exec &> >(tee /tmp/wujunrong.log)
ls
pwd
date
cd /var/jenkins_home/workspace/build-spark-enviromnet/default/java-devops
/var/jenkins_home/tools/hudson.tasks.Maven_MavenInstallation/pwc-maven/bin/mvn package
spark-submit  --class "SimpleApp"   --master local[4]   target/simple-project-1.0.jar

ssh root@192.168.56.17  docker cp /var/jenkins_home/workspace/build-spark-enviromnet/default/python-devops/SimpleApp.py spark:/root/gerrit/hadoop-spark/python-devops/SimpleApp.py
#ssh root@192.168.56.17 docker exec -ti  spark  /usr/local/spark/sbin/start-history-server.sh
ssh root@192.168.56.17  docker exec spark /usr/local/spark/bin/spark-submit  --master local[4] /root/python-devops/SimpleApp.py

``````````jira section`````
docker run --privileged -d -ti -v /sys/fs/cgroup:/sys/fs/cgroup:ro  --name jira      -p 18080:8080         -p 180:80 pwc/centos7:spark
启动：
/opt/atlassian/jira/bin/start-jira.sh

用已有的gerrit frank用户在gerrit上创建 jira用户
chmod 600 /root/.ssh/id_rsa
cat ~/.ssh/id_rsa.pub| ssh frank@192.168.56.17 -p 29418 gerrit create-account jira --email jira@pwc.com --full-name JIRA --ssh-key -

#上述是为本机添加用户，这条是为远端另一台机器添加用户:ssh -p 222 root@192.168.56.17 cat /root/.ssh/id_rsa.pub|ssh -p 29418  wujunrog-username@192.168.56.17  gerrit create-account --ssh-key - gerrit_replication_user2 --email publicwu-replication@163.com --full-name replication


https://github.com/MeetMe/jira-gerrit-plugin
Optionally change the Gerrit search query patterns:

    tr:%s - Look for the issue key in a "Bug:" or "Issue:" footer
    topic:%s - Look for the issue key in the change Topic (uploaded using HEAD:refs/for/master/(issueKey))
    message:%s - Look for the issue key in the commit message

https://community.atlassian.com/t5/Questions/JIRA-Gerrit-Plugin-config-issue/qaq-p/41445
       You should double-check the "Issues Search" value in the Gerrit configuration page. Usually that would be something like message:%s or tr:%s.  Whatever that happens to        be, replace the %s with the JIRA ticket key, and search in Gerrit for that string (i.e., if it's JIRA-123, then look for "tr:JIRA-123" or "message:JIRA-123").  If        Gerrit itself does not return anything for that search query, then the Gerrit plugin also cannot show anything, and you should try changing the search predicate for        something more appropriate.

In my shop, we use "message:%s" and it works fine

teamview:G6H2 H88C PPDK V44L
=====git remote section======git fetch section======

Frank@Frank-PC /cygdrive/c/work/PwC/hadoop-spark
$  git remote -v
gerrit  ssh://wujunrog-username@192.168.56.17:29418/hadoop-spark.git (fetch)
gerrit  ssh://wujunrog-username@192.168.56.17:29418/hadoop-spark.git (push)
origin  ssh://wujunrog-username@192.168.56.17:29418/hadoop-spark.git (fetch)
origin  ssh://wujunrog-username@192.168.56.17:29418/hadoop-spark.git (push)
Frank@Frank-PC /cygdrive/c/work/PwC/hadoop-spark
$ git branch -vv
  master  f729559 [origin/master: 领先 5] test branch on gerrit2-before checkout
* testing 71979fa I am on test branch
Frank@Frank-PC /cygdrive/c/work/PwC/hadoop-spark
$ git remote show origin
* 远程 origin
  获取地址：ssh://wujunrog-username@192.168.56.17:29418/hadoop-spark.git
  推送地址：ssh://wujunrog-username@192.168.56.17:29418/hadoop-spark.git
  HEAD 分支：master
  远程分支：
    master 已跟踪
  为 'git pull' 配置的本地分支：
    master 与远程 master 合并
  为 'git push' 配置的本地引用：
    master 推送至 master (本地已过时)
Frank@Frank-PC /cygdrive/c/work/PwC/hadoop-spark
$ git branch -a
  master
* testing
  remotes/gerrit/master
  remotes/origin/HEAD -> origin/master
  remotes/origin/master

Frank@Frank-PC /cygdrive/c/work/PwC/hadoop-spark
$ git config --list  #配置fetch只获取部分分支
user.email=publicwu@163.com
user.name=frank
merge.tool=meld
mergetool.meld.path=C:\Program Files (x86)\Meld\Meld.exe
core.repositoryformatversion=0
core.filemode=true
core.bare=false
core.logallrefupdates=true
core.ignorecase=true
remote.origin.url=ssh://wujunrog-username@192.168.56.17:29418/hadoop-spark.git
remote.origin.fetch=+refs/heads/*:refs/remotes/origin/*
branch.master.remote=origin
branch.master.merge=refs/heads/master
remote.gerrit.url=ssh://wujunrog-username@192.168.56.17:29418/hadoop-spark.git
remote.gerrit.fetch=+refs/heads/*:refs/remotes/gerrit/*
````demo git section`````gerrit replication`````
git init --bare  spark-server.git
sudo su -c 'ssh mirror1.us.some.org echo' gerrit2
git clone git+ssh://root@192.168.56.17:222/root/git-repository/spark-server.git
ssh -p 29418  wujunrog-username@192.168.56.17 gerrit plugin reload replication

``````gerrit plugin section`````gerrit build`````
#http://www.ingeniousmalarkey.com/2013/12/how-to-build-gerrit-replication-plugin.html

  1. git clone --recursive https://gerrit.googlesource.com/gerrit

    * You need the --recursive here because the plugins are actually git submodules and won't otherwise be cloned along with your repo.
    * If you're already cloned, you can run `git submodule init; git submodule update`
  2. cd gerrit
  3. git checkout -b stable-2.7 origin/stable-2.7
  4. mvn install -DskipTests=true -Dmaven.javadoc.skip=true

    * It's not necessary to skip the tests or generating Java Doc, but it will greatly improve your compile time and decrease the amount of memory maven uses
  5. cd gerrit-plugin-api
  6. mvn package -Dmaven.javadoc.skip=true

    * This creates the jar that will be necessary for the replication plugin to get built
  7. cd plugins/replication
  8. mvn package -Dmaven.javadoc.skip=true
  9. At this point, you have compiled and packaged the replication jar! All you need to do now is register it with your Gerrit server. For simplicity, I'll pretend your gerrit server is running at gerrit.example.com.
  10. scp target/replication-2.7.jar gerrit.example.com:/tmp/
  11. ssh -p 29418 gerrit.example.com gerrit plugin install -n replication /tmp/replication-2.7.jar

=========reviewboard section============
git push  git+ssh://root@10.0.0.4:522/root/git-repository/

sudo docker run --name centos --privileged -d -ti -v /sys/fs/cgroup:/sys/fs/cgroup:ro  -v /var/share:/var/share -p 1234:1234 -p 58080:8080 -p 580:80 -p 522:22 -p 58000:8000 centos7/systemd:20171129